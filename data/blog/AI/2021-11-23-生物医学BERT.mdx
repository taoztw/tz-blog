---
title: 生物医学BERT
date: 2021-11-23
lastmod: 2021-11-23
tags: ['AI']
draft: false
summary: 这篇文章介绍了几个生物医学领域的预训练语言模型,包括BioBERT、中文MC-BERT、Clinical BERT和Med-BERT。这些模型都是在大规模生物医学文本数据上进行预训练,以适应生物医学文本挖掘任务。文章还比较了不同模型的训练数据、训练策略和下游任务表现。
layout: PostSimple
---

> BioBERT: a pre-trained biomedical language representation model for biomedical text mining

在bert-base上继续训练，使用pubmed文章和摘要等数据。

8卡，98304 per iteration，mini-batch 192，训练时间23天

下游任务微调，batch size 10，16，32，64；learning rate 5e-5, 3e-5, 13-5

有很多版本，使用transformers库轻松验证，和相应脚本在基础上再进行预训练。

数据量：通过github下载验证，大约5亿+，需要自己处理next sentence预测任务

> Conceptualized Representation Learning for Chinese Biomedical Text Mining

alibaba训练的中文MC-bert

两种，masking技术，消融实验也分析是有用的。但是whole span mask会存在一些误差（mask短语，短路提取和生物医学领域判断回造成误差）。

训练了100k step， 学习率：1e-5, 没有使用lr warmup，作者说会导致灾难遗忘。

训练数据20M+

![BERT-base和MC-BERT比较](https://tz-1256822507.cos.ap-hongkong.myqcloud.com/typora/2021-11-3014008338.png)

> Publicly Available Clinical BERT Embeddings

使用MIMIC-III v1.4 database临床文本，200万

![image.png](https://tz-1256822507.cos.ap-hongkong.myqcloud.com/typora/2021-11-2179706205.png)

> Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction

使用数据Cerner Health Facts (general EHR) 需要申请 https://business.okstate.edu/chsi/data-requests.html

提供了可视化工具演示电子病历中的依赖语义，增加可解释性。

暂时不需要，有时间再看。
