---
title: Transformer模型训练技巧
date: 2021-11-10
lastmod: 2021-11-10
tags: ['AI']
draft: false
summary: 这篇文章主要探讨了影响Transformer模型训练质量和效率的各种参数设置。作者通过大量实验分析了batch size、学习率、warmup steps、最大句子长度等参数对模型性能的影响,并给出了一些实用的调参建议。文章还比较了不同GPU配置下的训练效果,以及checkpoint averaging等技巧对提升BLEU分数的作用。
layout: PostSimple
---

> Training Tips for the Transformer Model 2018

## 摘要

作者对影响翻译质量，内存使用，训练稳定性，训练时间的参数进行了实验分析。除了更多的数据和更大的模型外，作者提供了一些参数的设置经验，如batch size，learning rate，warmup steps，最大句子长度，checkpoint averaging。

## 评估方法

尽管有较好的实现Results of the WMT17 Metrics Shared Task,作者还是使用简单的BLEU....

并且作者发现，CHRF和BLEU的结果相似

### 模型训练停止条件

神经网络的训练是不确定的。模型训练的停止都是经验之谈，没有什么标准。一些论文中提及到的大都是step和训练时间。如果采用在验证集上不再增长的条件，那么不同设置、不同停止条件的模型之间的比较是不公平的。

文章最后使用了训练过程中BLEU的评价指标计算曲线。(每小时一次) 没有使用单一的值进行判断

```markdown
# 一些名词

training steps
batch size: t2t中一个batch包含的是tokens(subword)的数量。
Effective batch size: 当在8个卡上，以1500的batch 训练时候，effective batch size就是12k个subwords
Training Epoch: 将training step转化为epoch，需要 traing epoch _ effective batch size / total train data(token)
computation speed: 一个小时训练的step
training throughput: 已训练的数据大小。 可以通过 computation speed _ effective batch size计算
Convergence Speed or BLEU convergence：BLEU根据时间计算收敛速度。刚开始很快，后面基本转化为0
Time Till Score：达到设定BLEU值的时间
Examples Till Score：达到设定BLEU的训练样本总数。可以通过time till score _ computation speed _ effective batch size计算
```

> t2tlog中approx-bleu计算的是subword之后的belu。approx-bleu通常是1.2-1.8倍正常BLEU

## 数据 与 预处理

英语-捷克语，CzEng1.7 和Europarl, News Commentary, Common Crawl数据总58M

使用WMT newstest2013作为开发集（在训练数据中没有出现过）

最后使用训练的SOTA模型在WMT newstest 2017上进行测试对比。

作者关于预处理的几个Tips：

1. 确保subword是在足够大的语料上训练的
2. 可以通过设置max_length控制训练句子的数量。过滤掉长数量句子后，TFRecords训练文件将会更小，处理更快

（嗯。。。简单的Tips）

## 实验

两个参数集：transformer_big_single_gpu 和 transformer_base_single_gpu

BIG model:

```
batch-size=1500
```

```
train_steps=6000000
```

```
save_checkpoints=3600
```

```
schedule=train,禁止计算approx_bleu使得训练快一点
```

### 计算速度 数据吞吐量

![](https://www.tzer.top/usr/uploads/2021/11/2158147773.png)

可以看到，单GPUbase模型的数值约是BIG的两倍

多GPU下，GPU之间的信息同步会明显的减少训练速度。但是每次的训练数据可以获得明显的增加。

![](https://www.tzer.top/usr/uploads/2021/11/2822154296.png)

### 训练数据大小

![](https://www.tzer.top/usr/uploads/2021/11/3583711154.png)

Tips：

1. 对比不同的数据集，大的小的，干净的嘈杂的，都需要训练一段时间后查看
2. 对于大的训练数据，BLEU即使在8个GPU上训练一周也会有提升
3. 不能轻易地将一个数据集的结果插入到另一个数据集

### 模型大小

参数区别：

![](https://www.tzer.top/usr/uploads/2021/11/2039679488.png)

![](https://www.tzer.top/usr/uploads/2021/11/1773946352.png)

小模型的大batch size的情况下，效果仍然没有大模型好。

小模型大batch size在训练20个小时左右的情况下，与big模型的效果相似。

1. 要综合考虑机器内存，训练时间的影响。在短时间训练情况下，小模型 + 大batch size可以获得较好的效果
2. 可以使用transformer tiny进行快速调试

### 最大训练句子长度

![](https://www.tzer.top/usr/uploads/2021/11/1399189118.png)

max_lenth设置不同值时，batch size大小和训练和测试语料过滤比例

![](https://www.tzer.top/usr/uploads/2021/11/2599239125.png)

batch_size=1500，在不同max_length的影响。150和200，400的曲线基本相似，70差距较大。作者在batchsize=2000下做了实验，发现25，50的结果仍然比较差，但是70和更大的值有相同的曲线。所以得出结论：如果batch size是足够大的，max_len参数对BLEU没有影响。但这个结论需要在不同的数据集上进行验证。

作者发现即使改变解码参数，模型也无法产生比训练中使用的最大长度更长的翻译。（测试了一下，我没有发现这样的问题）

Tips:

1. 小的max_length，允许更大的batchsize，并且预防OOM错误
2. 设置时考虑train test的过滤比例，观察训练前几个小时的BLEU意外下降或停滞。

### batch size

![](https://www.tzer.top/usr/uploads/2021/11/1218624514.png)

![](https://www.tzer.top/usr/uploads/2021/11/3161905127.png)

big在batch size变小之后，模型就嗝屁了。

Tips：

1. batch size应该设置的大一些。

### 学习率和Warmup steps

![](https://www.tzer.top/usr/uploads/2021/11/276394096.png)

学习率为0.01，收敛速度极慢。学习率为0.3时候训练失败

预防diverged training就是降低学习率或提升warmup step。

![](https://www.tzer.top/usr/uploads/2021/11/2316228244.png)

学习率采用的t2t默认0.2，warmup steps 12k时，训练失败。48k时，可以看到模型初期收敛的速度稍微有点慢。

Tips：

1. 如果训练失败，可以尝试梯度裁剪和更多的warmup steps
2. 如果1方法 还不行，那就降低学习率

### GPUs

![](https://www.tzer.top/usr/uploads/2021/11/1500743589.png)

多GPU的训练，更少的训练数据，更容易收敛。

![](https://www.tzer.top/usr/uploads/2021/11/1927625455.png)

Tips：

1. 根据训练时间和收敛速度来看，与其并行4，4训练，不如8个卡依次训练

### 多GPU下学习率和warmup step

- 保持在单GPU下的最优学习率
- 不要期望调整warmup step来提升BLEU。

### 重新开始训练

这里有两种情况，第一种训练失败，由于checkpoint会保存adam momentum参数，所以重新开始训练和正常训练没有差异。但是checkpoint不会保存数据位置，训练数据是随机生成的。

第二种情况是要在小领域数据上进行finetuning几个epoch，这时候需要对checkpoint global_step参数造假防止学习率不会太小。

### checkpoint averaging

论文提到每10分钟保存一个checkpoint，选择last 20个checkpoint做avg。

作者实验说，一个小时保存一个效果好。

![](https://www.tzer.top/usr/uploads/2021/11/2644856262.png)

模型平均确实可以提高BLEU值。

可以通过评估结果根据策略自动保存checkpoint

## WMT17 比较

![](https://www.tzer.top/usr/uploads/2021/11/3430969598.png)

采用transformer big，在8卡上训练8天。在评分上Transformer big都保持最优

## 结论

In sum, experiments done for this article took about 4 years of GPU time.

这片文章和[2018 ACL The Best of Both Worlds: Combining Recent Advances in Neural Machine 翻译笔记](https://www.tzer.top/index.php/archives/146/)看完之后应该会对transformer模型有一些实践的理解。

tensor2tensor框架2020年已经不维护了，代码量大，较为复杂，opennmt-tf 2代码简单，专注在机器翻译，社区比较活跃，使用tf 2。fairseq由于模型不断增多。代码量也逐渐增大。可以从之前版本看起。
