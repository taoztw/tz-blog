---
title: WMT2019-新闻-Facebook
date: 2021-08-16
lastmod: 2021-08-16
tags: ['AI']
draft: false
summary: 这篇文章介绍了一个机器翻译系统,主要针对英德和英俄翻译方向。系统采用了大规模反向翻译、数据过滤、模型集成和噪声信道模型重排等技术,相比2018年提升了4.5个BLEU分。文章详细描述了数据预处理、模型训练和解码等各个环节的具体做法。
layout: PostSimple
---

## Abstract

英德 和 英俄 方向

baseline：large BPE-based transformer models which rely on sampled back translations.

测试了不同的双语数据过滤方案，增加了过滤的回译数据。模型集成 和 IND数据微调。decoding with noisy channel model reranking。

2019相较于2018提升了4.5BLEU points

## Introduction

证明了反向翻译利用高质量的单语数据是非常重要的。

反向翻译了Newscrawl dataset；还对噪音更大的Commoncrawl数据集的反向翻译部分进行了实验。

> Intelligent selection of language model training data

提升了4.5个BLEU points，Some of these gains can be attributed to differences in dataset quality, but we believe most of the improvement comes from **larger models**, **larger scale back-translation**, and **noisy channel model reranking with strong channel and language models**

## Data

### Data preprocessing

```markdown
en-de方向
normalize punctuation
Moses tokenizer
BPE 32k(joint byte pair encodings)

en-ru
BPE 24k for each language
对于英俄来说，使用separate BPE encoding比joint BPE效果更好
```

### Data filtering

互联网上抓取的数据集是非常嘈杂的。

```markdown
# bitext

1. langid：尽管langid不是最准确的，但是它可以过滤掉包含很多垃圾符号的非常嘈杂的句子。
2. 删除句子tokens长度大于250，并且source/target长度比超过1.5

这两个方法一共过滤掉了30%的数据。

# Monolingual

单语也适用langid进行过滤。
因为英俄方向俄语数据远少于英语或德语。为了从大的单语语料库Commoncrawl中选择高质量的领域数据，使用了Intelligent selection of language model training data方法。
```

![](https://tzer.top/usr/uploads/2021/08/3100395875.png)

## System Overview

```markdown
# base system

FAIRSEQ big Transformer
提高了embed dimension,FFN size, number of heads, number of layers，当保持网络大小可训练的情况下(while maintaining a manageable network size)，发现FFN size=8192有一些提升，后续的模型都使用larger FFN Transformer(FNN=8192)

# large-scale back-translation

通过训练target-to-source system去翻译单语数据，结合人工数据一起训练

> Understanding Back-Translation at Scale

we used back-translations obtained by sampling from an en-semble of three target-to-source models.
发现集成模型要比单个模型的效果要好。
发现 1:1的真假数据性能最好。

# back-translating commoncrawl

俄语的新闻领域数据很少，所以使用领域数据过滤方法从大的开源数据集commoncrawl中提取与Newscrawl相似的数据。 We select a cutoff of 0.01, and use all sentences that score higher than this value for back-translation, or about 5% of the entire dataset

> Intelligent selection of language model training data
```

**选择的方法**

设领域数据$I$,非领域数据$N$,我们的目标是从$N$找到$N_I$,$N_1$与$I$的分布相似。对于给定的句子$s$,可以计算s从$N_I$中抽取的概率，

![](https://tzer.top/usr/uploads/2021/08/3692238627.png)

$P(N_I|N)$是一个常数可以忽略，加log项，$log{P(N_I|s,N)}=logP(s|I)-logP(s|N)$ 公式中将$N_I$替换为$I$,我们假设了它们的分布是相同的。or after normalizing for length (sentence)$H_I(s)-H_N(s)$，我们可以根据$I,N$训练语言模型$L_I,L_N$，那么$H_I(s),H_N(s)$是句子s的word-normalized交叉熵评分，

```markdown
# fine-tuning

在bitext and back-translated data训练完成后，又在少量领域数据进行了训练。（使用了之前几年的测试集，没有说微调多少步）

# noisy channel model rerank

target sequence y and source x
```

![](https://tzer.top/usr/uploads/2021/08/1206413963.png)

$P(x)$是一个常数，剩下的$P(y|x),P(x|y),P(y)$分别是forward model， channel model和language model。

![](https://tzer.top/usr/uploads/2021/08/2775833039.png)

权重选择0-2，长度惩罚选择0-1 （1，2都不包含）

使用Beam size=50 选择最好的 n-best，然后使用语言模型和信道模型根据wight和length penalty选择best one。

![](https://tzer.top/usr/uploads/2021/08/3196993787.png)

```markdown
# postprocessing

后处理替换了固定的标点符号 quotation marks (“ ... ”) en
quotation marks ( ” ... “). de
```
