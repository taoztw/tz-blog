---
title: LLaMA_记录
date: 2023-04-26
lastmod: 2023-04-26
tags: ['AI']
draft: false
summary: 这篇文章介绍了LLaMA模型的训练方法和性能评估结果。LLaMA采用开源数据训练了7B到65B参数的模型,在多个任务上表现优异,13B模型超过GPT-3,65B模型与PaLM 540B相当。文章还指出,在给定计算预算下,在更多数据上训练较小模型比训练最大模型能获得更好的性能。
layout: PostSimple
---

使用开源数据,训练llama-7B- 65B

LLaMA-13B 比GPT3好。65B和Chinchilla / PaLM 540B差不多。

对于给定的计算预算，最佳性能不是最大的模型实现的，而是在更多的数据上训练较小的模型。

> Training Compute-Optimal Large Language Models
>
> scaling laws

## 方法

### 2.1 预训练数据

<img
  src="https://tz-1256822507.cos.ap-hongkong.myqcloud.com/typora/image-20230407110859656.png"
  alt="image-20230407110859656"
/>

除了Wikipedia和Books数据外，每个token在训练中只使用一次。

Wikipedia和Books训练了2 epochs

### 2.2 架构

对原始transformer进行的优化

pre-normalizaiton： 对sub-layer的输入进行norm而不是输出。

swiGLU activation [PaLM]

Rotary Embedding [GPTNeo] Su et al.(2021)

![image-20230407111746183](https://tz-1256822507.cos.ap-hongkong.myqcloud.com/typora/image-20230407111746183.png)

优化器：AdamW， $β_1=0.9,β_2=0.95$

cosine learning rate schedule: final learning rate是最大学习率的10%， wight decay 0.1, gradient clipping 1.0, 2000 warmup,

在提高训练速度上进行了优化。xformers, all_reduce

65B模型， 380 tokens/sec/GPU on 2048 A100 GPU with 80GB of RAM.

训练21天。

## 3 结果

### 常识推理

![image-20230407113815376](https://tz-1256822507.cos.ap-hongkong.myqcloud.com/typora/image-20230407113815376.png)

### QA

![image-20230407113917884](https://tz-1256822507.cos.ap-hongkong.myqcloud.com/typora/image-20230407113917884.png)

### 阅读理解

![image-20230407113958439](https://tz-1256822507.cos.ap-hongkong.myqcloud.com/typora/image-20230407113958439.png)

### 数学推理

![image-20230407114148983](https://tz-1256822507.cos.ap-hongkong.myqcloud.com/typora/image-20230407114148983.png)

### 代码生成

![image-20230407114326769](https://tz-1256822507.cos.ap-hongkong.myqcloud.com/typora/image-20230407114326769.png)

### 大规模多任务语言理解

![image-20230407114445365](https://tz-1256822507.cos.ap-hongkong.myqcloud.com/typora/image-20230407114445365.png)

### 训练过程中的loss

![image-20230407114843709](https://tz-1256822507.cos.ap-hongkong.myqcloud.com/typora/image-20230407114843709.png)

![image-20230407141446443](https://tz-1256822507.cos.ap-hongkong.myqcloud.com/typora/image-20230407141446443.png)

> 指令微调的结果没放
