---
title: Atman机器翻译模型笔记
date: 2021-08-13
lastmod: 2021-08-13
tags: ['AI']
draft: false
summary: 这篇文章主要介绍了机器翻译模型的训练过程和数据处理方法。文章详细描述了从大规模语料训练基础模型到领域精调的完整流程,包括使用平行语料和单语回译语料进行训练,以及采用联合训练、EWC约束等技术来优化模型性能。此外,文章还讨论了语料清洗和数据增强的方法,如使用语言模型、句对相似度和统计规则进行数据筛选,以及通过反向翻译和对偶学习等技术来扩充训练数据。
layout: PostSimple
---

### Atman

```markdown
# 数据

大规模的通用领域语料 + 领域内语料（平行 和 单语）

# 训练

1. 使用 所有平行语料 训练两个基础模型（英中 中英）
2. 使用 所有平行预料和单语回译语料 继续训练
3. 使用联合训练方法训练数轮 微软：Achieving Human Parity on Automatic Chinese to English News Translation
4. 精调阶段：领域内的数据训练，EWC约束确保对基础模型影响较大的参数不要变太多。然后再进行精调，在每一轮精调过程中，随机采样一些句子进行翻译，用人工评判翻译质量，找到其中的翻译问题。然后，从训练集中找出与有特定问题的翻译句子相似的语料来精调模型，直到这些翻译问题得以缓解。（加入MedDRA术语资源来约束翻译解码，确保关键字翻译的准确和一致）
   Progressive NN ：Progressive Neural Networks 渐进式神经网络https://www.cnblogs.com/zeze/p/8268388.html
   pathnet：是progressive nn的generalization。一开始我们每一层都有N个候选模块，对每一个potential的网络，每一层可以使用k个模块。然后我们随机若干个网络（也就是path，如何连接这些模块），然后训练若干episodes。训练了之后，我们用遗传算法，把不好的path都淘汰，留下好的path，然后对这些path进行变异，然后继续训练。最后我们就能获得一个很好的path。
   EWC约束：DeepMind：Overcoming catastrophic forgetting in neural networks

5. 多模型融合 和 重排序技术 进一步增强翻译效果
```

```markdown
> 语料问题：双语对齐，领域内外数据，单语质量
> 错别字，乱码，语法错误，不符合所属语言或领域表达习惯
> 如何判断是领域数据？

数据越大、质量越好，模型翻译效果就会越好

# Atman语料清洗

语言模型、句对相似度、对齐模型、统计规则
**语言模型**：
进行打分
**句对相似度**：
将每一对训练语料的对调（目标语言句子改作源语言句子，原来的源语言句子作为目标语言句子）也加入训练中去，利用某个领域高质量的平行语料训练一个神经机器双向翻译模型，让编码器学习把源语言句子和目标语言句子都压缩到同一个向量空间中去，然后计算语义相似度。
**对齐模型**：
双语语料词对齐关系，一句英到中的句对翻译，英文单词数是10，中文单词数是8，如果中文中的8个词对应到了英文中的5个词，那对齐比例就是1/2（5：10），那这个比例就偏小，
数据利用效率？
**统计规则**：1. 句子中词序列的长度比例 2. 单词的长度，如果单词的长度过长，那么就认为该句话空格丢失 3. 句子中每个词的重复次数，如果某个词在一句话汇总重复次数过多，则可认为不符合表达习惯 4. 句子中关键词（数字，URL等）是否对应出现。
假定这些规则的取值服从某个高斯分布，通过计算这些取值的**均值**和**方差**确定分布，并将某个置信区间之外的数据当做异常点（噪音数据）。这种方法省去了手动设置阈值的烦恼和纠结，概率分布较小的规则取值自动被认为是噪音数据。

# Atman数据增强

举个例子，Amoxicillin（阿莫西林），翻译模型若没见过或只见过特别少的相关训练语料，它就可能不知道这个词如何翻译，这就是语料数量不足的情况。
我们可以找一些相似的语料来补充训练集，加强训练，也可以造一批假数据，比如替换其他句子中的药名为阿莫西林给模型训练。
**BT**：
在英中领域：我们使用现有语料训练中英，然后翻译中文单语语料为英文。获得一批平行语料。
通过反向模型造一批平行语料，虽然其源语言句子质量不够好，但其目标语言句子是一个正常的句子。这样，模型学习的方向还是朝着一个正常表达的句子的目标来进行的。通过BT方法，语料的丰富程度得到了增强，进而使得模型效果得到大幅提升。
**联合训练**： 半监督方式
对back translation的一种扩展。1. 使用双语训练 英中 中英 模型 2. 使用中英模型将中文单语数据翻译为英文，获得英中伪双语语料，训练 英中 模型 3. 使用 英中 将英文单语语料翻译为 中文，获得中英伪双语语料，训练 中英 模型 4. 如此交替反复
**对偶学习**： 非监督形式
利用两个语言的单语语料交替训练两个方向的对偶模型。1. 先训练英中和中英两个方向的基线模型。2. 使用英中基础模型将一个英文单语句子以采样的方式翻译成数个中文句子 3. 利用中英基础模型计算将这些中文句子翻译回原始的英文句子的损失（loss），对英中基础模型做提升。
使其翻译过程中尽量不丢失信息，提高翻译模型的忠实度。中英方向模型修正过程类似，这样两个模型不断交替更新。
```
