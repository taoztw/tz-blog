---
title: CCMT2020-OPPO
date: 2021-08-15
lastmod: 2021-08-15
tags: ['AI']
draft: false
summary: 这篇文章主要介绍了OPPO在CCMT 2020机器翻译比赛中的系统设计和发现。OPPO在7个任务方向中的6个排名第一,主要采用了Transformer模型,并使用了回译、领域微调、知识蒸馏等技术。文章还发现,在低资源语料上简单应用不同的中文分词工具,可以在多个任务中带来明显的性能提升。
layout: PostSimple
---

> Description and Findings of OPPO’s Machine Translation Systems for CCMT 2020

在我们参与的7个方向中，我们在6个任务中排名第一，在其他任务中排名第二。发现简单地在低资源语料库上应用不同的中文分词工具，可以在不同的任务中带来明显的好处。

## Introduction

```markdown
task：1. 英汉双向翻译；使用Transformer model，使用规则和模型去清洗语料，并且使用了回译，领域微调，知识蒸馏，重排序。2. 日英翻译专利领域。使用多语言翻译的解决方案。和去年的解决方案不同。论文：OPPO Machine Translation System 3. 中国少数名族语言翻译为普通话，此外，惊奇地发现，对语料库进行一个简单的额外预处理可以为模型带来很大的好处。中文分词结果改进系统。
```

## Applying Multiple Word Segmentation Tools

```markdown
在少数名族语言翻译为普通话的任务下，使用不同的分词工具jieba和pkuseg，和分字对中文句子进行分词，这样平行语料的数量就增加了三倍。
去除了BPE分词中存在的 @@符号（也就是后缀），因为bpe sub-token和真实的token存在相同的含义，比如国际@@ 贸易中国际和国际@@是相同的，所以区分是没有必要的。
在删除BPE足够之后，我们再次迭代所有子词:对于已删除后缀的给定子词，如果词汇表中没有相同的完整词，那么我们将其再次分解为字符。这种决策背后的直觉是，我们认为这样的模型可以从字符级语料库中学习到更多的信息。

但是将这样的技术应用在英中这样有丰富平行语料的任务重，对模型效果没有好处也没有坏处，所以这个技术在低资源翻译场景下比较有用。
```

![image-20210810152537657.png](http://tzer.top/usr/uploads/2021/08/657223953.png)

## English ↔ Chinese Machine Translation Task

```markdown
数据预处理分成了两个阶段：规范化和过滤

# preprocessing part

- 繁体中文转化为简体
- 标点符号标准化；连字符hyphens，对于中文，全角转半角(不仅仅是标点符号，还有数字和拉丁字母)除了常用的标点符号，如句号、逗号、问号和感叹号。
- 中文分词，使用pkuseg
- Tokenization 使用Moses
- True casing for English，Moses

# filtering part

- 去掉包含过多的无意义(non-sense)符号
- 包含words过长160
- Remove the sentence pairs that the count difference between numbers in Chinese side and numbers in English side is greater than or equal to 3. 英语数字差距超过大于等于3
- 英中句子punctuations标点符号数目大于等于5
- 英汉长度比：2.2 - 0.7
- 去重
- 对齐过滤，对于句子级信息，使用前向后向对齐评分求均值；对于单词级信息，使用句子长度平均前后向评分。句子级：-16，单词级-2.5
  > A simple, fast, and effective reparameterization of ibm model 2.

17M --> 14M

# model training

联合中英数据来训练32k的bpe模型。并且分别建立了两种语言的词表。
使用8 heads transformer-big
对于中英任务，使用不同超参数训练模型以获得集成模型，学习率0.0003 to 0。0008，warmup steps fixed at 16000，dropout ranged from 0.2 to 0.3
对于英中任务，learning tate was 0.0003,warmup steps 15000, feedforward network dimension=15000.

## back translation

首先在训练语料上训练翻译模型，然后翻译target单语数据，组合真伪数据训练机器翻译模型。尽管一些论文上说使用sampling加入一些噪音会提高模型训练结果，但是在作者的实验中argmax-based beam search 仍然表现最好。
并且作者发现使用翻译模型翻译source language 仍然能提升模型的性能。
训练出来模型之后，进行集成，使用集成模型再一次back & forward translate 单语数据 （Ensemble distillation for neural machine translation）
并且迭代了三轮
Iterative back-translation for neural machine translation

## domain adaptation

两阶段微调方法，首先在大赛官方提供的数据上做第一次微调，
第二次微调是在一个很小的数据集上。

## ensemble

对于一个任务，使用不同的超参数或者random seeds训练几个模型，然后进行集成。实验表明集成模型在很多案例都可以提高结果。

## reranking

从集成模型中生成最好的10个候选集合，对这10个候选通过小模型进行打分。使用K-batched MIRA去重排序候选集合
forward-translation models（组合成集成模型的模型）
backward-translation models（回译数据生成）
语言模型
对于每一个模型，我们也训练他们的相反模型（将源语言 目标语言交换），丰富评分模型的选择。
Batch tuning strategies for statistical ma-chine translation

## Corpus Filtering Task

...
```

![image-20210810173946361.png](http://tzer.top/usr/uploads/2021/08/3434651949.png)

![image-20210810173955956.png](http://tzer.top/usr/uploads/2021/08/549386938.png)

## Japanese → English Translation Task (Patent Domain)

数据处理：转换日文语料中的CJK字符为日文kanji形式。（日文汉字形式）

官方没有提供日英的平行语料数据集，依赖于两个独立数据集，日中，英中。直接训练日中，中英两个模型结果无法保证翻译的质量。将英中中的中文的翻译为日文，获得日英的平行语料。并且之前实验说明了前向翻译是有效的，将日中中的中文翻译为英文。又可以获得一批语料。

在构建完合成jaen语料库后，我们尝试了一种受启发的多语言机器翻译方法：我们将这个合成数据集与另外两个官方发布的语料库结合起来，在句子的开头添加标签以指示具体的翻译方向，然后 在这个混合的多语言数据集上训练了几个模型。 这个多语言系统将单一模型提高了一点，从 39.5 到 40.5。 然而，集成模型只获得了 0.1 个点的增益。 此外，K-Batched MIRA 的重新排名也带来了 0.4 分的提升。(Google’ s multilingual neural machine translation system: Enabling zero-shot translation)

![image-20210810182657031.png](http://tzer.top/usr/uploads/2021/08/3979894186.png)
