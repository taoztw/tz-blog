<!DOCTYPE html><html lang="zh-cn" class="__variable_587f35 scroll-smooth "><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/2d141e1a38819612-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="https://www.tzer.top/usr/uploads/2021/10/2417897024.png"/><link rel="preload" as="image" href="https://www.tzer.top/usr/uploads/2021/10/782379925.png"/><link rel="preload" as="image" href="https://www.tzer.top/usr/uploads/2021/10/4136876816.png"/><link rel="stylesheet" href="/_next/static/css/b1aaec952e67a960.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/4fdcd319ea1029e9.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/74fec4f6a9c4330b.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/ddb77e5ad3e10c15.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-9be32bb3a62f2e62.js"/><script src="/_next/static/chunks/fd9d1056-00f38689bba1222a.js" async=""></script><script src="/_next/static/chunks/23-131f062755c8defb.js" async=""></script><script src="/_next/static/chunks/main-app-bc5df281fe431266.js" async=""></script><script src="/_next/static/chunks/231-a2a5e21f80783222.js" async=""></script><script src="/_next/static/chunks/173-fb59ec7f2f6147b4.js" async=""></script><script src="/_next/static/chunks/97-7260ee5660d3a094.js" async=""></script><script src="/_next/static/chunks/app/layout-956eb3ba3d426405.js" async=""></script><script src="/_next/static/chunks/app/blog/%5B...slug%5D/page-5201c8be1492bbcb.js" async=""></script><link rel="preload" href="https://www.googletagmanager.com/gtag/js?id=G-JJL8NM5GV2" as="script"/><link rel="preload" href="https://analytics.umami.is/script.js" as="script"/><title>2021_Facebook_AI_BPE对Transformer模型记忆力的影响 | Tz Blog</title><meta name="description" content="这篇文章主要研究了BPE词表大小对Transformer模型记忆能力的影响。实验表明,增加BPE词表大小可以提高模型的记忆能力,原因可能是BPE减少了训练序列的长度。作者通过三个任务验证了这一结论,并排除了其他可能的解释,最终确定序列长度的减少是观察到记忆效果增强的主要因素。"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link rel="canonical" href="https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-10-08-2021_Facebook_AI_BPE%E5%AF%B9Transformer%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BF%86%E5%8A%9B%E7%9A%84%E5%BD%B1%E5%93%8D"/><link rel="alternate" type="application/rss+xml" href="https://tailwind-nextjs-starter-blog.vercel.app/feed.xml"/><meta property="og:title" content="2021_Facebook_AI_BPE对Transformer模型记忆力的影响"/><meta property="og:description" content="这篇文章主要研究了BPE词表大小对Transformer模型记忆能力的影响。实验表明,增加BPE词表大小可以提高模型的记忆能力,原因可能是BPE减少了训练序列的长度。作者通过三个任务验证了这一结论,并排除了其他可能的解释,最终确定序列长度的减少是观察到记忆效果增强的主要因素。"/><meta property="og:url" content="https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-10-08-2021_Facebook_AI_BPE%E5%AF%B9Transformer%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BF%86%E5%8A%9B%E7%9A%84%E5%BD%B1%E5%93%8D"/><meta property="og:site_name" content="Tz Blog"/><meta property="og:locale" content="en_US"/><meta property="og:image" content="https://tailwind-nextjs-starter-blog.vercel.app/static/images/twitter-card.png"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2021-10-08T00:00:00.000Z"/><meta property="article:modified_time" content="2021-10-08T00:00:00.000Z"/><meta property="article:author" content="Tz"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="2021_Facebook_AI_BPE对Transformer模型记忆力的影响"/><meta name="twitter:description" content="这篇文章主要研究了BPE词表大小对Transformer模型记忆能力的影响。实验表明,增加BPE词表大小可以提高模型的记忆能力,原因可能是BPE减少了训练序列的长度。作者通过三个任务验证了这一结论,并排除了其他可能的解释,最终确定序列长度的减少是观察到记忆效果增强的主要因素。"/><meta name="twitter:image" content="https://tailwind-nextjs-starter-blog.vercel.app/static/images/twitter-card.png"/><meta name="next-size-adjust"/><link rel="apple-touch-icon" sizes="76x76" href="/static/favicons/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/static/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/static/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" href="/static/favicons/favicon.ico"/><link rel="manifest" href="/static/favicons/site.webmanifest"/><link rel="mask-icon" href="/static/favicons/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#000000"/><meta name="theme-color" media="(prefers-color-scheme: light)" content="#fff"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#000"/><link rel="alternate" type="application/rss+xml" href="/feed.xml"/><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="scrollbar-track-slate-400 bg-background pl-[calc(100vw-100%)] antialiased "><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><section class="mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0"><div class="flex h-screen flex-col justify-between font-sans"><header class="fixed inset-x-0 left-[calc(100vw-100%)] top-4 z-40 mx-8 flex h-[60px] items-center justify-between rounded-3xl border border-border bg-card px-4 shadow-sm saturate-100 backdrop-blur-[4px] transition-all duration-200 header-md:mx-auto header-md:max-w-[768px] header-md:px-8 header-lg:max-w-[1168px] false"><div class="mx-auto flex h-[60px] w-full items-center justify-between"><div><a aria-label="" href="/"><div class="flex items-center justify-between"><div class="mr-3"><img alt="The Blog Logo" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" style="color:transparent" src="/static/logo.svg"/></div><div class="ml-0 inline-flex max-w-full items-center rounded-full border border-primary-500 bg-transparent px-2.5 py-0.5 text-xs font-semibold text-primary backdrop-blur-md transition-colors duration-150 focus:outline-none focus:ring-2 focus:ring-lime-400 focus:ring-offset-2 sm:ml-0 md:ml-0"><div class="mr-1 flex aspect-square h-[14px] w-[14px] animate-pulse rounded-full bg-green-500/50 dark:bg-green-400/50"></div><span class="inline whitespace-nowrap">Innovating</span></div><div class="hidden h-6 text-2xl font-semibold sm:block"></div></div></a></div><div class="group flex items-center justify-center space-x-3"><a class="group hidden w-max rounded-md px-4 py-2 font-body text-sm  font-bold text-gray-900 transition-colors hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground  focus:outline-none disabled:pointer-events-none disabled:opacity-50 data-[active]:bg-accent/50 data-[state=open]:bg-accent/50 dark:text-gray-100 dark:hover:text-lime-400 sm:block" href="/blog">Blog</a><a class="group hidden w-max rounded-md px-4 py-2 font-body text-sm  font-bold text-gray-900 transition-colors hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground  focus:outline-none disabled:pointer-events-none disabled:opacity-50 data-[active]:bg-accent/50 data-[state=open]:bg-accent/50 dark:text-gray-100 dark:hover:text-lime-400 sm:block" href="/tags">Tags</a><a class="group hidden w-max rounded-md px-4 py-2 font-body text-sm  font-bold text-gray-900 transition-colors hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground  focus:outline-none disabled:pointer-events-none disabled:opacity-50 data-[active]:bg-accent/50 data-[state=open]:bg-accent/50 dark:text-gray-100 dark:hover:text-lime-400 sm:block" href="/projects">Projects</a><a class="group hidden w-max rounded-md px-4 py-2 font-body text-sm  font-bold text-gray-900 transition-colors hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground  focus:outline-none disabled:pointer-events-none disabled:opacity-50 data-[active]:bg-accent/50 data-[state=open]:bg-accent/50 dark:text-gray-100 dark:hover:text-lime-400 sm:block" href="/about">About</a><button aria-label="Search"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="h-6 w-6 text-gray-900 dark:text-gray-100"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-5.197-5.197m0 0A7.5 7.5 0 105.196 5.196a7.5 7.5 0 0010.607 10.607z"></path></svg></button><button id="theme-btn" aria-label="Toggle Dark Mode" type="button" class="ml-1 mr-1 h-8 w-8 rounded p-1 " tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="h-6 w-6 text-gray-900 dark:text-gray-100"><path d="M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001 0 1010.586 10.586z"></path></svg></button><button aria-label="Toggle Menu" class="sm:hidden"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="h-8 w-8 text-gray-900 dark:text-gray-100"><path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"></path></svg></button><div class="fixed left-0 top-0 z-10 h-full w-full transform bg-white opacity-95 duration-300 ease-in-out dark:bg-gray-950 dark:opacity-[0.98] invisible opacity-0"><div class="flex justify-end"><button class="mr-8 mt-11 h-8 w-8" aria-label="Toggle Menu"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="text-gray-900 dark:text-gray-100"><path fill-rule="evenodd" d="M4.293 4.293a1 1 0 011.414 0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></button></div><nav class="fixed mt-8 h-full"><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100" href="/">Home</a></div><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100" href="/blog">Blog</a></div><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100" href="/tags">Tags</a></div><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100" href="/projects">Projects</a></div><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100" href="/about">About</a></div></nav></div></div></div></header><main class="mb-auto pt-20"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"2021_Facebook_AI_BPE对Transformer模型记忆力的影响","datePublished":"2021-10-08T00:00:00.000Z","dateModified":"2021-10-08T00:00:00.000Z","description":"这篇文章主要研究了BPE词表大小对Transformer模型记忆能力的影响。实验表明,增加BPE词表大小可以提高模型的记忆能力,原因可能是BPE减少了训练序列的长度。作者通过三个任务验证了这一结论,并排除了其他可能的解释,最终确定序列长度的减少是观察到记忆效果增强的主要因素。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-10-08-2021_Facebook_AI_BPE对Transformer模型记忆力的影响","author":[{"@type":"Person","name":"Tz"}]}</script><section class="mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0"><div class="fixed bottom-8 right-8 hidden flex-col gap-3 md:hidden"><button aria-label="Scroll To Comment" class="rounded-full bg-gray-200 p-2 text-gray-500 transition-all hover:bg-gray-300 dark:bg-gray-700 dark:text-gray-400 dark:hover:bg-gray-600"><svg class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M18 10c0 3.866-3.582 7-8 7a8.841 8.841 0 01-4.083-.98L2 17l1.338-3.123C2.493 12.767 2 11.434 2 10c0-3.866 3.582-7 8-7s8 3.134 8 7zM7 9H5v2h2V9zm8 0h-2v2h2V9zM9 9h2v2H9V9z" clip-rule="evenodd"></path></svg></button><button aria-label="Scroll To Top" class="rounded-full bg-gray-200 p-2 text-gray-500 transition-all hover:bg-gray-300 dark:bg-gray-700 dark:text-gray-400 dark:hover:bg-gray-600"><svg class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z" clip-rule="evenodd"></path></svg></button></div><article><div><header><div class="space-y-1 border-b border-gray-200 pb-10 text-center dark:border-gray-700"><dl><div><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2021-10-08T00:00:00.000Z">October 8, 2021</time></dd></div></dl><div><h1 class="text-3xl font-extrabold leading-9 tracking-tight text-gray-900 dark:text-gray-100 sm:text-4xl sm:leading-10 md:text-5xl md:leading-14">2021_Facebook_AI_BPE对Transformer模型记忆力的影响</h1></div></div></header><div class="grid-rows-[auto_1fr] divide-y divide-gray-200 pb-8 dark:divide-gray-700 xl:divide-y-0"><div class="divide-y divide-gray-200 dark:divide-gray-700 xl:col-span-3 xl:row-span-2 xl:pb-0"><div class="prose max-w-none pb-8 pt-10 dark:prose-invert"><blockquote><p>HOW BPE AFFECTS MEMORIZATION IN TRANSFORMERS</p></blockquote><p>证明了增加BPE vocab大小可以进一步提高模型的记忆能力。（记忆可能导致数据泄漏，所以需要在更好的时候记忆，在不需要的时候避免记忆。）</p><p>通常，我们认为数据增强，参数量，正则会影响模型记忆。作者通过实验发现BPE词表越大，记忆越强。作者推测原因可能是：bpe减少训练序列的长度</p><p>Their conclusion that it is the sequence length that matters makes intuitive sense. The Transformer architecture internally compares all input token pairs many times and the number of token combinations grows quadratically with the sequence length. <strong>When we imagine the classification as searching for particular relation in the input, it totally makes sense that the fewer possible relations we have, the easier it is to find something.</strong></p><h2 class="content-header" id="学习记忆任务"><a href="#学习记忆任务" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>学习记忆任务</h2><p>如何验证模型的记忆能力，作者提出了三个任务来验证模型的记忆能力。</p><h3 class="content-header" id="学习随机标签映射"><a href="#学习随机标签映射" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>学习随机标签映射</h3><p>学习一个非系统映射的任务，来衡量模型的记忆能力。假设更高的训练准确率意味着更高的记忆能力。使用SNLI（斯坦福自然语言推理数据集）将前提和假设通过<code>::</code> 进行拼接，过滤掉中性的句子。转化为二分类任务。通过随机抽样标签替换原有标签。通过准确率来分析模型的记忆能力。</p><h3 class="content-header" id="成员推理"><a href="#成员推理" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>成员推理</h3><p>度量模型对成员推理攻击的脆弱性来判断模型记忆的程度大小。如果很容易的准确判断一个例子是否用于训练，那我们就可以认为他以某种形式记忆在模型的权重中。（具体方法看原始论文）</p><h3 class="content-header" id="训练数据恢复"><a href="#训练数据恢复" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>训练数据恢复</h3><p>关注问答领域，判断模型对训练数据中的问题的答案生成和原始答案的匹配度。数据集使用PAQ</p><h2 class="content-header" id="模型和超参数"><a href="#模型和超参数" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>模型和超参数</h2><p>作者使用三个Transformer模型，(因果)语言模型LM，masked language model(MLM), 和一个Encoder。在学习随机标签映射和成员推理上使用全部三个架构，问答使用LM。</p><h3 class="content-header" id="bpe-设置"><a href="#bpe-设置" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>BPE 设置</h3><blockquote><p>改变词汇表的大小</p></blockquote><p>对于SNLI数据采用<code>(0.5,1.5,10,20)✖️10^3</code>步，对应的vocab size是611,1097,4943,9574,18336。</p><p>PAQ dataset，<code>(0.5,1.5,10,,15,20)✖️10^3</code>对应1280, 1784, 5784, 10784, 15784, 20776 vocab size</p><h3 class="content-header" id="控制学习参数的数量"><a href="#控制学习参数的数量" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>控制学习参数的数量</h3><p>更改词表大小后，模型参数也会发生变换，为了解决这一影响，作者控制了模型的参数量，将嵌入层替换为一个一个嵌入层和一个全连接层的组合。</p><h3 class="content-header" id="改变模型为分类器"><a href="#改变模型为分类器" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>改变模型为分类器</h3><p>EOS token 映射等方法将模型改变为分类器</p><h3 class="content-header" id="模型和训练细节"><a href="#模型和训练细节" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>模型和训练细节</h3><p>略</p><h2 class="content-header" id="bpe影响记忆实验"><a href="#bpe影响记忆实验" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>BPE影响记忆实验</h2><p><img alt="" src="https://www.tzer.top/usr/uploads/2021/10/2417897024.png"/></p><p>Memorizing random Labels：a,b两图模型随着BPE词表的大小增加，对随机标签拟合的更好。</p><p>Membership inference: c图中看到，(M)LM的检验精度具有不同的增长区域，表明：泛化与记忆并不直接矛盾，有一定程度的粒度可以实现更好的记忆的更好的泛化</p><p><img alt="" src="https://www.tzer.top/usr/uploads/2021/10/782379925.png"/></p><p>Question answer recovery: 任务采用了两个不同的模型，transformer-large和base。可以看出large的效果总比base好，这说明模型大小对记忆力影响较大。并且随着vocab size的提升，准确度也提升。在参数控制上效果也是如此。</p><h2 class="content-header" id="解释"><a href="#解释" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>解释</h2><p>作者发现，BPE学习的词汇量越大，在Transforme模型中记忆能力越强。作者提出三个假设：长度假设(BPE vocab增加使输入序列变短)，冗余假设（BPE是一种压缩算法，对数据进行压缩，模型可以更容易记忆没有冗余的数据），词汇量假设（词汇量增加，在极限情况家，每个序列都可能有一个唯一的标记，标记和标签之间会存在一对一的关系）</p><p>作者进行了实验，去除了冗余假设。如下图a在不压缩数据上，vocab越大，记忆越强。</p><p><img alt="" src="https://www.tzer.top/usr/uploads/2021/10/4136876816.png"/></p><p>作者通过对两个新token替换一个等概率替换一个旧token，增大了两倍词汇量，如上图b，观测出vocab增加相对较大，记忆力增强效果，并且在一开始对记忆力是有害的。去除词汇量假设。</p><p>排除法：序列长度的减少是观察到记忆效果的主要因素。</p></div></div><div class="pb-6 pt-6 text-center text-gray-700 dark:text-gray-300" id="comment"><button>Load Comments</button></div><footer><div class="flex flex-col text-sm font-medium sm:flex-row sm:justify-between sm:text-base"><div class="pt-4 xl:pt-8"><a class="dark:hover:text-primary-400 text-primary-500 hover:text-primary-600" aria-label="Previous post: 2018_ACL_迭代回译" href="/blog/AI/2021-09-29-2018_ACL_迭代回译">← <!-- -->2018_ACL_迭代回译</a></div><div class="pt-4 xl:pt-8"><a class="dark:hover:text-primary-400 text-primary-500 hover:text-primary-600" aria-label="Next post: 为什么使用self-attention,机器翻译下的评估" href="/blog/AI/2021-10-09-为什么使用self-attention,机器翻译下的评估">为什么使用self-attention,机器翻译下的评估<!-- --> →</a></div></div></footer></div></div></article></section></main><footer><div class="mt-16 flex flex-col items-center"><div class="mb-3 flex space-x-4"><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="mailto:tztw4723@gmail.com"><span class="sr-only">mail</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" class="dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6"><title>Mail</title><path d="M2.003 5.884L10 9.882l7.997-3.998A2 2 0 0016 4H4a2 2 0 00-1.997 1.884z"></path><path d="M18 8.118l-8 4-8-4V14a2 2 0 002 2h12a2 2 0 002-2V8.118z"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://github.com/taoztw"><span class="sr-only">github</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6"><title>Github</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://facebook.com"><span class="sr-only">facebook</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6"><title>Facebook</title><path d="M24 12.073c0-6.627-5.373-12-12-12s-12 5.373-12 12c0 5.99 4.388 10.954 10.125 11.854v-8.385H7.078v-3.47h3.047V9.43c0-3.007 1.792-4.669 4.533-4.669 1.312 0 2.686.235 2.686.235v2.953H15.83c-1.491 0-1.956.925-1.956 1.874v2.25h3.328l-.532 3.47h-2.796v8.385C19.612 23.027 24 18.062 24 12.073z"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://youtube.com"><span class="sr-only">youtube</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6"><title>Youtube</title><path d="M23.499 6.203a3.008 3.008 0 00-2.089-2.089c-1.87-.501-9.4-.501-9.4-.501s-7.509-.01-9.399.501a3.008 3.008 0 00-2.088 2.09A31.258 31.26 0 000 12.01a31.258 31.26 0 00.523 5.785 3.008 3.008 0 002.088 2.089c1.869.502 9.4.502 9.4.502s7.508 0 9.399-.502a3.008 3.008 0 002.089-2.09 31.258 31.26 0 00.5-5.784 31.258 31.26 0 00-.5-5.808zm-13.891 9.4V8.407l6.266 3.604z"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com"><span class="sr-only">linkedin</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6"><title>Linkedin</title><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://twitter.com/x"><span class="sr-only">x</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6"><title>X</title><path d="M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 6.932ZM17.61 20.644h2.039L6.486 3.24H4.298Z"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://www.instagram.com"><span class="sr-only">instagram</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6"><title>Instagram</title><path d="M12 0C8.74 0 8.333.015 7.053.072 5.775.132 4.905.333 4.14.63c-.789.306-1.459.717-2.126 1.384S.935 3.35.63 4.14C.333 4.905.131 5.775.072 7.053.012 8.333 0 8.74 0 12s.015 3.667.072 4.947c.06 1.277.261 2.148.558 2.913.306.788.717 1.459 1.384 2.126.667.666 1.336 1.079 2.126 1.384.766.296 1.636.499 2.913.558C8.333 23.988 8.74 24 12 24s3.667-.015 4.947-.072c1.277-.06 2.148-.262 2.913-.558.788-.306 1.459-.718 2.126-1.384.666-.667 1.079-1.335 1.384-2.126.296-.765.499-1.636.558-2.913.06-1.28.072-1.687.072-4.947s-.015-3.667-.072-4.947c-.06-1.277-.262-2.149-.558-2.913-.306-.789-.718-1.459-1.384-2.126C21.319 1.347 20.651.935 19.86.63c-.765-.297-1.636-.499-2.913-.558C15.667.012 15.26 0 12 0zm0 2.16c3.203 0 3.585.016 4.85.071 1.17.055 1.805.249 2.227.415.562.217.96.477 1.382.896.419.42.679.819.896 1.381.164.422.36 1.057.413 2.227.057 1.266.07 1.646.07 4.85s-.015 3.585-.074 4.85c-.061 1.17-.256 1.805-.421 2.227-.224.562-.479.96-.899 1.382-.419.419-.824.679-1.38.896-.42.164-1.065.36-2.235.413-1.274.057-1.649.07-4.859.07-3.211 0-3.586-.015-4.859-.074-1.171-.061-1.816-.256-2.236-.421-.569-.224-.96-.479-1.379-.899-.421-.419-.69-.824-.9-1.38-.165-.42-.359-1.065-.42-2.235-.045-1.26-.061-1.649-.061-4.844 0-3.196.016-3.586.061-4.861.061-1.17.255-1.814.42-2.234.21-.57.479-.96.9-1.381.419-.419.81-.689 1.379-.898.42-.166 1.051-.361 2.221-.421 1.275-.045 1.65-.06 4.859-.06l.045.03zm0 3.678c-3.405 0-6.162 2.76-6.162 6.162 0 3.405 2.76 6.162 6.162 6.162 3.405 0 6.162-2.76 6.162-6.162 0-3.405-2.76-6.162-6.162-6.162zM12 16c-2.21 0-4-1.79-4-4s1.79-4 4-4 4 1.79 4 4-1.79 4-4 4zm7.846-10.405c0 .795-.646 1.44-1.44 1.44-.795 0-1.44-.646-1.44-1.44 0-.794.646-1.439 1.44-1.439.793-.001 1.44.645 1.44 1.439z"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://www.threads.net"><span class="sr-only">threads</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6"><title>Threads</title><path d="M12.186 24h-.007c-3.581-.024-6.334-1.205-8.184-3.509C2.35 18.44 1.5 15.586 1.472 12.01v-.017c.03-3.579.879-6.43 2.525-8.482C5.845 1.205 8.6.024 12.18 0h.014c2.746.02 5.043.725 6.826 2.098 1.677 1.29 2.858 3.13 3.509 5.467l-2.04.569c-1.104-3.96-3.898-5.984-8.304-6.015-2.91.022-5.11.936-6.54 2.717C4.307 6.504 3.616 8.914 3.589 12c.027 3.086.718 5.496 2.057 7.164 1.43 1.783 3.631 2.698 6.54 2.717 2.623-.02 4.358-.631 5.8-2.045 1.647-1.613 1.618-3.593 1.09-4.798-.31-.71-.873-1.3-1.634-1.75-.192 1.352-.622 2.446-1.284 3.272-.886 1.102-2.14 1.704-3.73 1.79-1.202.065-2.361-.218-3.259-.801-1.063-.689-1.685-1.74-1.752-2.964-.065-1.19.408-2.285 1.33-3.082.88-.76 2.119-1.207 3.583-1.291a13.853 13.853 0 0 1 3.02.142c-.126-.742-.375-1.332-.75-1.757-.513-.586-1.308-.883-2.359-.89h-.029c-.844 0-1.992.232-2.721 1.32L7.734 7.847c.98-1.454 2.568-2.256 4.478-2.256h.044c3.194.02 5.097 1.975 5.287 5.388.108.046.216.094.321.142 1.49.7 2.58 1.761 3.154 3.07.797 1.82.871 4.79-1.548 7.158-1.85 1.81-4.094 2.628-7.277 2.65Zm1.003-11.69c-.242 0-.487.007-.739.021-1.836.103-2.98.946-2.916 2.143.067 1.256 1.452 1.839 2.784 1.767 1.224-.065 2.818-.543 3.086-3.71a10.5 10.5 0 0 0-2.215-.221z"></path></svg></a></div><div class="mb-2 flex space-x-2 text-sm text-gray-500 dark:text-gray-400"><div>Tz</div><div> • </div><div>© 2024</div><div> • </div><a target="_blank" rel="noopener noreferrer" href="https://beian.miit.gov.cn/">京ICP备2023010160号</a></div><div class="mb-8 text-sm text-gray-500 dark:text-gray-400"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timlrx/tailwind-nextjs-starter-blog">Tailwind Nextjs Theme</a></div></div></footer></div></section><script src="/_next/static/chunks/webpack-9be32bb3a62f2e62.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/2d141e1a38819612-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/b1aaec952e67a960.css\",\"style\"]\n3:HL[\"/_next/static/css/4fdcd319ea1029e9.css\",\"style\"]\n4:HL[\"/_next/static/css/74fec4f6a9c4330b.css\",\"style\"]\n5:HL[\"/_next/static/css/ddb77e5ad3e10c15.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"6:I[5751,[],\"\"]\n9:I[9275,[],\"\"]\nb:I[1343,[],\"\"]\nc:I[4404,[\"231\",\"static/chunks/231-a2a5e21f80783222.js\",\"173\",\"static/chunks/173-fb59ec7f2f6147b4.js\",\"97\",\"static/chunks/97-7260ee5660d3a094.js\",\"185\",\"static/chunks/app/layout-956eb3ba3d426405.js\"],\"GoogleAnalytics\"]\nd:I[8700,[\"231\",\"static/chunks/231-a2a5e21f80783222.js\",\"173\",\"static/chunks/173-fb59ec7f2f6147b4.js\",\"97\",\"static/chunks/97-7260ee5660d3a094.js\",\"185\",\"static/chunks/app/layout-956eb3ba3d426405.js\"],\"ThemeProviders\"]\ne:I[4080,[\"231\",\"static/chunks/231-a2a5e21f80783222.js\",\"173\",\"static/chunks/173-fb59ec7f2f6147b4.js\",\"97\",\"static/chunks/97-7260ee5660d3a094.js\",\"185\",\"static/chunks/app/layout-956eb3ba3d426405.js\"],\"\"]\nf:I[9032,[\"231\",\"static/chunks/231-a2a5e21f80783222.js\",\"173\",\"static/chunks/173-fb59ec7f2f6147b4.js\",\"97\",\"static/chunks/97-7260ee5660d3a094.js\",\"185\",\"static/chunks/app/layout-956eb3ba3d426405.js\"],\"KBarSearchProvider\"]\n10:I[5133,[\"231\",\"static/chunks/231-a2a5e21f80783222.js\",\"173\",\"static/chunks/173-fb59ec7f2f6147b4.js\",\"97\",\"static/chunks/97-7260ee5660d3a094.js\",\"185\",\"static/chunks/app/layout-956eb3ba3d426405.js\"],\"default\"]\n11:I[231,[\"231\",\"static/chunks/231-a2a5e21f80783222.js\",\"173\",\"static/chunks/173-fb59ec7f2f6147b4.js\",\"797\",\"static/chunks/app/blog/%5B...slug%5D/page-5201c8be1492bbcb.js\"],\"\"]\n15:I[6130,[],\"\"]\na:[\"slug\",\"AI/2021-10-08-2021_Facebook_AI_BPE%E5%AF%B9Transformer%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BF%86%E5%8A%9B%E7%9A%84%E5%BD%B1%E5%93%8D\",\"c\"]\n12:T69f,M12 0C8.74 0 8.333.015 7.053.072 5.775.132 4.905.333 4.14.63c-.789.306-1.459.717-2.126 1.384S.935 3.35.63 4.14C.333 4.905.131 5.775.072 7.053.012 8.333 0 8.74 0 12s.015 3.667.072 4.947c.06 1.277.261 2.148.558 2.913.306.788.717 1.459 1.384 2.126.667.666 1.336 1.079 2.126 1.384.766.296 1.636.499 2.913.558C8.333 23.988 8.74 24 12 24s3.667-.015 4.947-.072c1.277-.06 2.148-.262 2.913-.558.788-.306 1.459-.718 2.126-1.384.666-.667 1.079-1.335 1.384-2.126.296-.765.499-1.636.558-2.913.06-1.28.072-1.687.072-4.947s-.015-3.667-.072-4.947c-.06-1.277-.262-2.149-.558-2.913-.306-.789-.718-"])</script><script>self.__next_f.push([1,"1.459-1.384-2.126C21.319 1.347 20.651.935 19.86.63c-.765-.297-1.636-.499-2.913-.558C15.667.012 15.26 0 12 0zm0 2.16c3.203 0 3.585.016 4.85.071 1.17.055 1.805.249 2.227.415.562.217.96.477 1.382.896.419.42.679.819.896 1.381.164.422.36 1.057.413 2.227.057 1.266.07 1.646.07 4.85s-.015 3.585-.074 4.85c-.061 1.17-.256 1.805-.421 2.227-.224.562-.479.96-.899 1.382-.419.419-.824.679-1.38.896-.42.164-1.065.36-2.235.413-1.274.057-1.649.07-4.859.07-3.211 0-3.586-.015-4.859-.074-1.171-.061-1.816-.256-2.236-.421-.569-.224-.96-.479-1.379-.899-.421-.419-.69-.824-.9-1.38-.165-.42-.359-1.065-.42-2.235-.045-1.26-.061-1.649-.061-4.844 0-3.196.016-3.586.061-4.861.061-1.17.255-1.814.42-2.234.21-.57.479-.96.9-1.381.419-.419.81-.689 1.379-.898.42-.166 1.051-.361 2.221-.421 1.275-.045 1.65-.06 4.859-.06l.045.03zm0 3.678c-3.405 0-6.162 2.76-6.162 6.162 0 3.405 2.76 6.162 6.162 6.162 3.405 0 6.162-2.76 6.162-6.162 0-3.405-2.76-6.162-6.162-6.162zM12 16c-2.21 0-4-1.79-4-4s1.79-4 4-4 4 1.79 4 4-1.79 4-4 4zm7.846-10.405c0 .795-.646 1.44-1.44 1.44-.795 0-1.44-.646-1.44-1.44 0-.794.646-1.439 1.44-1.439.793-.001 1.44.645 1.44 1.439z13:T498,M12.186 24h-.007c-3.581-.024-6.334-1.205-8.184-3.509C2.35 18.44 1.5 15.586 1.472 12.01v-.017c.03-3.579.879-6.43 2.525-8.482C5.845 1.205 8.6.024 12.18 0h.014c2.746.02 5.043.725 6.826 2.098 1.677 1.29 2.858 3.13 3.509 5.467l-2.04.569c-1.104-3.96-3.898-5.984-8.304-6.015-2.91.022-5.11.936-6.54 2.717C4.307 6.504 3.616 8.914 3.589 12c.027 3.086.718 5.496 2.057 7.164 1.43 1.783 3.631 2.698 6.54 2.717 2.623-.02 4.358-.631 5.8-2.045 1.647-1.613 1.618-3.593 1.09-4.798-.31-.71-.873-1.3-1.634-1.75-.192 1.352-.622 2.446-1.284 3.272-.886 1.102-2.14 1.704-3.73 1.79-1.202.065-2.361-.218-3.259-.801-1.063-.689-1.685-1.74-1.752-2.964-.065-1.19.408-2.285 1.33-3.082.88-.76 2.119-1.207 3.583-1.291a13.853 13.853 0 0 1 3.02.142c-.126-.742-.375-1.332-.75-1.757-.513-.586-1.308-.883-2.359-.89h-.029c-.844 0-1.992.232-2.721 1.32L7.734 7.847c.98-1.454 2.568-2.256 4.478-2.256h.044c3.194.02 5.097 1.975 5.287 5.388.108.046.216.094.321.142 1.49"])</script><script>self.__next_f.push([1,".7 2.58 1.761 3.154 3.07.797 1.82.871 4.79-1.548 7.158-1.85 1.81-4.094 2.628-7.277 2.65Zm1.003-11.69c-.242 0-.487.007-.739.021-1.836.103-2.98.946-2.916 2.143.067 1.256 1.452 1.839 2.784 1.767 1.224-.065 2.818-.543 3.086-3.71a10.5 10.5 0 0 0-2.215-.221z16:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/b1aaec952e67a960.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/4fdcd319ea1029e9.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}],[\"$\",\"link\",\"2\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/74fec4f6a9c4330b.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L6\",null,{\"buildId\":\"4GpVTI5t3QYsmO2LiU8Xz\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/blog/AI/2021-10-08-2021_Facebook_AI_BPE%E5%AF%B9Transformer%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BF%86%E5%8A%9B%E7%9A%84%E5%BD%B1%E5%93%8D\",\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"AI/2021-10-08-2021_Facebook_AI_BPE%E5%AF%B9Transformer%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BF%86%E5%8A%9B%E7%9A%84%E5%BD%B1%E5%93%8D\",\"c\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":[\\\"AI\\\",\\\"2021-10-08-2021_Facebook_AI_BPE对Transformer模型记忆力的影响\\\"]}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"AI/2021-10-08-2021_Facebook_AI_BPE%E5%AF%B9Transformer%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BF%86%E5%8A%9B%E7%9A%84%E5%BD%B1%E5%93%8D\",\"c\"],{\"children\":[\"__PAGE__\",{},[[\"$L7\",\"$L8\"],null],null]},[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",\"$a\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/ddb77e5ad3e10c15.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]]}],null]},[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"zh-cn\",\"className\":\"__variable_587f35 scroll-smooth \",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"link\",null,{\"rel\":\"apple-touch-icon\",\"sizes\":\"76x76\",\"href\":\"/static/favicons/apple-touch-icon.png\"}],[\"$\",\"link\",null,{\"rel\":\"icon\",\"type\":\"image/png\",\"sizes\":\"32x32\",\"href\":\"/static/favicons/favicon-32x32.png\"}],[\"$\",\"link\",null,{\"rel\":\"icon\",\"type\":\"image/png\",\"sizes\":\"16x16\",\"href\":\"/static/favicons/favicon-16x16.png\"}],[\"$\",\"link\",null,{\"rel\":\"icon\",\"type\":\"image/png\",\"href\":\"/static/favicons/favicon.ico\"}],[\"$\",\"link\",null,{\"rel\":\"manifest\",\"href\":\"/static/favicons/site.webmanifest\"}],[\"$\",\"link\",null,{\"rel\":\"mask-icon\",\"href\":\"/static/favicons/safari-pinned-tab.svg\",\"color\":\"#5bbad5\"}],[\"$\",\"meta\",null,{\"name\":\"msapplication-TileColor\",\"content\":\"#000000\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: light)\",\"content\":\"#fff\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: dark)\",\"content\":\"#000\"}],[\"$\",\"link\",null,{\"rel\":\"alternate\",\"type\":\"application/rss+xml\",\"href\":\"/feed.xml\"}],[\"$\",\"body\",null,{\"className\":\"scrollbar-track-slate-400 bg-background pl-[calc(100vw-100%)] antialiased \",\"children\":[[\"$\",\"$Lc\",null,{\"gaId\":\"G-JJL8NM5GV2\"}],[\"$\",\"$Ld\",null,{\"children\":[[\"$undefined\",\"$undefined\",\"$undefined\",[\"$\",\"$Le\",null,{\"async\":true,\"defer\":true,\"data-website-id\":\"$undefined\",\"src\":\"https://analytics.umami.is/script.js\"}],\"$undefined\",\"$undefined\"],[\"$\",\"section\",null,{\"className\":\"mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex h-screen flex-col justify-between font-sans\",\"children\":[[\"$\",\"$Lf\",null,{\"kbarConfig\":{\"searchDocumentsPath\":\"search.json\"},\"children\":[[\"$\",\"$L10\",null,{}],[\"$\",\"main\",null,{\"className\":\"mb-auto pt-20\",\"children\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"flex flex-col items-start justify-start md:mt-24 md:flex-row md:items-center md:justify-center md:space-x-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"space-x-2 pb-8 pt-6 md:space-y-5\",\"children\":[\"$\",\"h1\",null,{\"className\":\"text-6xl font-extrabold leading-9 tracking-tight text-gray-900 dark:text-gray-100 md:border-r-2 md:px-6 md:text-8xl md:leading-14\",\"children\":\"404\"}]}],[\"$\",\"div\",null,{\"className\":\"max-w-md\",\"children\":[[\"$\",\"p\",null,{\"className\":\"mb-4 text-xl font-bold leading-normal md:text-2xl\",\"children\":\"Sorry we couldn't find this page.\"}],[\"$\",\"p\",null,{\"className\":\"mb-8\",\"children\":\"But dont worry, you can find plenty of other things on our homepage.\"}],[\"$\",\"$L11\",null,{\"href\":\"/\",\"className\":\"focus:shadow-outline-blue inline rounded-lg border border-transparent bg-blue-600 px-4 py-2 text-sm font-medium leading-5 text-white shadow transition-colors duration-150 hover:bg-blue-700 focus:outline-none dark:hover:bg-blue-500\",\"children\":\"Back to homepage\"}]]}]]}],\"notFoundStyles\":[],\"styles\":null}]}]]}],[\"$\",\"footer\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"mt-16 flex flex-col items-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mb-3 flex space-x-4\",\"children\":[[\"$\",\"a\",null,{\"className\":\"text-sm text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"mailto:tztw4723@gmail.com\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"mail\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 20 20\",\"className\":\"dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6\",\"children\":[[\"$\",\"title\",null,{\"children\":\"Mail\"}],[\"$\",\"path\",null,{\"d\":\"M2.003 5.884L10 9.882l7.997-3.998A2 2 0 0016 4H4a2 2 0 00-1.997 1.884z\"}],[\"$\",\"path\",null,{\"d\":\"M18 8.118l-8 4-8-4V14a2 2 0 002 2h12a2 2 0 002-2V8.118z\"}]]}]]}],[\"$\",\"a\",null,{\"className\":\"text-sm text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://github.com/taoztw\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"github\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 24 24\",\"className\":\"dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6\",\"children\":[[\"$\",\"title\",null,{\"children\":\"Github\"}],[\"$\",\"path\",null,{\"d\":\"M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12\"}]]}]]}],[\"$\",\"a\",null,{\"className\":\"text-sm text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://facebook.com\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"facebook\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 24 24\",\"className\":\"dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6\",\"children\":[[\"$\",\"title\",null,{\"children\":\"Facebook\"}],[\"$\",\"path\",null,{\"d\":\"M24 12.073c0-6.627-5.373-12-12-12s-12 5.373-12 12c0 5.99 4.388 10.954 10.125 11.854v-8.385H7.078v-3.47h3.047V9.43c0-3.007 1.792-4.669 4.533-4.669 1.312 0 2.686.235 2.686.235v2.953H15.83c-1.491 0-1.956.925-1.956 1.874v2.25h3.328l-.532 3.47h-2.796v8.385C19.612 23.027 24 18.062 24 12.073z\"}]]}]]}],[\"$\",\"a\",null,{\"className\":\"text-sm text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://youtube.com\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"youtube\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 24 24\",\"className\":\"dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6\",\"children\":[[\"$\",\"title\",null,{\"children\":\"Youtube\"}],[\"$\",\"path\",null,{\"d\":\"M23.499 6.203a3.008 3.008 0 00-2.089-2.089c-1.87-.501-9.4-.501-9.4-.501s-7.509-.01-9.399.501a3.008 3.008 0 00-2.088 2.09A31.258 31.26 0 000 12.01a31.258 31.26 0 00.523 5.785 3.008 3.008 0 002.088 2.089c1.869.502 9.4.502 9.4.502s7.508 0 9.399-.502a3.008 3.008 0 002.089-2.09 31.258 31.26 0 00.5-5.784 31.258 31.26 0 00-.5-5.808zm-13.891 9.4V8.407l6.266 3.604z\"}]]}]]}],[\"$\",\"a\",null,{\"className\":\"text-sm text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://www.linkedin.com\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"linkedin\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 24 24\",\"className\":\"dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6\",\"children\":[[\"$\",\"title\",null,{\"children\":\"Linkedin\"}],[\"$\",\"path\",null,{\"d\":\"M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z\"}]]}]]}],null,[\"$\",\"a\",null,{\"className\":\"text-sm text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://twitter.com/x\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"x\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 24 24\",\"className\":\"dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6\",\"children\":[[\"$\",\"title\",null,{\"children\":\"X\"}],[\"$\",\"path\",null,{\"d\":\"M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 6.932ZM17.61 20.644h2.039L6.486 3.24H4.298Z\"}]]}]]}],[\"$\",\"a\",null,{\"className\":\"text-sm text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://www.instagram.com\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"instagram\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 24 24\",\"className\":\"dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6\",\"children\":[[\"$\",\"title\",null,{\"children\":\"Instagram\"}],[\"$\",\"path\",null,{\"d\":\"$12\"}]]}]]}],[\"$\",\"a\",null,{\"className\":\"text-sm text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://www.threads.net\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"threads\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 24 24\",\"className\":\"dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6\",\"children\":[[\"$\",\"title\",null,{\"children\":\"Threads\"}],[\"$\",\"path\",null,{\"d\":\"$13\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"mb-2 flex space-x-2 text-sm text-gray-500 dark:text-gray-400\",\"children\":[[\"$\",\"div\",null,{\"children\":\"Tz\"}],[\"$\",\"div\",null,{\"children\":\" • \"}],[\"$\",\"div\",null,{\"children\":\"© 2024\"}],[\"$\",\"div\",null,{\"children\":\" • \"}],[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://beian.miit.gov.cn/\",\"children\":\"京ICP备2023010160号\"}]]}],[\"$\",\"div\",null,{\"className\":\"mb-8 text-sm text-gray-500 dark:text-gray-400\",\"children\":[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://github.com/timlrx/tailwind-nextjs-starter-blog\",\"children\":\"Tailwind Nextjs Theme\"}]}]]}]}]]}]}]]}]]}]]}],null],null],\"couldBeIntercepted\":false,\"initialHead\":[false,\"$L14\"],\"globalErrorComponent\":\"$15\",\"missingSlots\":\"$W16\"}]]\n"])</script><script>self.__next_f.push([1,"17:I[4347,[\"231\",\"static/chunks/231-a2a5e21f80783222.js\",\"173\",\"static/chunks/173-fb59ec7f2f6147b4.js\",\"797\",\"static/chunks/app/blog/%5B...slug%5D/page-5201c8be1492bbcb.js\"],\"default\"]\n18:I[9629,[\"231\",\"static/chunks/231-a2a5e21f80783222.js\",\"173\",\"static/chunks/173-fb59ec7f2f6147b4.js\",\"797\",\"static/chunks/app/blog/%5B...slug%5D/page-5201c8be1492bbcb.js\"],\"default\"]\n"])</script><script>self.__next_f.push([1,"8:[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BlogPosting\\\",\\\"headline\\\":\\\"2021_Facebook_AI_BPE对Transformer模型记忆力的影响\\\",\\\"datePublished\\\":\\\"2021-10-08T00:00:00.000Z\\\",\\\"dateModified\\\":\\\"2021-10-08T00:00:00.000Z\\\",\\\"description\\\":\\\"这篇文章主要研究了BPE词表大小对Transformer模型记忆能力的影响。实验表明,增加BPE词表大小可以提高模型的记忆能力,原因可能是BPE减少了训练序列的长度。作者通过三个任务验证了这一结论,并排除了其他可能的解释,最终确定序列长度的减少是观察到记忆效果增强的主要因素。\\\",\\\"image\\\":\\\"/static/images/twitter-card.png\\\",\\\"url\\\":\\\"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-10-08-2021_Facebook_AI_BPE对Transformer模型记忆力的影响\\\",\\\"author\\\":[{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"Tz\\\"}]}\"}}],[\"$\",\"section\",null,{\"className\":\"mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0\",\"children\":[[\"$\",\"$L17\",null,{}],[\"$\",\"article\",null,{\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"header\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"space-y-1 border-b border-gray-200 pb-10 text-center dark:border-gray-700\",\"children\":[[\"$\",\"dl\",null,{\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"dt\",null,{\"className\":\"sr-only\",\"children\":\"Published on\"}],[\"$\",\"dd\",null,{\"className\":\"text-base font-medium leading-6 text-gray-500 dark:text-gray-400\",\"children\":[\"$\",\"time\",null,{\"dateTime\":\"2021-10-08T00:00:00.000Z\",\"children\":\"October 8, 2021\"}]}]]}]}],[\"$\",\"div\",null,{\"children\":[\"$\",\"h1\",null,{\"className\":\"text-3xl font-extrabold leading-9 tracking-tight text-gray-900 dark:text-gray-100 sm:text-4xl sm:leading-10 md:text-5xl md:leading-14\",\"children\":\"2021_Facebook_AI_BPE对Transformer模型记忆力的影响\"}]}]]}]}],[\"$\",\"div\",null,{\"className\":\"grid-rows-[auto_1fr] divide-y divide-gray-200 pb-8 dark:divide-gray-700 xl:divide-y-0\",\"children\":[[\"$\",\"div\",null,{\"className\":\"divide-y divide-gray-200 dark:divide-gray-700 xl:col-span-3 xl:row-span-2 xl:pb-0\",\"children\":[\"$\",\"div\",null,{\"className\":\"prose max-w-none pb-8 pt-10 dark:prose-invert\",\"children\":[[\"$\",\"blockquote\",null,{\"children\":[\"$\",\"p\",null,{\"children\":\"HOW BPE AFFECTS MEMORIZATION IN TRANSFORMERS\"}]}],[\"$\",\"p\",null,{\"children\":\"证明了增加BPE vocab大小可以进一步提高模型的记忆能力。（记忆可能导致数据泄漏，所以需要在更好的时候记忆，在不需要的时候避免记忆。）\"}],[\"$\",\"p\",null,{\"children\":\"通常，我们认为数据增强，参数量，正则会影响模型记忆。作者通过实验发现BPE词表越大，记忆越强。作者推测原因可能是：bpe减少训练序列的长度\"}],[\"$\",\"p\",null,{\"children\":[\"Their conclusion that it is the sequence length that matters makes intuitive sense. The Transformer architecture internally compares all input token pairs many times and the number of token combinations grows quadratically with the sequence length. \",[\"$\",\"strong\",null,{\"children\":\"When we imagine the classification as searching for particular relation in the input, it totally makes sense that the fewer possible relations we have, the easier it is to find something.\"}]]}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"学习记忆任务\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#学习记忆任务\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"学习记忆任务\"]}],[\"$\",\"p\",null,{\"children\":\"如何验证模型的记忆能力，作者提出了三个任务来验证模型的记忆能力。\"}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"学习随机标签映射\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#学习随机标签映射\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"学习随机标签映射\"]}],[\"$\",\"p\",null,{\"children\":[\"学习一个非系统映射的任务，来衡量模型的记忆能力。假设更高的训练准确率意味着更高的记忆能力。使用SNLI（斯坦福自然语言推理数据集）将前提和假设通过\",[\"$\",\"code\",null,{\"children\":\"::\"}],\" 进行拼接，过滤掉中性的句子。转化为二分类任务。通过随机抽样标签替换原有标签。通过准确率来分析模型的记忆能力。\"]}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"成员推理\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#成员推理\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"成员推理\"]}],[\"$\",\"p\",null,{\"children\":\"度量模型对成员推理攻击的脆弱性来判断模型记忆的程度大小。如果很容易的准确判断一个例子是否用于训练，那我们就可以认为他以某种形式记忆在模型的权重中。（具体方法看原始论文）\"}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"训练数据恢复\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#训练数据恢复\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"训练数据恢复\"]}],[\"$\",\"p\",null,{\"children\":\"关注问答领域，判断模型对训练数据中的问题的答案生成和原始答案的匹配度。数据集使用PAQ\"}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"模型和超参数\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#模型和超参数\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"模型和超参数\"]}],[\"$\",\"p\",null,{\"children\":\"作者使用三个Transformer模型，(因果)语言模型LM，masked language model(MLM), 和一个Encoder。在学习随机标签映射和成员推理上使用全部三个架构，问答使用LM。\"}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"bpe-设置\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#bpe-设置\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"BPE 设置\"]}],[\"$\",\"blockquote\",null,{\"children\":[\"$\",\"p\",null,{\"children\":\"改变词汇表的大小\"}]}],[\"$\",\"p\",null,{\"children\":[\"对于SNLI数据采用\",[\"$\",\"code\",null,{\"children\":\"(0.5,1.5,10,20)✖️10^3\"}],\"步，对应的vocab size是611,1097,4943,9574,18336。\"]}],[\"$\",\"p\",null,{\"children\":[\"PAQ dataset，\",[\"$\",\"code\",null,{\"children\":\"(0.5,1.5,10,,15,20)✖️10^3\"}],\"对应1280, 1784, 5784, 10784, 15784, 20776 vocab size\"]}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"控制学习参数的数量\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#控制学习参数的数量\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"控制学习参数的数量\"]}],[\"$\",\"p\",null,{\"children\":\"更改词表大小后，模型参数也会发生变换，为了解决这一影响，作者控制了模型的参数量，将嵌入层替换为一个一个嵌入层和一个全连接层的组合。\"}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"改变模型为分类器\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#改变模型为分类器\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"改变模型为分类器\"]}],[\"$\",\"p\",null,{\"children\":\"EOS token 映射等方法将模型改变为分类器\"}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"模型和训练细节\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#模型和训练细节\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"模型和训练细节\"]}],[\"$\",\"p\",null,{\"children\":\"略\"}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"bpe影响记忆实验\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#bpe影响记忆实验\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"BPE影响记忆实验\"]}],[\"$\",\"p\",null,{\"children\":[\"$\",\"img\",null,{\"alt\":\"\",\"src\":\"https://www.tzer.top/usr/uploads/2021/10/2417897024.png\"}]}],[\"$\",\"p\",null,{\"children\":\"Memorizing random Labels：a,b两图模型随着BPE词表的大小增加，对随机标签拟合的更好。\"}],[\"$\",\"p\",null,{\"children\":\"Membership inference: c图中看到，(M)LM的检验精度具有不同的增长区域，表明：泛化与记忆并不直接矛盾，有一定程度的粒度可以实现更好的记忆的更好的泛化\"}],[\"$\",\"p\",null,{\"children\":[\"$\",\"img\",null,{\"alt\":\"\",\"src\":\"https://www.tzer.top/usr/uploads/2021/10/782379925.png\"}]}],[\"$\",\"p\",null,{\"children\":\"Question answer recovery: 任务采用了两个不同的模型，transformer-large和base。可以看出large的效果总比base好，这说明模型大小对记忆力影响较大。并且随着vocab size的提升，准确度也提升。在参数控制上效果也是如此。\"}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"解释\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#解释\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"解释\"]}],[\"$\",\"p\",null,{\"children\":\"作者发现，BPE学习的词汇量越大，在Transforme模型中记忆能力越强。作者提出三个假设：长度假设(BPE vocab增加使输入序列变短)，冗余假设（BPE是一种压缩算法，对数据进行压缩，模型可以更容易记忆没有冗余的数据），词汇量假设（词汇量增加，在极限情况家，每个序列都可能有一个唯一的标记，标记和标签之间会存在一对一的关系）\"}],[\"$\",\"p\",null,{\"children\":\"作者进行了实验，去除了冗余假设。如下图a在不压缩数据上，vocab越大，记忆越强。\"}],[\"$\",\"p\",null,{\"children\":[\"$\",\"img\",null,{\"alt\":\"\",\"src\":\"https://www.tzer.top/usr/uploads/2021/10/4136876816.png\"}]}],[\"$\",\"p\",null,{\"children\":\"作者通过对两个新token替换一个等概率替换一个旧token，增大了两倍词汇量，如上图b，观测出vocab增加相对较大，记忆力增强效果，并且在一开始对记忆力是有害的。去除词汇量假设。\"}],[\"$\",\"p\",null,{\"children\":\"排除法：序列长度的减少是观察到记忆效果的主要因素。\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"pb-6 pt-6 text-center text-gray-700 dark:text-gray-300\",\"id\":\"comment\",\"children\":[\"$\",\"$L18\",null,{\"slug\":\"AI/2021-10-08-2021_Facebook_AI_BPE对Transformer模型记忆力的影响\"}]}],[\"$\",\"footer\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col text-sm font-medium sm:flex-row sm:justify-between sm:text-base\",\"children\":[[\"$\",\"div\",null,{\"className\":\"pt-4 xl:pt-8\",\"children\":[\"$\",\"$L11\",null,{\"href\":\"/blog/AI/2021-09-29-2018_ACL_迭代回译\",\"className\":\"dark:hover:text-primary-400 text-primary-500 hover:text-primary-600\",\"aria-label\":\"Previous post: 2018_ACL_迭代回译\",\"children\":[\"← \",\"2018_ACL_迭代回译\"]}]}],[\"$\",\"div\",null,{\"className\":\"pt-4 xl:pt-8\",\"children\":[\"$\",\"$L11\",null,{\"href\":\"/blog/AI/2021-10-09-为什么使用self-attention,机器翻译下的评估\",\"className\":\"dark:hover:text-primary-400 text-primary-500 hover:text-primary-600\",\"aria-label\":\"Next post: 为什么使用self-attention,机器翻译下的评估\",\"children\":[\"为什么使用self-attention,机器翻译下的评估\",\" →\"]}]}]]}]}]]}]]}]}]]}]]\n"])</script><script>self.__next_f.push([1,"14:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"2021_Facebook_AI_BPE对Transformer模型记忆力的影响 | Tz Blog\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"这篇文章主要研究了BPE词表大小对Transformer模型记忆能力的影响。实验表明,增加BPE词表大小可以提高模型的记忆能力,原因可能是BPE减少了训练序列的长度。作者通过三个任务验证了这一结论,并排除了其他可能的解释,最终确定序列长度的减少是观察到记忆效果增强的主要因素。\"}],[\"$\",\"meta\",\"4\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"5\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"6\",{\"rel\":\"canonical\",\"href\":\"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-10-08-2021_Facebook_AI_BPE%E5%AF%B9Transformer%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BF%86%E5%8A%9B%E7%9A%84%E5%BD%B1%E5%93%8D\"}],[\"$\",\"link\",\"7\",{\"rel\":\"alternate\",\"type\":\"application/rss+xml\",\"href\":\"https://tailwind-nextjs-starter-blog.vercel.app/feed.xml\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:title\",\"content\":\"2021_Facebook_AI_BPE对Transformer模型记忆力的影响\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:description\",\"content\":\"这篇文章主要研究了BPE词表大小对Transformer模型记忆能力的影响。实验表明,增加BPE词表大小可以提高模型的记忆能力,原因可能是BPE减少了训练序列的长度。作者通过三个任务验证了这一结论,并排除了其他可能的解释,最终确定序列长度的减少是观察到记忆效果增强的主要因素。\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:url\",\"content\":\"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-10-08-2021_Facebook_AI_BPE%E5%AF%B9Transformer%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BF%86%E5%8A%9B%E7%9A%84%E5%BD%B1%E5%93%8D\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:site_name\",\"content\":\"Tz Blog\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:image\",\"content\":\"https://tailwind-nextjs-starter-blog.vercel.app/static/images/twitter-card.png\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"15\",{\"property\":\"article:published_time\",\"content\":\"2021-10-08T00:00:00.000Z\"}],[\"$\",\"meta\",\"16\",{\"property\":\"article:modified_time\",\"content\":\"2021-10-08T00:00:00.000Z\"}],[\"$\",\"meta\",\"17\",{\"property\":\"article:author\",\"content\":\"Tz\"}],[\"$\",\"meta\",\"18\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"19\",{\"name\":\"twitter:title\",\"content\":\"2021_Facebook_AI_BPE对Transformer模型记忆力的影响\"}],[\"$\",\"meta\",\"20\",{\"name\":\"twitter:description\",\"content\":\"这篇文章主要研究了BPE词表大小对Transformer模型记忆能力的影响。实验表明,增加BPE词表大小可以提高模型的记忆能力,原因可能是BPE减少了训练序列的长度。作者通过三个任务验证了这一结论,并排除了其他可能的解释,最终确定序列长度的减少是观察到记忆效果增强的主要因素。\"}],[\"$\",\"meta\",\"21\",{\"name\":\"twitter:image\",\"content\":\"https://tailwind-nextjs-starter-blog.vercel.app/static/images/twitter-card.png\"}],[\"$\",\"meta\",\"22\",{\"name\":\"next-size-adjust\"}]]\n"])</script><script>self.__next_f.push([1,"7:null\n"])</script></body></html>