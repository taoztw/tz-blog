2:I[1583,["231","static/chunks/231-a2a5e21f80783222.js","255","static/chunks/255-e3a8fd33daf7b25c.js","605","static/chunks/app/tags/%5Btag%5D/page-ef23034bc6d7c21b.js"],"default"]
3:I[9275,[],""]
5:I[1343,[],""]
6:I[4404,["231","static/chunks/231-a2a5e21f80783222.js","173","static/chunks/173-fb59ec7f2f6147b4.js","97","static/chunks/97-7260ee5660d3a094.js","185","static/chunks/app/layout-0079de00d5c3aa9f.js"],"GoogleAnalytics"]
7:I[8700,["231","static/chunks/231-a2a5e21f80783222.js","173","static/chunks/173-fb59ec7f2f6147b4.js","97","static/chunks/97-7260ee5660d3a094.js","185","static/chunks/app/layout-0079de00d5c3aa9f.js"],"ThemeProviders"]
8:I[4080,["231","static/chunks/231-a2a5e21f80783222.js","173","static/chunks/173-fb59ec7f2f6147b4.js","97","static/chunks/97-7260ee5660d3a094.js","185","static/chunks/app/layout-0079de00d5c3aa9f.js"],""]
9:I[9032,["231","static/chunks/231-a2a5e21f80783222.js","173","static/chunks/173-fb59ec7f2f6147b4.js","97","static/chunks/97-7260ee5660d3a094.js","185","static/chunks/app/layout-0079de00d5c3aa9f.js"],"KBarSearchProvider"]
a:I[5133,["231","static/chunks/231-a2a5e21f80783222.js","173","static/chunks/173-fb59ec7f2f6147b4.js","97","static/chunks/97-7260ee5660d3a094.js","185","static/chunks/app/layout-0079de00d5c3aa9f.js"],"default"]
b:I[231,["231","static/chunks/231-a2a5e21f80783222.js","904","static/chunks/app/tags/page-e45caa5372886f15.js"],""]
4:["tag","ai","d"]
c:T69f,M12 0C8.74 0 8.333.015 7.053.072 5.775.132 4.905.333 4.14.63c-.789.306-1.459.717-2.126 1.384S.935 3.35.63 4.14C.333 4.905.131 5.775.072 7.053.012 8.333 0 8.74 0 12s.015 3.667.072 4.947c.06 1.277.261 2.148.558 2.913.306.788.717 1.459 1.384 2.126.667.666 1.336 1.079 2.126 1.384.766.296 1.636.499 2.913.558C8.333 23.988 8.74 24 12 24s3.667-.015 4.947-.072c1.277-.06 2.148-.262 2.913-.558.788-.306 1.459-.718 2.126-1.384.666-.667 1.079-1.335 1.384-2.126.296-.765.499-1.636.558-2.913.06-1.28.072-1.687.072-4.947s-.015-3.667-.072-4.947c-.06-1.277-.262-2.149-.558-2.913-.306-.789-.718-1.459-1.384-2.126C21.319 1.347 20.651.935 19.86.63c-.765-.297-1.636-.499-2.913-.558C15.667.012 15.26 0 12 0zm0 2.16c3.203 0 3.585.016 4.85.071 1.17.055 1.805.249 2.227.415.562.217.96.477 1.382.896.419.42.679.819.896 1.381.164.422.36 1.057.413 2.227.057 1.266.07 1.646.07 4.85s-.015 3.585-.074 4.85c-.061 1.17-.256 1.805-.421 2.227-.224.562-.479.96-.899 1.382-.419.419-.824.679-1.38.896-.42.164-1.065.36-2.235.413-1.274.057-1.649.07-4.859.07-3.211 0-3.586-.015-4.859-.074-1.171-.061-1.816-.256-2.236-.421-.569-.224-.96-.479-1.379-.899-.421-.419-.69-.824-.9-1.38-.165-.42-.359-1.065-.42-2.235-.045-1.26-.061-1.649-.061-4.844 0-3.196.016-3.586.061-4.861.061-1.17.255-1.814.42-2.234.21-.57.479-.96.9-1.381.419-.419.81-.689 1.379-.898.42-.166 1.051-.361 2.221-.421 1.275-.045 1.65-.06 4.859-.06l.045.03zm0 3.678c-3.405 0-6.162 2.76-6.162 6.162 0 3.405 2.76 6.162 6.162 6.162 3.405 0 6.162-2.76 6.162-6.162 0-3.405-2.76-6.162-6.162-6.162zM12 16c-2.21 0-4-1.79-4-4s1.79-4 4-4 4 1.79 4 4-1.79 4-4 4zm7.846-10.405c0 .795-.646 1.44-1.44 1.44-.795 0-1.44-.646-1.44-1.44 0-.794.646-1.439 1.44-1.439.793-.001 1.44.645 1.44 1.439zd:T498,M12.186 24h-.007c-3.581-.024-6.334-1.205-8.184-3.509C2.35 18.44 1.5 15.586 1.472 12.01v-.017c.03-3.579.879-6.43 2.525-8.482C5.845 1.205 8.6.024 12.18 0h.014c2.746.02 5.043.725 6.826 2.098 1.677 1.29 2.858 3.13 3.509 5.467l-2.04.569c-1.104-3.96-3.898-5.984-8.304-6.015-2.91.022-5.11.936-6.54 2.717C4.307 6.504 3.616 8.914 3.589 12c.027 3.086.718 5.496 2.057 7.164 1.43 1.783 3.631 2.698 6.54 2.717 2.623-.02 4.358-.631 5.8-2.045 1.647-1.613 1.618-3.593 1.09-4.798-.31-.71-.873-1.3-1.634-1.75-.192 1.352-.622 2.446-1.284 3.272-.886 1.102-2.14 1.704-3.73 1.79-1.202.065-2.361-.218-3.259-.801-1.063-.689-1.685-1.74-1.752-2.964-.065-1.19.408-2.285 1.33-3.082.88-.76 2.119-1.207 3.583-1.291a13.853 13.853 0 0 1 3.02.142c-.126-.742-.375-1.332-.75-1.757-.513-.586-1.308-.883-2.359-.89h-.029c-.844 0-1.992.232-2.721 1.32L7.734 7.847c.98-1.454 2.568-2.256 4.478-2.256h.044c3.194.02 5.097 1.975 5.287 5.388.108.046.216.094.321.142 1.49.7 2.58 1.761 3.154 3.07.797 1.82.871 4.79-1.548 7.158-1.85 1.81-4.094 2.628-7.277 2.65Zm1.003-11.69c-.242 0-.487.007-.739.021-1.836.103-2.98.946-2.916 2.143.067 1.256 1.452 1.839 2.784 1.767 1.224-.065 2.818-.543 3.086-3.71a10.5 10.5 0 0 0-2.215-.221z0:["xkrYm55tC7e-1BRsv2CuV",[[["",{"children":["tags",{"children":[["tag","ai","d"],{"children":["__PAGE__?{\"tag\":\"ai\"}",{}]}]}]},"$undefined","$undefined",true],["",{"children":["tags",{"children":[["tag","ai","d"],{"children":["__PAGE__",{},[["$L1",["$","$L2",null,{"posts":[{"title":"马毅-_从人工智能到自主智能_演讲记录","date":"2023-07-02T00:00:00.000Z","tags":["AI"],"lastmod":"2023-07-02T00:00:00.000Z","draft":false,"summary":"人工智能向自主智能的发展趋势。文章回顾了人工智能的历史发展,指出过去十年的AI主要集中在感知和预测方面,而未来的发展方向是从黑盒到白盒、从开环到闭环、从人工到自主和自然。文章还提出了通用计算机制的目标,即适用于所有规模智能系统的机制。","layout":"PostSimple","type":"Blog","readingTime":{"text":"2 min read","minutes":1.63,"time":97800,"words":326},"slug":"AI/2023-07-02-马毅-_从人工智能到自主智能_演讲记录","path":"blog/AI/2023-07-02-马毅-_从人工智能到自主智能_演讲记录","filePath":"blog/AI/2023-07-02-马毅-_从人工智能到自主智能_演讲记录.mdx","toc":[],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"马毅-_从人工智能到自主智能_演讲记录","datePublished":"2023-07-02T00:00:00.000Z","dateModified":"2023-07-02T00:00:00.000Z","description":"人工智能向自主智能的发展趋势。文章回顾了人工智能的历史发展,指出过去十年的AI主要集中在感知和预测方面,而未来的发展方向是从黑盒到白盒、从开环到闭环、从人工到自主和自然。文章还提出了通用计算机制的目标,即适用于所有规模智能系统的机制。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2023-07-02-马毅-_从人工智能到自主智能_演讲记录"}},{"title":"让LLM精确计算两数之和","date":"2023-05-14T00:00:00.000Z","tags":["AI"],"lastmod":"2023-05-14T00:00:00.000Z","draft":false,"summary":"如何使用LLM(大型语言模型)来精确计算两个数的和。文章展示了两种方法:使用LLM chain和LLM agent,通过调用Python代码或预定义的工具函数来实现准确计算。此外,文章还简要提到了其他相关工具,如语音转文本、语音合成和图像生成等。","layout":"PostSimple","type":"Blog","readingTime":{"text":"3 min read","minutes":2.845,"time":170700,"words":569},"slug":"AI/2023-05-14-让LLM精确计算两数之和","path":"blog/AI/2023-05-14-让LLM精确计算两数之和","filePath":"blog/AI/2023-05-14-让LLM精确计算两数之和.mdx","toc":[{"value":"LLM chain","url":"#llm-chain-1","depth":2},{"value":"LLM agent","url":"#llm-agent-1","depth":2},{"value":"Other tools","url":"#other-tools-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"让LLM精确计算两数之和","datePublished":"2023-05-14T00:00:00.000Z","dateModified":"2023-05-14T00:00:00.000Z","description":"如何使用LLM(大型语言模型)来精确计算两个数的和。文章展示了两种方法:使用LLM chain和LLM agent,通过调用Python代码或预定义的工具函数来实现准确计算。此外,文章还简要提到了其他相关工具,如语音转文本、语音合成和图像生成等。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2023-05-14-让LLM精确计算两数之和"}},{"title":"prompt_example","date":"2023-05-05T00:00:00.000Z","tags":["AI"],"lastmod":"2023-05-05T00:00:00.000Z","draft":false,"summary":"这篇文章总结了ChatGPT提示工程的几种常见技巧和应用场景。主要包括结构化输出、文本摘要、信息提取、情感分析、主题推断以及文本转换(如翻译、语气调整、格式转换等)。这些技巧可以帮助用户更有效地利用ChatGPT完成各种自然语言处理任务。","layout":"PostSimple","type":"Blog","readingTime":{"text":"3 min read","minutes":2.415,"time":144900,"words":483},"slug":"AI/2023-05-05-prompt_example","path":"blog/AI/2023-05-05-prompt_example","filePath":"blog/AI/2023-05-05-prompt_example.mdx","toc":[],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"prompt_example","datePublished":"2023-05-05T00:00:00.000Z","dateModified":"2023-05-05T00:00:00.000Z","description":"这篇文章总结了ChatGPT提示工程的几种常见技巧和应用场景。主要包括结构化输出、文本摘要、信息提取、情感分析、主题推断以及文本转换(如翻译、语气调整、格式转换等)。这些技巧可以帮助用户更有效地利用ChatGPT完成各种自然语言处理任务。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2023-05-05-prompt_example","author":[{"@type":"Person","name":"Tz"}]}},{"title":"LLaMA_记录","date":"2023-04-26T00:00:00.000Z","tags":["AI"],"lastmod":"2023-04-26T00:00:00.000Z","draft":false,"summary":"这篇文章介绍了LLaMA模型的训练方法和性能评估结果。LLaMA采用开源数据训练了7B到65B参数的模型,在多个任务上表现优异,13B模型超过GPT-3,65B模型与PaLM 540B相当。文章还指出,在给定计算预算下,在更多数据上训练较小模型比训练最大模型能获得更好的性能。","layout":"PostSimple","type":"Blog","readingTime":{"text":"2 min read","minutes":1.35,"time":81000,"words":270},"slug":"AI/2023-04-26-LLaMA_记录","path":"blog/AI/2023-04-26-LLaMA_记录","filePath":"blog/AI/2023-04-26-LLaMA_记录.mdx","toc":[{"value":"方法","url":"#方法","depth":2},{"value":"2.1 预训练数据","url":"#21-预训练数据","depth":3},{"value":"2.2 架构","url":"#22-架构","depth":3},{"value":"3 结果","url":"#3-结果","depth":2},{"value":"常识推理","url":"#常识推理","depth":3},{"value":"QA","url":"#qa","depth":3},{"value":"阅读理解","url":"#阅读理解","depth":3},{"value":"数学推理","url":"#数学推理","depth":3},{"value":"代码生成","url":"#代码生成","depth":3},{"value":"大规模多任务语言理解","url":"#大规模多任务语言理解","depth":3},{"value":"训练过程中的loss","url":"#训练过程中的loss","depth":3}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"LLaMA_记录","datePublished":"2023-04-26T00:00:00.000Z","dateModified":"2023-04-26T00:00:00.000Z","description":"这篇文章介绍了LLaMA模型的训练方法和性能评估结果。LLaMA采用开源数据训练了7B到65B参数的模型,在多个任务上表现优异,13B模型超过GPT-3,65B模型与PaLM 540B相当。文章还指出,在给定计算预算下,在更多数据上训练较小模型比训练最大模型能获得更好的性能。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2023-04-26-LLaMA_记录","author":[{"@type":"Person","name":"Tz"}]}},{"title":"Generate_AI_model一些评价角度","date":"2023-03-05T00:00:00.000Z","tags":["AI"],"lastmod":"2023-03-05T00:00:00.000Z","draft":false,"summary":"总结了ChatGPT在多个领域的局限性和失败案例,包括推理、逻辑、数学、事实准确性等方面。同时,文章也探讨了ChatGPT对社会的影响,如隐私、抄袭、环境影响等问题,并指出了未来研究的方向。","layout":"PostSimple","type":"Blog","readingTime":{"text":"8 min read","minutes":7.405,"time":444300,"words":1481},"slug":"AI/2023-03-05-Generate_AI_model一些评价角度","path":"blog/AI/2023-03-05-Generate_AI_model一些评价角度","filePath":"blog/AI/2023-03-05-Generate_AI_model一些评价角度.mdx","toc":[{"value":"推理 Reasoning","url":"#推理-reasoning-1","depth":2},{"value":"spatial reasoning 空间推理","url":"#spatial-reasoning-空间推理-1","depth":3},{"value":"Temporal reasoning 时间推理","url":"#temporal-reasoning-时间推理-1","depth":3},{"value":"Physical reasoning 物理推理","url":"#physical-reasoning-物理推理-1","depth":3},{"value":"Psychological reasoning 心理推理","url":"#psychological-reasoning-心理推理-1","depth":3},{"value":"Logic","url":"#logic-1","depth":2},{"value":"Math and arithmetic 数学与算数","url":"#math-and-arithmetic-数学与算数-1","depth":2},{"value":"Factual Errors","url":"#factual-errors-1","depth":2},{"value":"Bias and Discrimination","url":"#bias-and-discrimination-1","depth":2},{"value":"Wit and Humor 机智 幽默","url":"#wit-and-humor-机智-幽默-1","depth":2},{"value":"Codding","url":"#codding-1","depth":2},{"value":"Syntactic Structure, Spelling, and Grammar","url":"#syntactic-structure-spelling-and-grammar-1","depth":2},{"value":"self awareness","url":"#self-awareness-1","depth":2},{"value":"Ethics and Morality 伦理道德","url":"#ethics-and-morality-伦理道德-1","depth":2},{"value":"Other","url":"#other-1","depth":2},{"value":"对社会的影响","url":"#对社会的影响-1","depth":2},{"value":"Transparency and Trustworthiness 透明度 可信度","url":"#transparency-and-trustworthiness-透明度-可信度-1","depth":3},{"value":"Robustness and Security","url":"#robustness-and-security-1","depth":3},{"value":"Privacy 隐私","url":"#privacy-隐私-1","depth":3},{"value":"Plagiarism 抄袭","url":"#plagiarism-抄袭-1","depth":3},{"value":"Environmental Impact and Sustainability","url":"#environmental-impact-and-sustainability-1","depth":3},{"value":"Future work","url":"#future-work-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"Generate_AI_model一些评价角度","datePublished":"2023-03-05T00:00:00.000Z","dateModified":"2023-03-05T00:00:00.000Z","description":"总结了ChatGPT在多个领域的局限性和失败案例,包括推理、逻辑、数学、事实准确性等方面。同时,文章也探讨了ChatGPT对社会的影响,如隐私、抄袭、环境影响等问题,并指出了未来研究的方向。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2023-03-05-Generate_AI_model一些评价角度"}},{"title":"笔记-nanoGPT","date":"2023-02-12T00:00:00.000Z","tags":["AI"],"lastmod":"2023-02-12T00:00:00.000Z","draft":false,"summary":"这篇文章主要介绍了GPT模型的结构和实现细节。文章详细描述了GPT模型的核心组件,包括多头自注意力机制、前馈神经网络、位置编码等,并给出了相应的Python代码实现。此外,文章还介绍了GPT模型的训练过程,包括学习率调整策略等。","layout":"PostSimple","type":"Blog","readingTime":{"text":"7 min read","minutes":6.475,"time":388500,"words":1295},"slug":"AI/2023-02-12-笔记-nanoGPT","path":"blog/AI/2023-02-12-笔记-nanoGPT","filePath":"blog/AI/2023-02-12-笔记-nanoGPT.mdx","toc":[{"value":"GPTConfig","url":"#gptconfig","depth":2},{"value":"CausalSelfAttention","url":"#causalselfattention","depth":2},{"value":"MLP","url":"#mlp","depth":2},{"value":"Block","url":"#block","depth":2},{"value":"GPT","url":"#gpt","depth":2},{"value":"generate","url":"#generate","depth":2},{"value":"train","url":"#train","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"笔记-nanoGPT","datePublished":"2023-02-12T00:00:00.000Z","dateModified":"2023-02-12T00:00:00.000Z","description":"这篇文章主要介绍了GPT模型的结构和实现细节。文章详细描述了GPT模型的核心组件,包括多头自注意力机制、前馈神经网络、位置编码等,并给出了相应的Python代码实现。此外,文章还介绍了GPT模型的训练过程,包括学习率调整策略等。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2023-02-12-笔记-nanoGPT"}},{"title":"GPT2","date":"2023-01-15T00:00:00.000Z","tags":["AI"],"lastmod":"2023-01-15T00:00:00.000Z","draft":false,"summary":"GPT-2语言模型,一个无监督的多任务学习器。GPT-2在多个任务上实现了零样本学习的最先进结果,展示了语言模型作为通用任务学习器的潜力。文章还讨论了增加模型容量可以以对数线性方式提高性能,以及大规模多样化数据集对模型泛化能力的重要性。","layout":"PostSimple","type":"Blog","readingTime":{"text":"7 min read","minutes":6.95,"time":417000,"words":1390},"slug":"AI/2023-01-15-GPT2","path":"blog/AI/2023-01-15-GPT2","filePath":"blog/AI/2023-01-15-GPT2.mdx","toc":[{"value":"Approach","url":"#approach-2","depth":2},{"value":"数据","url":"#数据-2","depth":3},{"value":"数据输入","url":"#数据输入-2","depth":3},{"value":"模型","url":"#模型-2","depth":3},{"value":"实验","url":"#实验-2","depth":2},{"value":"","url":"#-2","depth":2},{"value":"泛化与记忆","url":"#泛化与记忆-2","depth":2},{"value":"相关工作","url":"#相关工作-2","depth":2},{"value":"讨论","url":"#讨论-2","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"GPT2","datePublished":"2023-01-15T00:00:00.000Z","dateModified":"2023-01-15T00:00:00.000Z","description":"GPT-2语言模型,一个无监督的多任务学习器。GPT-2在多个任务上实现了零样本学习的最先进结果,展示了语言模型作为通用任务学习器的潜力。文章还讨论了增加模型容量可以以对数线性方式提高性能,以及大规模多样化数据集对模型泛化能力的重要性。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2023-01-15-GPT2"}},{"title":"GPT3","date":"2023-01-15T00:00:00.000Z","tags":["AI"],"lastmod":"2023-01-15T00:00:00.000Z","draft":false,"summary":"GPT-3是一个拥有1750亿参数的大型语言模型,通过增大模型规模显著提高了小样本学习能力。在问答、填空、翻译等多项任务上,GPT-3无需微调就能取得不错的性能,但在某些数据集上仍存在困难。该研究还探讨了如何在大规模数据上训练如此庞大的语言模型。","layout":"PostSimple","type":"Blog","readingTime":{"text":"6 min read","minutes":5.845,"time":350700,"words":1169},"slug":"AI/2023-01-15-GPT3","path":"blog/AI/2023-01-15-GPT3","filePath":"blog/AI/2023-01-15-GPT3.mdx","toc":[{"value":"简介","url":"#简介-1","depth":2},{"value":"方法","url":"#方法-4","depth":2},{"value":"模型和架构","url":"#模型和架构-1","depth":3},{"value":"训练数据","url":"#训练数据-1","depth":3},{"value":"训练过程","url":"#训练过程-1","depth":3},{"value":"结果","url":"#结果-5","depth":2},{"value":"Measuring and Preventing Memorization Of Benchmarks","url":"#measuring-and-preventing-memorization-of-benchmarks-1","depth":2},{"value":"局限性","url":"#局限性-1","depth":2},{"value":"instructions","url":"#instructions-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"GPT3","datePublished":"2023-01-15T00:00:00.000Z","dateModified":"2023-01-15T00:00:00.000Z","description":"GPT-3是一个拥有1750亿参数的大型语言模型,通过增大模型规模显著提高了小样本学习能力。在问答、填空、翻译等多项任务上,GPT-3无需微调就能取得不错的性能,但在某些数据集上仍存在困难。该研究还探讨了如何在大规模数据上训练如此庞大的语言模型。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2023-01-15-GPT3"}},{"title":"自编码-自回归_BERT-GPT-LLM_","date":"2023-01-15T00:00:00.000Z","tags":["AI"],"lastmod":"2023-01-15T00:00:00.000Z","draft":false,"summary":"自回归和自编码模型在自然语言处理中的应用,以及BERT、GPT等大型语言模型的发展。文章重点讨论了BERT及其变体(如ALBERT、RoBERTa等)的改进,以及GPT、XLNet等自回归模型的特点。最后,文章简要概述了大型语言模型(LLM)的发展历程及其在NLP任务中的应用前景。","layout":"PostSimple","type":"Blog","readingTime":{"text":"11 min read","minutes":10.66,"time":639600,"words":2132},"slug":"AI/2023-01-15-自编码-自回归_BERT-GPT-LLM_","path":"blog/AI/2023-01-15-自编码-自回归_BERT-GPT-LLM_","filePath":"blog/AI/2023-01-15-自编码-自回归_BERT-GPT-LLM_.mdx","toc":[{"value":"自回归 和 自编码","url":"#自回归-和-自编码-1","depth":2},{"value":"BERT","url":"#bert-1","depth":2},{"value":"ALBERT","url":"#albert-1","depth":3},{"value":"RoBERTa","url":"#roberta-1","depth":3},{"value":"FinBert","url":"#finbert-1","depth":3},{"value":"K-Bert KG-bert","url":"#k-bert-kg-bert-1","depth":3},{"value":"SpanBert","url":"#spanbert-1","depth":3},{"value":"GPT","url":"#gpt-3","depth":2},{"value":"XLnet","url":"#xlnet-1","depth":2},{"value":"LLM","url":"#llm-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"自编码-自回归_BERT-GPT-LLM_","datePublished":"2023-01-15T00:00:00.000Z","dateModified":"2023-01-15T00:00:00.000Z","description":"自回归和自编码模型在自然语言处理中的应用,以及BERT、GPT等大型语言模型的发展。文章重点讨论了BERT及其变体(如ALBERT、RoBERTa等)的改进,以及GPT、XLNet等自回归模型的特点。最后,文章简要概述了大型语言模型(LLM)的发展历程及其在NLP任务中的应用前景。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2023-01-15-自编码-自回归_BERT-GPT-LLM_"}},{"title":"Reinforcement_Learning","date":"2023-01-04T00:00:00.000Z","tags":["AI"],"lastmod":"2023-01-04T00:00:00.000Z","draft":false,"summary":"这篇文章主要介绍了强化学习的基本概念和定义。文章解释了概率密度函数、期望、状态、动作、策略、奖励等基础术语，并定义了回报、折扣回报、动作价值函数、最优动作价值函数和状态价值函数等关键概念。文章还通过马里奥游戏的例子来具体说明这些概念在实际应用中的含义。","layout":"PostSimple","type":"Blog","readingTime":{"text":"4 min read","minutes":3.655,"time":219300,"words":731},"slug":"AI/2023-01-04-Reinforcement_Learning","path":"blog/AI/2023-01-04-Reinforcement_Learning","filePath":"blog/AI/2023-01-04-Reinforcement_Learning.mdx","toc":[{"value":"基础","url":"#基础-2","depth":2},{"value":"术语","url":"#术语-2","depth":2},{"value":"随机性","url":"#随机性-2","depth":2},{"value":"定义","url":"#定义-2","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reinforcement_Learning","datePublished":"2023-01-04T00:00:00.000Z","dateModified":"2023-01-04T00:00:00.000Z","description":"这篇文章主要介绍了强化学习的基本概念和定义。文章解释了概率密度函数、期望、状态、动作、策略、奖励等基础术语，并定义了回报、折扣回报、动作价值函数、最优动作价值函数和状态价值函数等关键概念。文章还通过马里奥游戏的例子来具体说明这些概念在实际应用中的含义。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2023-01-04-Reinforcement_Learning"}},{"title":"逻辑回归_BASE","date":"2022-06-05T00:00:00.000Z","tags":["AI"],"lastmod":"2022-06-05T00:00:00.000Z","draft":false,"summary":"这篇文章介绍了逻辑回归的核心概念,包括最大似然估计、sigmoid函数和交叉熵。文章通过抛硬币和银行放款的例子来解释这些概念,并提供了一个训练逻辑回归模型的代码notebook。","layout":"PostSimple","type":"Blog","readingTime":{"text":"1 min read","minutes":0.215,"time":12900,"words":43},"slug":"AI/2022-06-05-逻辑回归_BASE","path":"blog/AI/2022-06-05-逻辑回归_BASE","filePath":"blog/AI/2022-06-05-逻辑回归_BASE.mdx","toc":[{"value":"银行放款似然函数","url":"#银行放款似然函数-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"逻辑回归_BASE","datePublished":"2022-06-05T00:00:00.000Z","dateModified":"2022-06-05T00:00:00.000Z","description":"这篇文章介绍了逻辑回归的核心概念,包括最大似然估计、sigmoid函数和交叉熵。文章通过抛硬币和银行放款的例子来解释这些概念,并提供了一个训练逻辑回归模型的代码notebook。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2022-06-05-逻辑回归_BASE"}},{"title":"n-gram检索domain_Adaptation：_Non-Parametric_Adaptation_for_Neural_Machine_Translation","date":"2022-04-01T00:00:00.000Z","tags":["AI"],"lastmod":"2022-04-01T00:00:00.000Z","draft":false,"summary":"这篇文章提出了一种半参数方法,通过n-gram检索来实现神经机器翻译在新领域的无参数适应。作者设计了新的架构来编码源语言和目标语言信息,并通过消融分析验证了方法的有效性。该方法在异构数据和稀有短语翻译上表现良好,避免了微调可能带来的灾难性遗忘问题。","layout":"PostSimple","type":"Blog","readingTime":{"text":"2 min read","minutes":1.59,"time":95400,"words":318},"slug":"AI/2022-04-01-n-gram检索domain_Adaptation：_Non-Parametric_Adaptation_for_Neural_Machine_Translation","path":"blog/AI/2022-04-01-n-gram检索domain_Adaptation：_Non-Parametric_Adaptation_for_Neural_Machine_Translation","filePath":"blog/AI/2022-04-01-n-gram检索domain_Adaptation：_Non-Parametric_Adaptation_for_Neural_Machine_Translation.mdx","toc":[{"value":"检索方法","url":"#检索方法-1","depth":2},{"value":"IDF句子检索","url":"#idf句子检索-1","depth":3},{"value":"IDF N-gram检索","url":"#idf-n-gram检索-1","depth":3},{"value":"N-gram向量","url":"#n-gram向量-1","depth":3},{"value":"新架构","url":"#新架构-1","depth":2},{"value":"架构","url":"#架构-1","depth":3},{"value":"CSTM","url":"#cstm-1","depth":3},{"value":"模型效果","url":"#模型效果-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"n-gram检索domain_Adaptation：_Non-Parametric_Adaptation_for_Neural_Machine_Translation","datePublished":"2022-04-01T00:00:00.000Z","dateModified":"2022-04-01T00:00:00.000Z","description":"这篇文章提出了一种半参数方法,通过n-gram检索来实现神经机器翻译在新领域的无参数适应。作者设计了新的架构来编码源语言和目标语言信息,并通过消融分析验证了方法的有效性。该方法在异构数据和稀有短语翻译上表现良好,避免了微调可能带来的灾难性遗忘问题。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2022-04-01-n-gram检索domain_Adaptation：_Non-Parametric_Adaptation_for_Neural_Machine_Translation","author":[{"@type":"Person","name":"Tz"}]}},{"title":"GEC语法错误纠正-GECToR","date":"2022-03-31T00:00:00.000Z","tags":["AI"],"lastmod":"2022-03-31T00:00:00.000Z","draft":false,"summary":"这篇文章介绍了一种名为GECToR的语法纠错方法,采用序列标注模型对错误tokens进行变换标记,而不是直接重写句子。该方法通过三步训练过程和推理技巧提高了模型性能,在保持高准确率的同时大幅提升了推理速度。","layout":"PostSimple","type":"Blog","readingTime":{"text":"6 min read","minutes":5.24,"time":314400,"words":1048},"slug":"AI/2022-03-31-GEC语法错误纠正-GECToR","path":"blog/AI/2022-03-31-GEC语法错误纠正-GECToR","filePath":"blog/AI/2022-03-31-GEC语法错误纠正-GECToR.mdx","toc":[{"value":"Abstract","url":"#abstract-3","depth":2},{"value":"1. Introduction","url":"#1-introduction-2","depth":2},{"value":"2. 数据","url":"#2-数据-2","depth":2},{"value":"3. Token-level transformations","url":"#3-token-level-transformations-2","depth":2},{"value":"基础变换","url":"#基础变换-2","depth":3},{"value":"g-transformations","url":"#g-transformations-2","depth":3},{"value":"预处理","url":"#预处理-2","depth":3},{"value":"4. Tagging model架构","url":"#4-tagging-model架构-2","depth":2},{"value":"5. 迭代方法","url":"#5-迭代方法-2","depth":2},{"value":"6. Experiments","url":"#6-experiments-2","depth":2},{"value":"三步训练阶段","url":"#三步训练阶段-2","depth":3},{"value":"不同预训练模型作为encoder","url":"#不同预训练模型作为encoder-2","depth":3},{"value":"推理调整","url":"#推理调整-2","depth":3},{"value":"推理速度","url":"#推理速度-2","depth":3},{"value":"M2输出为incor cor文件脚本","url":"#m2输出为incor-cor文件脚本-2","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"GEC语法错误纠正-GECToR","datePublished":"2022-03-31T00:00:00.000Z","dateModified":"2022-03-31T00:00:00.000Z","description":"这篇文章介绍了一种名为GECToR的语法纠错方法,采用序列标注模型对错误tokens进行变换标记,而不是直接重写句子。该方法通过三步训练过程和推理技巧提高了模型性能,在保持高准确率的同时大幅提升了推理速度。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2022-03-31-GEC语法错误纠正-GECToR"}},{"title":"MT翻译记忆融合-Encoding_Gated_Translation_Memory_into_Neural_Machine_Translation","date":"2022-03-31T00:00:00.000Z","tags":["AI"],"lastmod":"2022-03-31T00:00:00.000Z","draft":false,"summary":"这篇文章提出了一种将翻译记忆(TM)融入神经机器翻译(NMT)的方法。通过两个独立的编码器对输入和TM匹配进行编码,并使用TM门控网络计算权重,将TM信息加权融入翻译生成过程。实验验证了不同模糊匹配分数(FMS)对结果的影响,并分析了TM门控值与语义相似度的关系。","layout":"PostSimple","type":"Blog","readingTime":{"text":"1 min read","minutes":0.735,"time":44100,"words":147},"slug":"AI/2022-03-31-MT翻译记忆融合-Encoding_Gated_Translation_Memory_into_Neural_Machine_Translation","path":"blog/AI/2022-03-31-MT翻译记忆融合-Encoding_Gated_Translation_Memory_into_Neural_Machine_Translation","filePath":"blog/AI/2022-03-31-MT翻译记忆融合-Encoding_Gated_Translation_Memory_into_Neural_Machine_Translation.mdx","toc":[],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"MT翻译记忆融合-Encoding_Gated_Translation_Memory_into_Neural_Machine_Translation","datePublished":"2022-03-31T00:00:00.000Z","dateModified":"2022-03-31T00:00:00.000Z","description":"这篇文章提出了一种将翻译记忆(TM)融入神经机器翻译(NMT)的方法。通过两个独立的编码器对输入和TM匹配进行编码,并使用TM门控网络计算权重,将TM信息加权融入翻译生成过程。实验验证了不同模糊匹配分数(FMS)对结果的影响,并分析了TM门控值与语义相似度的关系。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2022-03-31-MT翻译记忆融合-Encoding_Gated_Translation_Memory_into_Neural_Machine_Translation"}},{"title":"增强术语翻译（修改输入）-Training_Neural_Machine_Translation_To_Apply_Terminology_Constraints_","date":"2022-03-31T00:00:00.000Z","tags":["AI"],"lastmod":"2022-03-31T00:00:00.000Z","draft":false,"summary":"这篇文章介绍了一种通过在输入中增加目标端术语信息来提高神经机器翻译模型术语翻译能力的方法。该方法使用replace和append两种方式添加术语注解，让模型学习\"复制机制\"，并考虑术语的形态变化。研究结果显示这种方法可以提高术语翻译准确性，但在BLEU评分上有所下降，且通用性有限。","layout":"PostSimple","type":"Blog","readingTime":{"text":"2 min read","minutes":1.175,"time":70500,"words":235},"slug":"AI/2022-03-31-增强术语翻译（修改输入）-Training_Neural_Machine_Translation_To_Apply_Terminology_Constraints_","path":"blog/AI/2022-03-31-增强术语翻译（修改输入）-Training_Neural_Machine_Translation_To_Apply_Terminology_Constraints_","filePath":"blog/AI/2022-03-31-增强术语翻译（修改输入）-Training_Neural_Machine_Translation_To_Apply_Terminology_Constraints_.mdx","toc":[],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"增强术语翻译（修改输入）-Training_Neural_Machine_Translation_To_Apply_Terminology_Constraints_","datePublished":"2022-03-31T00:00:00.000Z","dateModified":"2022-03-31T00:00:00.000Z","description":"这篇文章介绍了一种通过在输入中增加目标端术语信息来提高神经机器翻译模型术语翻译能力的方法。该方法使用replace和append两种方式添加术语注解，让模型学习\"复制机制\"，并考虑术语的形态变化。研究结果显示这种方法可以提高术语翻译准确性，但在BLEU评分上有所下降，且通用性有限。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2022-03-31-增强术语翻译（修改输入）-Training_Neural_Machine_Translation_To_Apply_Terminology_Constraints_"}},{"title":"低资源领域适应MT","date":"2022-03-22T00:00:00.000Z","tags":["AI"],"lastmod":"2022-03-22T00:00:00.000Z","draft":false,"summary":"这篇文章介绍了几种利用丰富通用语料来训练低资源领域机器翻译模型的方法,包括增量训练、集成解码、合并训练数据和数据加权等。其中数据加权方法通过对领域内数据进行过采样,在训练过程中让模型\"见到\"更多领域数据,在领域数据量为50k-500k时效果较好。","layout":"PostSimple","type":"Blog","readingTime":{"text":"4 min read","minutes":3.555,"time":213300,"words":711},"slug":"AI/2022-03-22-低资源领域适应MT","path":"blog/AI/2022-03-22-低资源领域适应MT","filePath":"blog/AI/2022-03-22-低资源领域适应MT.mdx","toc":[{"value":"1. Incremental Training","url":"#1-incremental-training-1","depth":2},{"value":"2. Ensemble Decoding（2 models）","url":"#2-ensemble-decoding2-models-1","depth":2},{"value":"3. Combining Training Data","url":"#3-combining-training-data-1","depth":2},{"value":"4. Data Weighting","url":"#4-data-weighting-1","depth":2},{"value":"4.1 Mixed Fine-Tuning","url":"#41-mixed-fine-tuning-1","depth":3}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"低资源领域适应MT","datePublished":"2022-03-22T00:00:00.000Z","dateModified":"2022-03-22T00:00:00.000Z","description":"这篇文章介绍了几种利用丰富通用语料来训练低资源领域机器翻译模型的方法,包括增量训练、集成解码、合并训练数据和数据加权等。其中数据加权方法通过对领域内数据进行过采样,在训练过程中让模型\"见到\"更多领域数据,在领域数据量为50k-500k时效果较好。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2022-03-22-低资源领域适应MT"}},{"title":"笔记-（Adaptive）Nearest_Neighbor_Machine_Translation_","date":"2022-03-08T00:00:00.000Z","tags":["AI"],"lastmod":"2022-03-08T00:00:00.000Z","draft":false,"summary":"Nearest Neighbor Machine Translation (KNN-MT)","layout":"PostSimple","type":"Blog","readingTime":{"text":"5 min read","minutes":4.87,"time":292200,"words":974},"slug":"AI/2022-03-08-笔记-（Adaptive）Nearest_Neighbor_Machine_Translation_","path":"blog/AI/2022-03-08-笔记-（Adaptive）Nearest_Neighbor_Machine_Translation_","filePath":"blog/AI/2022-03-08-笔记-（Adaptive）Nearest_Neighbor_Machine_Translation_.mdx","toc":[{"value":"原理：","url":"#原理-1","depth":2},{"value":"KNN概率分布的计算方式","url":"#knn概率分布的计算方式-1","depth":2},{"value":"构建Datastore","url":"#构建datastore-1","depth":3},{"value":"计算KNN的分布","url":"#计算knn的分布-1","depth":3},{"value":"Experiments","url":"#experiments-3","depth":2},{"value":"单一语言对翻译","url":"#单一语言对翻译-1","depth":3},{"value":"多语言模型","url":"#多语言模型-1","depth":3},{"value":"域适应","url":"#域适应-1","depth":3},{"value":"关键超参数调整","url":"#关键超参数调整-1","depth":2},{"value":"softmax temperature","url":"#softmax-temperature-1","depth":3},{"value":"查询neighbors的个数","url":"#查询neighbors的个数-1","depth":3},{"value":"datastore大小","url":"#datastore大小-1","depth":3},{"value":"T和lambda","url":"#t和lambda-1","depth":3},{"value":"案例","url":"#案例-1","depth":2},{"value":"其他优化","url":"#其他优化-1","depth":2},{"value":"Adaptive Nearest Neighbor Machine Translation","url":"#adaptive-nearest-neighbor-machine-translation-1","depth":3},{"value":"Learning Kernel-Smoothed Machine Translation with RetrievedExamples","url":"#learning-kernel-smoothed-machine-translation-with-retrievedexamples-1","depth":3}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"笔记-（Adaptive）Nearest_Neighbor_Machine_Translation_","datePublished":"2022-03-08T00:00:00.000Z","dateModified":"2022-03-08T00:00:00.000Z","description":"Nearest Neighbor Machine Translation (KNN-MT)","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2022-03-08-笔记-（Adaptive）Nearest_Neighbor_Machine_Translation_"}},{"title":"笔记-Learning_Kernel-Smoothed_Machine_Translation_with_RetrievedExamples","date":"2022-02-25T00:00:00.000Z","tags":["AI"],"lastmod":"2022-02-25T00:00:00.000Z","draft":false,"summary":"KSTER的机器翻译方法,通过可学习的核函数和自适应混合权重来改进基于检索的神经机器翻译。KSTER在领域适应和多领域翻译任务中表现优异,相比基线模型在BLEU分数上提高了1.1-1.5分。该方法通过动态计算检索实例的相关性和自适应混合模型预测与检索结果,在保持通用性能的同时提高了特定领域的翻译质量。","layout":"PostSimple","type":"Blog","readingTime":{"text":"7 min read","minutes":6.025,"time":361500,"words":1205},"slug":"AI/2022-02-25-笔记-Learning_Kernel-Smoothed_Machine_Translation_with_RetrievedExamples","path":"blog/AI/2022-02-25-笔记-Learning_Kernel-Smoothed_Machine_Translation_with_RetrievedExamples","filePath":"blog/AI/2022-02-25-笔记-Learning_Kernel-Smoothed_Machine_Translation_with_RetrievedExamples.mdx","toc":[{"value":"1 Introduction","url":"#1-introduction-6","depth":2},{"value":"2 Related Word","url":"#2-related-word-1","depth":2},{"value":"3 Methodology","url":"#3-methodology-1","depth":2},{"value":"3.1 Kernel-Smoothed Machine Translation","url":"#31-kernel-smoothed-machine-translation-1","depth":3},{"value":"3.2 Learnable Kernel Function","url":"#32-learnable-kernel-function-1","depth":3},{"value":"3.3 Adaptive Mixing of Base Prediction andRetrieved Examples","url":"#33-adaptive-mixing-of-base-prediction-andretrieved-examples-1","depth":3},{"value":"3.4 Training","url":"#34-training-1","depth":3},{"value":"3.5 Retrieval Dropout","url":"#35-retrieval-dropout-1","depth":3},{"value":"4 Experiments","url":"#4-experiments-1","depth":2},{"value":"4.1 Datasets and Implementation Details","url":"#41-datasets-and-implementation-details-1","depth":3},{"value":"4.2 Domain Adaptation for MachineTranslation","url":"#42-domain-adaptation-for-machinetranslation-1","depth":3},{"value":"4.3 Multi-Domain Machine Translation","url":"#43-multi-domain-machine-translation-1","depth":3},{"value":"4.4 推理速度","url":"#44-推理速度-1","depth":3},{"value":"5 分析","url":"#5-分析-1","depth":2},{"value":"5.1 Ablation Studies of Proposed Methods","url":"#51-ablation-studies-of-proposed-methods-1","depth":3},{"value":"5.2 对翻译的细粒度影响","url":"#52-对翻译的细粒度影响-1","depth":3},{"value":"论文复现结果","url":"#论文复现结果-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"笔记-Learning_Kernel-Smoothed_Machine_Translation_with_RetrievedExamples","datePublished":"2022-02-25T00:00:00.000Z","dateModified":"2022-02-25T00:00:00.000Z","description":"KSTER的机器翻译方法,通过可学习的核函数和自适应混合权重来改进基于检索的神经机器翻译。KSTER在领域适应和多领域翻译任务中表现优异,相比基线模型在BLEU分数上提高了1.1-1.5分。该方法通过动态计算检索实例的相关性和自适应混合模型预测与检索结果,在保持通用性能的同时提高了特定领域的翻译质量。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2022-02-25-笔记-Learning_Kernel-Smoothed_Machine_Translation_with_RetrievedExamples","author":[{"@type":"Person","name":"Tz"}]}},{"title":"笔记-As_Easy_as_1,_2,_3:_Behavioural_Testing_of_NMT_Systemsfor_Numerical_Translation","date":"2022-02-23T00:00:00.000Z","tags":["AI"],"lastmod":"2022-02-23T00:00:00.000Z","draft":false,"summary":"这篇文章介绍了一项针对机器翻译模型数字翻译准确率的测试研究。研究对比了不同类型的翻译模型,总结出四种常见的数字翻译错误类型,并提出了几种可能的改进策略。结果显示,所有测试的模型在各种错误类型上都未能达到100%的准确率。","layout":"PostSimple","type":"Blog","readingTime":{"text":"2 min read","minutes":1.725,"time":103500,"words":345},"slug":"AI/2022-02-23-笔记-As_Easy_as_1,_2,_3:_Behavioural_Testing_of_NMT_Systemsfor_Numerical_Translation","path":"blog/AI/2022-02-23-笔记-As_Easy_as_1,_2,_3:_Behavioural_Testing_of_NMT_Systemsfor_Numerical_Translation","filePath":"blog/AI/2022-02-23-笔记-As_Easy_as_1,_2,_3:_Behavioural_Testing_of_NMT_Systemsfor_Numerical_Translation.mdx","toc":[{"value":"错误说明","url":"#错误说明-1","depth":2},{"value":"作者提出的缓解策略","url":"#作者提出的缓解策略-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"笔记-As_Easy_as_1,_2,_3:_Behavioural_Testing_of_NMT_Systemsfor_Numerical_Translation","datePublished":"2022-02-23T00:00:00.000Z","dateModified":"2022-02-23T00:00:00.000Z","description":"这篇文章介绍了一项针对机器翻译模型数字翻译准确率的测试研究。研究对比了不同类型的翻译模型,总结出四种常见的数字翻译错误类型,并提出了几种可能的改进策略。结果显示,所有测试的模型在各种错误类型上都未能达到100%的准确率。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2022-02-23-笔记-As_Easy_as_1,_2,_3:_Behavioural_Testing_of_NMT_Systemsfor_Numerical_Translation"}},{"title":"小牛论坛笔记（2021）","date":"2022-02-21T00:00:00.000Z","tags":["AI"],"lastmod":"2022-02-21T00:00:00.000Z","draft":false,"summary":"这篇文章主要讨论了神经机器翻译领域的几个研究方向,包括模型压缩、质量评估和训练策略等。文章介绍了减少模型冗余的方法,如深编码器-浅解码器结构和知识蒸馏等。同时还探讨了翻译模型的学习规律,提出了基于课程学习的训练策略。","layout":"PostSimple","type":"Blog","readingTime":{"text":"3 min read","minutes":2.63,"time":157800,"words":526},"slug":"AI/2022-02-21-小牛论坛笔记（2021）","path":"blog/AI/2022-02-21-小牛论坛笔记（2021）","filePath":"blog/AI/2022-02-21-小牛论坛笔记（2021）.mdx","toc":[{"value":"小牛","url":"#小牛-1","depth":2},{"value":"结构冗余","url":"#结构冗余-1","depth":3},{"value":"参数冗余","url":"#参数冗余-1","depth":3},{"value":"计算冗余","url":"#计算冗余-1","depth":3},{"value":"华为分享","url":"#华为分享-1","depth":2},{"value":"NMT Quality Estimation","url":"#nmt-quality-estimation-1","depth":3},{"value":"澳门大学","url":"#澳门大学-1","depth":2},{"value":"科大讯飞","url":"#科大讯飞-1","depth":2},{"value":"南京大学","url":"#南京大学-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"小牛论坛笔记（2021）","datePublished":"2022-02-21T00:00:00.000Z","dateModified":"2022-02-21T00:00:00.000Z","description":"这篇文章主要讨论了神经机器翻译领域的几个研究方向,包括模型压缩、质量评估和训练策略等。文章介绍了减少模型冗余的方法,如深编码器-浅解码器结构和知识蒸馏等。同时还探讨了翻译模型的学习规律,提出了基于课程学习的训练策略。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2022-02-21-小牛论坛笔记（2021）"}},{"title":"笔记-Improving_Neural_Machine_Translation_by_Bidirectional_Training","date":"2022-02-17T00:00:00.000Z","tags":["AI"],"lastmod":"2022-02-17T00:00:00.000Z","draft":false,"summary":"这篇文章提出了一种名为BiT的新方法,通过使用双向模型作为单向模型的初始化来提高机器翻译性能。BiT方法在训练早期阶段将源语言到目标语言的数据组合为源语言+目标语言到目标语言+源语言的形式进行预训练,然后再使用常规的源语言到目标语言数据进行训练。实验表明,BiT方法在8个语言对上都取得了优于现有最佳方法的性能提升,并且能提高模型的对齐质量和低资源场景下的效果。","layout":"PostSimple","type":"Blog","readingTime":{"text":"2 min read","minutes":1.2,"time":72000,"words":240},"slug":"AI/2022-02-17-笔记-Improving_Neural_Machine_Translation_by_Bidirectional_Training","path":"blog/AI/2022-02-17-笔记-Improving_Neural_Machine_Translation_by_Bidirectional_Training","filePath":"blog/AI/2022-02-17-笔记-Improving_Neural_Machine_Translation_by_Bidirectional_Training.mdx","toc":[],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"笔记-Improving_Neural_Machine_Translation_by_Bidirectional_Training","datePublished":"2022-02-17T00:00:00.000Z","dateModified":"2022-02-17T00:00:00.000Z","description":"这篇文章提出了一种名为BiT的新方法,通过使用双向模型作为单向模型的初始化来提高机器翻译性能。BiT方法在训练早期阶段将源语言到目标语言的数据组合为源语言+目标语言到目标语言+源语言的形式进行预训练,然后再使用常规的源语言到目标语言数据进行训练。实验表明,BiT方法在8个语言对上都取得了优于现有最佳方法的性能提升,并且能提高模型的对齐质量和低资源场景下的效果。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2022-02-17-笔记-Improving_Neural_Machine_Translation_by_Bidirectional_Training"}},{"title":"TranSmart-笔记","date":"2022-02-14T00:00:00.000Z","tags":["AI"],"lastmod":"2022-02-14T00:00:00.000Z","draft":false,"summary":"这篇文章介绍了一个交互式机器翻译系统的主要功能和技术实现。系统的核心功能包括词级和句子级自动补全、增强翻译记忆等,采用了通用翻译模型、词汇约束、基于图的翻译记忆等技术。评估结果显示,该系统在词级准确率和BLEU分数上都有显著提升。","layout":"PostSimple","type":"Blog","readingTime":{"text":"4 min read","minutes":3.33,"time":199800,"words":666},"slug":"AI/2022-02-14-TranSmart-笔记","path":"blog/AI/2022-02-14-TranSmart-笔记","filePath":"blog/AI/2022-02-14-TranSmart-笔记.mdx","toc":[{"value":"2 System Features","url":"#2-system-features-1","depth":2},{"value":"3 Implemented Techniques","url":"#3-implemented-techniques-1","depth":2},{"value":"3.1 Generic Translation Model","url":"#31-generic-translation-model-1","depth":3},{"value":"Data Rejuvenation","url":"#data-rejuvenation-1","depth":4},{"value":"Data Augmentation","url":"#data-augmentation-1","depth":4},{"value":"3.2 General Word-level Autocompletion","url":"#32-general-word-level-autocompletion-1","depth":3},{"value":"3.3 Sentence-level Autocompletion by Lexical Constraints","url":"#33-sentence-level-autocompletion-by-lexical-constraints-1","depth":3},{"value":"3.4 Graph based Translation Memory","url":"#34-graph-based-translation-memory-1","depth":3},{"value":"3.5 others","url":"#35-others-1","depth":3},{"value":"word alignment","url":"#word-alignment-1","depth":4},{"value":"5 System Evaluation","url":"#5-system-evaluation-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"TranSmart-笔记","datePublished":"2022-02-14T00:00:00.000Z","dateModified":"2022-02-14T00:00:00.000Z","description":"这篇文章介绍了一个交互式机器翻译系统的主要功能和技术实现。系统的核心功能包括词级和句子级自动补全、增强翻译记忆等,采用了通用翻译模型、词汇约束、基于图的翻译记忆等技术。评估结果显示,该系统在词级准确率和BLEU分数上都有显著提升。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2022-02-14-TranSmart-笔记"}},{"title":"opennmt-tf_验证","date":"2022-02-10T00:00:00.000Z","tags":["AI"],"lastmod":"2022-02-10T00:00:00.000Z","draft":false,"summary":"这篇文章介绍了使用OpenNMT-tf进行机器翻译模型训练的步骤,包括创建词汇表、配置训练参数、开始训练、模型推理和BLEU评分计算。文章还比较了不同模型平均数量和beam search参数对BLEU评分的影响,最终得到最佳的模型配置。","layout":"PostSimple","type":"Blog","readingTime":{"text":"2 min read","minutes":1.21,"time":72600,"words":242},"slug":"AI/2022-02-10-opennmt-tf_验证","path":"blog/AI/2022-02-10-opennmt-tf_验证","filePath":"blog/AI/2022-02-10-opennmt-tf_验证.mdx","toc":[{"value":"创建vocab","url":"#创建vocab-1","depth":2},{"value":"开始训练","url":"#开始训练-3","depth":2},{"value":"Inference","url":"#inference-3","depth":2},{"value":"BLEU计算","url":"#bleu计算-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"opennmt-tf_验证","datePublished":"2022-02-10T00:00:00.000Z","dateModified":"2022-02-10T00:00:00.000Z","description":"这篇文章介绍了使用OpenNMT-tf进行机器翻译模型训练的步骤,包括创建词汇表、配置训练参数、开始训练、模型推理和BLEU评分计算。文章还比较了不同模型平均数量和beam search参数对BLEU评分的影响,最终得到最佳的模型配置。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2022-02-10-opennmt-tf_验证"}},{"title":"频率派vs贝叶斯派","date":"2022-02-10T00:00:00.000Z","tags":["AI"],"lastmod":"2022-02-10T00:00:00.000Z","draft":false,"summary":"这篇文章主要介绍了频率派和贝叶斯派对概率的不同诠释。频率派认为参数θ是常量，通过最大似然估计求解；贝叶斯派则认为θ满足先验分布，通过最大后验估计求解。文章还简要对比了两种方法的发展方向，频率派演变为优化问题，贝叶斯派发展为概率图模型。","layout":"PostSimple","type":"Blog","readingTime":{"text":"3 min read","minutes":2.51,"time":150600,"words":502},"slug":"AI/2022-02-10-频率派vs贝叶斯派","path":"blog/AI/2022-02-10-频率派vs贝叶斯派","filePath":"blog/AI/2022-02-10-频率派vs贝叶斯派.mdx","toc":[{"value":"Introduction","url":"#introduction-19","depth":1},{"value":"频率派的观点","url":"#频率派的观点-1","depth":2},{"value":"贝叶斯派的观点","url":"#贝叶斯派的观点-1","depth":2},{"value":"小结","url":"#小结-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"频率派vs贝叶斯派","datePublished":"2022-02-10T00:00:00.000Z","dateModified":"2022-02-10T00:00:00.000Z","description":"这篇文章主要介绍了频率派和贝叶斯派对概率的不同诠释。频率派认为参数θ是常量，通过最大似然估计求解；贝叶斯派则认为θ满足先验分布，通过最大后验估计求解。文章还简要对比了两种方法的发展方向，频率派演变为优化问题，贝叶斯派发展为概率图模型。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2022-02-10-频率派vs贝叶斯派"}},{"title":"笔记-A_Novel_Hybrid_Approach_to_Improve_Neural_Machine_Translation_Decoding_using_Phrase-Based_Statistical_Machine_Translation","date":"2022-01-25T00:00:00.000Z","tags":["AI"],"lastmod":"2022-01-25T00:00:00.000Z","draft":false,"summary":"这篇文章提出了一种结合短语统计机器翻译(SMT)来改进神经机器翻译(NMT)解码的混合方法。在beam search过程中,如果SMT翻译的token存在于beam中,就将其概率提升至beam中的最大概率。实验结果显示,该方法在某些策略下可以小幅提升BLEU和METEOR分数。","layout":"PostSimple","type":"Blog","readingTime":{"text":"2 min read","minutes":1.83,"time":109800,"words":366},"slug":"AI/2022-01-25-笔记-A_Novel_Hybrid_Approach_to_Improve_Neural_Machine_Translation_Decoding_using_Phrase-Based_Statistical_Machine_Translation","path":"blog/AI/2022-01-25-笔记-A_Novel_Hybrid_Approach_to_Improve_Neural_Machine_Translation_Decoding_using_Phrase-Based_Statistical_Machine_Translation","filePath":"blog/AI/2022-01-25-笔记-A_Novel_Hybrid_Approach_to_Improve_Neural_Machine_Translation_Decoding_using_Phrase-Based_Statistical_Machine_Translation.mdx","toc":[],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"笔记-A_Novel_Hybrid_Approach_to_Improve_Neural_Machine_Translation_Decoding_using_Phrase-Based_Statistical_Machine_Translation","datePublished":"2022-01-25T00:00:00.000Z","dateModified":"2022-01-25T00:00:00.000Z","description":"这篇文章提出了一种结合短语统计机器翻译(SMT)来改进神经机器翻译(NMT)解码的混合方法。在beam search过程中,如果SMT翻译的token存在于beam中,就将其概率提升至beam中的最大概率。实验结果显示,该方法在某些策略下可以小幅提升BLEU和METEOR分数。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2022-01-25-笔记-A_Novel_Hybrid_Approach_to_Improve_Neural_Machine_Translation_Decoding_using_Phrase-Based_Statistical_Machine_Translation"}},{"title":"笔记-Dynamic_Terminology_Integration_for_COVID-19_and_other_EmergingDomains","date":"2022-01-25T00:00:00.000Z","tags":["AI"],"lastmod":"2022-01-25T00:00:00.000Z","draft":false,"summary":"这篇文章提出了一种动态术语集成方法,用于提高新兴领域如COVID-19的机器翻译准确率。作者通过术语过滤、识别和集成等步骤,在不干扰训练过程的情况下提高了术语翻译的准确性,在测试集上实现了94%的COVID-19术语准确率。文章强调了高质量术语集的重要性,并指出术语改进对BLEU分数影响不大可能导致这方面研究被忽视。","layout":"PostSimple","type":"Blog","readingTime":{"text":"4 min read","minutes":3.79,"time":227400,"words":758},"slug":"AI/2022-01-25-笔记-Dynamic_Terminology_Integration_for_COVID-19_and_other_EmergingDomains","path":"blog/AI/2022-01-25-笔记-Dynamic_Terminology_Integration_for_COVID-19_and_other_EmergingDomains","filePath":"blog/AI/2022-01-25-笔记-Dynamic_Terminology_Integration_for_COVID-19_and_other_EmergingDomains.mdx","toc":[{"value":"2. Methods","url":"#2-methods-1","depth":2},{"value":"2.1 术语过滤","url":"#21-术语过滤-1","depth":3},{"value":"2.2 术语识别","url":"#22-术语识别-1","depth":3},{"value":"2.3 集成术语约束","url":"#23-集成术语约束-1","depth":3},{"value":"Result","url":"#result-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"笔记-Dynamic_Terminology_Integration_for_COVID-19_and_other_EmergingDomains","datePublished":"2022-01-25T00:00:00.000Z","dateModified":"2022-01-25T00:00:00.000Z","description":"这篇文章提出了一种动态术语集成方法,用于提高新兴领域如COVID-19的机器翻译准确率。作者通过术语过滤、识别和集成等步骤,在不干扰训练过程的情况下提高了术语翻译的准确性,在测试集上实现了94%的COVID-19术语准确率。文章强调了高质量术语集的重要性,并指出术语改进对BLEU分数影响不大可能导致这方面研究被忽视。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2022-01-25-笔记-Dynamic_Terminology_Integration_for_COVID-19_and_other_EmergingDomains","author":[{"@type":"Person","name":"Tz"}]}},{"title":"Google-GNMT","date":"2022-01-10T00:00:00.000Z","tags":["AI"],"lastmod":"2022-01-10T00:00:00.000Z","draft":false,"summary":"这篇文章介绍了Google的神经机器翻译系统,采用了深层LSTM、残差连接、注意力机制等技术来提高翻译质量。系统使用wordpiece模型来处理稀有词,并通过强化学习、beam search优化等方法进一步改进性能。在WMT14英法和英德翻译任务上取得了最佳结果,人工评测中比短语翻译系统错误减少60%。","layout":"PostSimple","type":"Blog","readingTime":{"text":"6 min read","minutes":5.21,"time":312600,"words":1042},"slug":"AI/2022-01-10-Google-GNMT","path":"blog/AI/2022-01-10-Google-GNMT","filePath":"blog/AI/2022-01-10-Google-GNMT.mdx","toc":[{"value":"Abstract","url":"#abstract-12","depth":2},{"value":"1 Introduction","url":"#1-introduction-5","depth":2},{"value":"3 Model Architecture","url":"#3-model-architecture-1","depth":2},{"value":"3.1 Residual Connections","url":"#31-residual-connections-1","depth":3},{"value":"3.2 Bi-directional Encoder for First Layer","url":"#32-bi-directional-encoder-for-first-layer-1","depth":3},{"value":"3.3 Model Parallelism","url":"#33-model-parallelism-1","depth":3},{"value":"4. Segmentation Approaches","url":"#4-segmentation-approaches-1","depth":2},{"value":"4.1 Wordpiece Model","url":"#41-wordpiece-model-1","depth":3},{"value":"4.2 Mixed Word/character Model","url":"#42-mixed-wordcharacter-model-1","depth":3},{"value":"5. Training Criteria","url":"#5-training-criteria-1","depth":2},{"value":"6. Quantizable Model and Quantized Inference","url":"#6-quantizable-model-and-quantized-inference-1","depth":2},{"value":"7. Decoder","url":"#7-decoder-1","depth":2},{"value":"8. Experiments and Results","url":"#8-experiments-and-results-1","depth":2},{"value":"8.1 Datasets","url":"#81-datasets-1","depth":3},{"value":"8.2 Evaluation Metrics","url":"#82-evaluation-metrics-1","depth":3},{"value":"8.3 Training Procedure","url":"#83-training-procedure-1","depth":3},{"value":"Evaluation after Maximum Likelihood Training","url":"#evaluation-after-maximum-likelihood-training-1","depth":3},{"value":"8.5 Evaliation of RL-refined Models","url":"#85-evaliation-of-rl-refined-models-1","depth":3},{"value":"8.6 Model Ensemble and Human Evaluation","url":"#86-model-ensemble-and-human-evaluation-1","depth":3},{"value":"8.7 Results on Production Data","url":"#87-results-on-production-data-1","depth":3}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"Google-GNMT","datePublished":"2022-01-10T00:00:00.000Z","dateModified":"2022-01-10T00:00:00.000Z","description":"这篇文章介绍了Google的神经机器翻译系统,采用了深层LSTM、残差连接、注意力机制等技术来提高翻译质量。系统使用wordpiece模型来处理稀有词,并通过强化学习、beam search优化等方法进一步改进性能。在WMT14英法和英德翻译任务上取得了最佳结果,人工评测中比短语翻译系统错误减少60%。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2022-01-10-Google-GNMT"}},{"title":"MT_paper简单笔记","date":"2021-12-23T00:00:00.000Z","tags":["AI"],"lastmod":"2021-12-23T00:00:00.000Z","draft":false,"summary":"这篇文章探讨了定制化神经机器翻译模型的开发，介绍了几个相关的开源项目。文章还证明了当前方法在领域适应、数据清洗和数据增强方面的实用性。另外，文章对句子级BLEU评分的平滑技术进行了系统比较，探讨了BLEU评分的应用原因。","layout":"PostSimple","type":"Blog","readingTime":{"text":"1 min read","minutes":0.33,"time":19800,"words":66},"slug":"AI/2021-12-23-MT_paper简单笔记","path":"blog/AI/2021-12-23-MT_paper简单笔记","filePath":"blog/AI/2021-12-23-MT_paper简单笔记.mdx","toc":[],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"MT_paper简单笔记","datePublished":"2021-12-23T00:00:00.000Z","dateModified":"2021-12-23T00:00:00.000Z","description":"这篇文章探讨了定制化神经机器翻译模型的开发，介绍了几个相关的开源项目。文章还证明了当前方法在领域适应、数据清洗和数据增强方面的实用性。另外，文章对句子级BLEU评分的平滑技术进行了系统比较，探讨了BLEU评分的应用原因。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-12-23-MT_paper简单笔记"}},{"title":"计算机辅助翻译-coursera课程1-3","date":"2021-12-23T00:00:00.000Z","tags":["AI"],"lastmod":"2021-12-23T00:00:00.000Z","draft":false,"summary":"这篇文章主要介绍了计算机辅助翻译(CAT)的相关知识,包括翻译过程、译员能力评估、翻译问题分类等。文章还讨论了翻译技术的组成,如翻译记忆、术语管理等,以及语料库在翻译研究和实践中的应用。最后介绍了一些常用的语料库检索工具。","layout":"PostSimple","type":"Blog","readingTime":{"text":"9 min read","minutes":8.815,"time":528900,"words":1763},"slug":"AI/2021-12-23-计算机辅助翻译-coursera课程1-3","path":"blog/AI/2021-12-23-计算机辅助翻译-coursera课程1-3","filePath":"blog/AI/2021-12-23-计算机辅助翻译-coursera课程1-3.mdx","toc":[{"value":"翻译过程的论述","url":"#翻译过程的论述-1","depth":2},{"value":"译员翻译能力评估","url":"#译员翻译能力评估-1","depth":2},{"value":"翻译问题分类","url":"#翻译问题分类-1","depth":2},{"value":"辞典问题","url":"#辞典问题-1","depth":2},{"value":"翻译技术组成","url":"#翻译技术组成-1","depth":2},{"value":"常用工具","url":"#常用工具-1","depth":3},{"value":"自动化翻译","url":"#自动化翻译-1","depth":3},{"value":"商用计算机辅助翻译软件","url":"#商用计算机辅助翻译软件-1","depth":3},{"value":"语料库","url":"#语料库-1","depth":3},{"value":"互联网搜索引擎/信息服务翻译实践","url":"#互联网搜索引擎信息服务翻译实践-1","depth":2},{"value":"搜索引擎的分类","url":"#搜索引擎的分类-1","depth":3},{"value":"基本工作原理","url":"#基本工作原理-1","depth":3},{"value":"使用规则","url":"#使用规则-1","depth":3},{"value":"学术数据库 电子期刊数据库","url":"#学术数据库-电子期刊数据库-1","depth":3},{"value":"学术数据库一般流程","url":"#学术数据库一般流程-1","depth":3},{"value":"语料库与翻译研究","url":"#语料库与翻译研究-1","depth":2},{"value":"语料库产生背景","url":"#语料库产生背景-1","depth":3},{"value":"语料库建设","url":"#语料库建设-1","depth":3},{"value":"语料库特点","url":"#语料库特点-1","depth":3},{"value":"语料库支持的翻译研究和实践","url":"#语料库支持的翻译研究和实践-1","depth":3},{"value":"语料库检索工具","url":"#语料库检索工具-1","depth":3}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"计算机辅助翻译-coursera课程1-3","datePublished":"2021-12-23T00:00:00.000Z","dateModified":"2021-12-23T00:00:00.000Z","description":"这篇文章主要介绍了计算机辅助翻译(CAT)的相关知识,包括翻译过程、译员能力评估、翻译问题分类等。文章还讨论了翻译技术的组成,如翻译记忆、术语管理等,以及语料库在翻译研究和实践中的应用。最后介绍了一些常用的语料库检索工具。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-12-23-计算机辅助翻译-coursera课程1-3"}},{"title":"Deep_Transformer（DLCL,_pre-norm）","date":"2021-12-14T00:00:00.000Z","tags":["AI"],"lastmod":"2021-12-14T00:00:00.000Z","draft":false,"summary":"这篇文章提出了两种方法来改进Transformer模型用于机器翻译:pre-norm和dlcl。这些方法可以训练更深的网络,缓解梯度消失问题,同时减小模型大小并加快训练速度。实验结果显示BLEU分数提升0.4-2.4分,但整体性能提升不大。","layout":"PostSimple","type":"Blog","readingTime":{"text":"1 min read","minutes":0.645,"time":38700,"words":129},"slug":"AI/2021-12-14-Deep_Transformer（DLCL,_pre-norm）","path":"blog/AI/2021-12-14-Deep_Transformer（DLCL,_pre-norm）","filePath":"blog/AI/2021-12-14-Deep_Transformer（DLCL,_pre-norm）.mdx","toc":[{"value":"pre-norm","url":"#pre-norm-1","depth":2},{"value":"Dynamic Linear Combination of Layers","url":"#dynamic-linear-combination-of-layers-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"Deep_Transformer（DLCL,_pre-norm）","datePublished":"2021-12-14T00:00:00.000Z","dateModified":"2021-12-14T00:00:00.000Z","description":"这篇文章提出了两种方法来改进Transformer模型用于机器翻译:pre-norm和dlcl。这些方法可以训练更深的网络,缓解梯度消失问题,同时减小模型大小并加快训练速度。实验结果显示BLEU分数提升0.4-2.4分,但整体性能提升不大。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-12-14-Deep_Transformer（DLCL,_pre-norm）"}},{"title":"tensor2tensor框架记录","date":"2021-12-09T00:00:00.000Z","tags":["AI"],"lastmod":"2021-12-09T00:00:00.000Z","draft":false,"summary":"这篇文章主要讨论了tensor2tensor和tensorflow的版本依赖问题，以及一些重要参数的设置。文章重点介绍了学习率的计算方式，包括constant、linear_warmup、rsqrt_decay和rsqrt_hidden_size四个部分，并提供了一个Python函数来计算学习率。","layout":"PostSimple","type":"Blog","readingTime":{"text":"1 min read","minutes":0.635,"time":38100,"words":127},"slug":"AI/2021-12-09-tensor2tensor框架记录","path":"blog/AI/2021-12-09-tensor2tensor框架记录","filePath":"blog/AI/2021-12-09-tensor2tensor框架记录.mdx","toc":[],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"tensor2tensor框架记录","datePublished":"2021-12-09T00:00:00.000Z","dateModified":"2021-12-09T00:00:00.000Z","description":"这篇文章主要讨论了tensor2tensor和tensorflow的版本依赖问题，以及一些重要参数的设置。文章重点介绍了学习率的计算方式，包括constant、linear_warmup、rsqrt_decay和rsqrt_hidden_size四个部分，并提供了一个Python函数来计算学习率。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-12-09-tensor2tensor框架记录"}},{"title":"邻近搜索","date":"2021-12-05T00:00:00.000Z","tags":["AI"],"lastmod":"2021-12-05T00:00:00.000Z","draft":false,"summary":"这篇文章主要介绍了几种用于大数据场景下邻近搜索的算法,包括Annoy、HNSW、KD Tree和LSH。文章重点讲解了Annoy和HNSW两种算法的原理和实现方法,Annoy通过建立二叉树来实现快速查找,HNSW则是基于图结构并引入了分层机制来提高搜索效率。","layout":"PostSimple","type":"Blog","readingTime":{"text":"5 min read","minutes":4.115,"time":246900,"words":823},"slug":"AI/2021-12-05-邻近搜索","path":"blog/AI/2021-12-05-邻近搜索","filePath":"blog/AI/2021-12-05-邻近搜索.mdx","toc":[{"value":"邻近搜索","url":"#邻近搜索-1","depth":3},{"value":"Annoy","url":"#annoy-1","depth":4},{"value":"HNSW","url":"#hnsw-1","depth":4},{"value":"KD Tree","url":"#kd-tree-1","depth":4},{"value":"LSH 局部敏感哈希","url":"#lsh-局部敏感哈希-1","depth":4}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"邻近搜索","datePublished":"2021-12-05T00:00:00.000Z","dateModified":"2021-12-05T00:00:00.000Z","description":"这篇文章主要介绍了几种用于大数据场景下邻近搜索的算法,包括Annoy、HNSW、KD Tree和LSH。文章重点讲解了Annoy和HNSW两种算法的原理和实现方法,Annoy通过建立二叉树来实现快速查找,HNSW则是基于图结构并引入了分层机制来提高搜索效率。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-12-05-邻近搜索","author":[{"@type":"Person","name":"Tz"}]}},{"title":"HMM_参数估计","date":"2021-11-28T00:00:00.000Z","tags":["AI"],"lastmod":"2021-11-28T00:00:00.000Z","draft":false,"summary":"这篇文章主要介绍了隐马尔可夫模型(HMM)的基本概念和两个主要任务:推断和参数估计。文章详细讲解了完整数据和不完整数据情况下的参数估计方法,包括EM算法、前向-后向算法等,并给出了估计初始概率分布、发射概率和转移概率矩阵的具体步骤。","layout":"PostSimple","type":"Blog","readingTime":{"text":"3 min read","minutes":2.425,"time":145500,"words":485},"slug":"AI/2021-11-28-HMM_参数估计","path":"blog/AI/2021-11-28-HMM_参数估计","filePath":"blog/AI/2021-11-28-HMM_参数估计.mdx","toc":[{"value":"HMM基本概念","url":"#hmm基本概念-1","depth":2},{"value":"参数","url":"#参数-1","depth":3},{"value":"Two Major Tasks","url":"#two-major-tasks-1","depth":3},{"value":"Inference","url":"#inference-2","depth":4},{"value":"参数估计","url":"#参数估计-1","depth":4},{"value":"Complete Case","url":"#complete-case-1","depth":5},{"value":"Incomplete Case","url":"#incomplete-case-1","depth":5},{"value":"估计PI","url":"#估计pi-1","depth":6},{"value":"估计B","url":"#估计b-1","depth":6},{"value":"估计A","url":"#估计a-1","depth":6}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"HMM_参数估计","datePublished":"2021-11-28T00:00:00.000Z","dateModified":"2021-11-28T00:00:00.000Z","description":"这篇文章主要介绍了隐马尔可夫模型(HMM)的基本概念和两个主要任务:推断和参数估计。文章详细讲解了完整数据和不完整数据情况下的参数估计方法,包括EM算法、前向-后向算法等,并给出了估计初始概率分布、发射概率和转移概率矩阵的具体步骤。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-11-28-HMM_参数估计"}},{"title":"梯度消失和BN","date":"2021-11-28T00:00:00.000Z","tags":["AI"],"lastmod":"2021-11-28T00:00:00.000Z","draft":false,"summary":"这篇文章主要讨论了深度学习中的梯度消失问题及其解决方案,以及不同的归一化方法(如BN、LN、WN等)。文章指出,归一化方法可以缓解协变量偏移问题,加速网络收敛,并具有权重和数据伸缩不变性,从而提高模型的鲁棒性和泛化能力。","layout":"PostSimple","type":"Blog","readingTime":{"text":"8 min read","minutes":7.25,"time":435000,"words":1450},"slug":"AI/2021-11-28-梯度消失和BN","path":"blog/AI/2021-11-28-梯度消失和BN","filePath":"blog/AI/2021-11-28-梯度消失和BN.mdx","toc":[{"value":"梯度消失","url":"#梯度消失-1","depth":2},{"value":"Normalization BN/LN/WN","url":"#normalization-bnlnwn-1","depth":2},{"value":"Covariate shift问题：","url":"#covariate-shift问题-1","depth":3},{"value":"BN","url":"#bn-1","depth":3},{"value":"LN","url":"#ln-1","depth":3},{"value":"WN","url":"#wn-1","depth":3},{"value":"CN-Cosine Normalization","url":"#cn-cosine-normalization-1","depth":3},{"value":"Normalization为什么有效","url":"#normalization为什么有效-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"梯度消失和BN","datePublished":"2021-11-28T00:00:00.000Z","dateModified":"2021-11-28T00:00:00.000Z","description":"这篇文章主要讨论了深度学习中的梯度消失问题及其解决方案,以及不同的归一化方法(如BN、LN、WN等)。文章指出,归一化方法可以缓解协变量偏移问题,加速网络收敛,并具有权重和数据伸缩不变性,从而提高模型的鲁棒性和泛化能力。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-11-28-梯度消失和BN"}},{"title":"生物医学BERT","date":"2021-11-23T00:00:00.000Z","tags":["AI"],"lastmod":"2021-11-23T00:00:00.000Z","draft":false,"summary":"这篇文章介绍了几个生物医学领域的预训练语言模型,包括BioBERT、中文MC-BERT、Clinical BERT和Med-BERT。这些模型都是在大规模生物医学文本数据上进行预训练,以适应生物医学文本挖掘任务。文章还比较了不同模型的训练数据、训练策略和下游任务表现。","layout":"PostSimple","type":"Blog","readingTime":{"text":"2 min read","minutes":1.455,"time":87300,"words":291},"slug":"AI/2021-11-23-生物医学BERT","path":"blog/AI/2021-11-23-生物医学BERT","filePath":"blog/AI/2021-11-23-生物医学BERT.mdx","toc":[],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"生物医学BERT","datePublished":"2021-11-23T00:00:00.000Z","dateModified":"2021-11-23T00:00:00.000Z","description":"这篇文章介绍了几个生物医学领域的预训练语言模型,包括BioBERT、中文MC-BERT、Clinical BERT和Med-BERT。这些模型都是在大规模生物医学文本数据上进行预训练,以适应生物医学文本挖掘任务。文章还比较了不同模型的训练数据、训练策略和下游任务表现。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-11-23-生物医学BERT"}},{"title":"不同数据噪音对SMT_NMT模型的影响","date":"2021-11-18T00:00:00.000Z","tags":["AI"],"lastmod":"2021-11-18T00:00:00.000Z","draft":false,"summary":"这篇文章研究了不同类型噪音数据对神经机器翻译(NMT)和统计机器翻译(SMT)的影响。结果表明，NMT对噪音数据更敏感，特别是未翻译句子对NMT影响最大；而SMT对噪音数据的抵抗力较强。短句段(2-5个词)对两种模型都有轻微的增强作用。","layout":"PostSimple","type":"Blog","readingTime":{"text":"1 min read","minutes":0.955,"time":57300,"words":191},"slug":"AI/2021-11-18-不同数据噪音对SMT_NMT模型的影响","path":"blog/AI/2021-11-18-不同数据噪音对SMT_NMT模型的影响","filePath":"blog/AI/2021-11-18-不同数据噪音对SMT_NMT模型的影响.mdx","toc":[],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"不同数据噪音对SMT_NMT模型的影响","datePublished":"2021-11-18T00:00:00.000Z","dateModified":"2021-11-18T00:00:00.000Z","description":"这篇文章研究了不同类型噪音数据对神经机器翻译(NMT)和统计机器翻译(SMT)的影响。结果表明，NMT对噪音数据更敏感，特别是未翻译句子对NMT影响最大；而SMT对噪音数据的抵抗力较强。短句段(2-5个词)对两种模型都有轻微的增强作用。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-11-18-不同数据噪音对SMT_NMT模型的影响"}},{"title":"机器翻译模型架构记录","date":"2021-11-17T00:00:00.000Z","tags":["AI"],"lastmod":"2021-11-17T00:00:00.000Z","draft":false,"summary":"这篇文章主要讨论了机器翻译架构的一些研究发现。文章指出LSTM作为解码器在某些情况下性能优于Transformer解码器,并探讨了embedding大小、双向LSTM、注意力机制等因素对翻译性能的影响。文章还比较了不同架构的训练时间和BLEU得分,发现LSTM训练速度快,而基础Transformer模型效果较好。","layout":"PostSimple","type":"Blog","readingTime":{"text":"2 min read","minutes":1.645,"time":98700,"words":329},"slug":"AI/2021-11-17-机器翻译模型架构记录","path":"blog/AI/2021-11-17-机器翻译模型架构记录","filePath":"blog/AI/2021-11-17-机器翻译模型架构记录.mdx","toc":[],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"机器翻译模型架构记录","datePublished":"2021-11-17T00:00:00.000Z","dateModified":"2021-11-17T00:00:00.000Z","description":"这篇文章主要讨论了机器翻译架构的一些研究发现。文章指出LSTM作为解码器在某些情况下性能优于Transformer解码器,并探讨了embedding大小、双向LSTM、注意力机制等因素对翻译性能的影响。文章还比较了不同架构的训练时间和BLEU得分,发现LSTM训练速度快,而基础Transformer模型效果较好。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-11-17-机器翻译模型架构记录"}},{"title":"Facebook_AI_2021_WMT论文笔记","date":"2021-11-15T00:00:00.000Z","tags":["AI"],"lastmod":"2021-11-15T00:00:00.000Z","draft":false,"summary":"Facebook AI在WMT21新闻翻译任务中采用了多语言翻译模型和Mixture-of-Expert技术，在14个翻译方向上取得了第一名的成绩。他们使用了加深的Transformer模型作为基线，并通过大规模回译、增加训练数据、模型微调和模型平均等技术进一步提升了翻译质量。","layout":"PostSimple","type":"Blog","readingTime":{"text":"2 min read","minutes":1.165,"time":69900,"words":233},"slug":"AI/2021-11-15-Facebook_AI_2021_WMT论文笔记","path":"blog/AI/2021-11-15-Facebook_AI_2021_WMT论文笔记","filePath":"blog/AI/2021-11-15-Facebook_AI_2021_WMT论文笔记.mdx","toc":[],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"Facebook_AI_2021_WMT论文笔记","datePublished":"2021-11-15T00:00:00.000Z","dateModified":"2021-11-15T00:00:00.000Z","description":"Facebook AI在WMT21新闻翻译任务中采用了多语言翻译模型和Mixture-of-Expert技术，在14个翻译方向上取得了第一名的成绩。他们使用了加深的Transformer模型作为基线，并通过大规模回译、增加训练数据、模型微调和模型平均等技术进一步提升了翻译质量。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-11-15-Facebook_AI_2021_WMT论文笔记","author":[{"@type":"Person","name":"Tz"}]}},{"title":"Transformer模型训练技巧","date":"2021-11-10T00:00:00.000Z","tags":["AI"],"lastmod":"2021-11-10T00:00:00.000Z","draft":false,"summary":"这篇文章主要探讨了影响Transformer模型训练质量和效率的各种参数设置。作者通过大量实验分析了batch size、学习率、warmup steps、最大句子长度等参数对模型性能的影响,并给出了一些实用的调参建议。文章还比较了不同GPU配置下的训练效果,以及checkpoint averaging等技巧对提升BLEU分数的作用。","layout":"PostSimple","type":"Blog","readingTime":{"text":"9 min read","minutes":8.49,"time":509400,"words":1698},"slug":"AI/2021-11-10-Transformer模型训练技巧","path":"blog/AI/2021-11-10-Transformer模型训练技巧","filePath":"blog/AI/2021-11-10-Transformer模型训练技巧.mdx","toc":[{"value":"摘要","url":"#摘要-3","depth":2},{"value":"评估方法","url":"#评估方法-1","depth":2},{"value":"模型训练停止条件","url":"#模型训练停止条件-1","depth":3},{"value":"数据 与 预处理","url":"#数据-与-预处理-1","depth":2},{"value":"实验","url":"#实验-2","depth":2},{"value":"计算速度 数据吞吐量","url":"#计算速度-数据吞吐量-1","depth":3},{"value":"训练数据大小","url":"#训练数据大小-1","depth":3},{"value":"模型大小","url":"#模型大小-1","depth":3},{"value":"最大训练句子长度","url":"#最大训练句子长度-1","depth":3},{"value":"batch size","url":"#batch-size-1","depth":3},{"value":"学习率和Warmup steps","url":"#学习率和warmup-steps-1","depth":3},{"value":"GPUs","url":"#gpus-1","depth":3},{"value":"多GPU下学习率和warmup step","url":"#多gpu下学习率和warmup-step-1","depth":3},{"value":"重新开始训练","url":"#重新开始训练-1","depth":3},{"value":"checkpoint averaging","url":"#checkpoint-averaging-1","depth":3},{"value":"WMT17 比较","url":"#wmt17-比较-1","depth":2},{"value":"结论","url":"#结论-3","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"Transformer模型训练技巧","datePublished":"2021-11-10T00:00:00.000Z","dateModified":"2021-11-10T00:00:00.000Z","description":"这篇文章主要探讨了影响Transformer模型训练质量和效率的各种参数设置。作者通过大量实验分析了batch size、学习率、warmup steps、最大句子长度等参数对模型性能的影响,并给出了一些实用的调参建议。文章还比较了不同GPU配置下的训练效果,以及checkpoint averaging等技巧对提升BLEU分数的作用。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-11-10-Transformer模型训练技巧"}},{"title":"2021_EMNLP_机器翻译预训练和回译的互补性","date":"2021-11-09T00:00:00.000Z","tags":["AI"],"lastmod":"2021-11-09T00:00:00.000Z","draft":false,"summary":"研究了预训练(PT)和反向翻译(BT)对神经机器翻译模型的影响。研究发现PT主要作用于编码器,BT主要作用于解码器,两者具有互补性。结合PT和BT可以提高翻译质量,在WMT16英语-罗马尼亚语和英语-俄语任务上取得了最先进的结果。","layout":"PostSimple","type":"Blog","readingTime":{"text":"5 min read","minutes":4.965,"time":297900,"words":993},"slug":"AI/2021-11-09-2021_EMNLP_机器翻译预训练和回译的互补性","path":"blog/AI/2021-11-09-2021_EMNLP_机器翻译预训练和回译的互补性","filePath":"blog/AI/2021-11-09-2021_EMNLP_机器翻译预训练和回译的互补性.mdx","toc":[{"value":"摘要","url":"#摘要-2","depth":2},{"value":"介绍","url":"#介绍-3","depth":2},{"value":"预备知识","url":"#预备知识-1","depth":2},{"value":"背景","url":"#背景-3","depth":3},{"value":"实验设置","url":"#实验设置-1","depth":3},{"value":"理解PT和BT","url":"#理解pt和bt-1","depth":2},{"value":"使用BT和PT提升","url":"#使用bt和pt提升-1","depth":2},{"value":"未来工作","url":"#未来工作-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"2021_EMNLP_机器翻译预训练和回译的互补性","datePublished":"2021-11-09T00:00:00.000Z","dateModified":"2021-11-09T00:00:00.000Z","description":"研究了预训练(PT)和反向翻译(BT)对神经机器翻译模型的影响。研究发现PT主要作用于编码器,BT主要作用于解码器,两者具有互补性。结合PT和BT可以提高翻译质量,在WMT16英语-罗马尼亚语和英语-俄语任务上取得了最先进的结果。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-11-09-2021_EMNLP_机器翻译预训练和回译的互补性"}},{"title":"机器翻译中域内小样本微调的正则","date":"2021-11-04T00:00:00.000Z","tags":["AI"],"lastmod":"2021-11-04T00:00:00.000Z","draft":false,"summary":"探讨了在神经机器翻译中使用小规模领域数据进行微调时的过拟合问题。作者测试了三种正则化技术(Dropout、MAP-L2和Tuneout)来防止过拟合,发现使用Dropout和MAP-L2的组合可以使训练更加稳定,并显著提高BLEU评分。实验结果表明,正则化技术可以有效缓解微调过程中的过拟合问题,提高模型在小数据集上的泛化能力。","layout":"PostSimple","type":"Blog","readingTime":{"text":"2 min read","minutes":1.88,"time":112800,"words":376},"slug":"AI/2021-11-04-机器翻译中域内小样本微调的正则","path":"blog/AI/2021-11-04-机器翻译中域内小样本微调的正则","filePath":"blog/AI/2021-11-04-机器翻译中域内小样本微调的正则.mdx","toc":[{"value":"正则化技术","url":"#正则化技术-1","depth":2},{"value":"Dropout","url":"#dropout-1","depth":3},{"value":"MAP-L2","url":"#map-l2-1","depth":3},{"value":"Tuneout","url":"#tuneout-1","depth":3},{"value":"结果","url":"#结果-4","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"机器翻译中域内小样本微调的正则","datePublished":"2021-11-04T00:00:00.000Z","dateModified":"2021-11-04T00:00:00.000Z","description":"探讨了在神经机器翻译中使用小规模领域数据进行微调时的过拟合问题。作者测试了三种正则化技术(Dropout、MAP-L2和Tuneout)来防止过拟合,发现使用Dropout和MAP-L2的组合可以使训练更加稳定,并显著提高BLEU评分。实验结果表明,正则化技术可以有效缓解微调过程中的过拟合问题,提高模型在小数据集上的泛化能力。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-11-04-机器翻译中域内小样本微调的正则"}},{"title":"对比学习-减少翻译漏词错误","date":"2021-11-02T00:00:00.000Z","tags":["AI"],"lastmod":"2021-11-02T00:00:00.000Z","draft":false,"summary":"一种基于对比学习的方法来减少神经机器翻译中的词语遗漏错误。通过随机遗漏、按词频遗漏和按词性遗漏三种方式构建负例，并使用最大边际损失来微调翻译模型，从而提高翻译质量。","layout":"PostSimple","type":"Blog","readingTime":{"text":"1 min read","minutes":0.365,"time":21900,"words":73},"slug":"AI/2021-11-02-对比学习-减少翻译漏词错误","path":"blog/AI/2021-11-02-对比学习-减少翻译漏词错误","filePath":"blog/AI/2021-11-02-对比学习-减少翻译漏词错误.mdx","toc":[],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"对比学习-减少翻译漏词错误","datePublished":"2021-11-02T00:00:00.000Z","dateModified":"2021-11-02T00:00:00.000Z","description":"一种基于对比学习的方法来减少神经机器翻译中的词语遗漏错误。通过随机遗漏、按词频遗漏和按词性遗漏三种方式构建负例，并使用最大边际损失来微调翻译模型，从而提高翻译质量。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-11-02-对比学习-减少翻译漏词错误"}},{"title":"2021_transformer_综述（部分）","date":"2021-11-01T00:00:00.000Z","tags":["AI"],"lastmod":"2021-11-01T00:00:00.000Z","draft":false,"summary":"Transformer模型的发展和优化方向。文章分析了Transformer在模型效率、泛化能力和领域适应性方面的改进,并将优化工作分为架构改进、预训练和应用三个方面。文章重点讨论了注意力机制的优化,包括稀疏注意力、线性化注意力等方法,以解决长序列计算复杂度高和缺乏归纳偏置的问题。","layout":"PostSimple","type":"Blog","readingTime":{"text":"6 min read","minutes":5.7,"time":342000,"words":1140},"slug":"AI/2021-11-01-2021_transformer_综述（部分）","path":"blog/AI/2021-11-01-2021_transformer_综述（部分）","filePath":"blog/AI/2021-11-01-2021_transformer_综述（部分）.mdx","toc":[{"value":"1 Introduction","url":"#1-introduction-4","depth":2},{"value":"2. BACKGROUND","url":"#2-background-1","depth":2},{"value":"2.3 Model Analysis","url":"#23-model-analysis-1","depth":3},{"value":"2.4 与其他模型对比","url":"#24-与其他模型对比-1","depth":3},{"value":"self attention分析","url":"#self-attention分析-1","depth":4},{"value":"归纳偏差","url":"#归纳偏差-1","depth":4},{"value":"3. Transformer分类","url":"#3-transformer分类-1","depth":2},{"value":"4. Attention","url":"#4-attention-1","depth":2},{"value":"Sparse Attention","url":"#sparse-attention-1","depth":3},{"value":"基于位置","url":"#基于位置-1","depth":4},{"value":"基于内容","url":"#基于内容-1","depth":4},{"value":"Linearized Attention","url":"#linearized-attention-1","depth":3},{"value":"待续...","url":"#待续-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"2021_transformer_综述（部分）","datePublished":"2021-11-01T00:00:00.000Z","dateModified":"2021-11-01T00:00:00.000Z","description":"Transformer模型的发展和优化方向。文章分析了Transformer在模型效率、泛化能力和领域适应性方面的改进,并将优化工作分为架构改进、预训练和应用三个方面。文章重点讨论了注意力机制的优化,包括稀疏注意力、线性化注意力等方法,以解决长序列计算复杂度高和缺乏归纳偏置的问题。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-11-01-2021_transformer_综述（部分）"}},{"title":"truecase和detruecase使用","date":"2021-10-25T00:00:00.000Z","tags":["AI"],"lastmod":"2021-10-25T00:00:00.000Z","draft":false,"summary":"truecase模型文件的结构和使用方法。文章解释了模型如何记录单词的大小写出现次数,以及在truecase过程中如何保留某些词的原有大小写形式。最后提到通常需要在truecase后进行detruecase,以恢复句首字母的大写。","layout":"PostSimple","type":"Blog","readingTime":{"text":"1 min read","minutes":0.775,"time":46500,"words":155},"slug":"AI/2021-10-25-truecase和detruecase使用","path":"blog/AI/2021-10-25-truecase和detruecase使用","filePath":"blog/AI/2021-10-25-truecase和detruecase使用.mdx","toc":[],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"truecase和detruecase使用","datePublished":"2021-10-25T00:00:00.000Z","dateModified":"2021-10-25T00:00:00.000Z","description":"truecase模型文件的结构和使用方法。文章解释了模型如何记录单词的大小写出现次数,以及在truecase过程中如何保留某些词的原有大小写形式。最后提到通常需要在truecase后进行detruecase,以恢复句首字母的大写。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-10-25-truecase和detruecase使用"}},{"title":"2018-2019_Tecent_AI_Lab_machine_translation_相关内容","date":"2021-10-13T00:00:00.000Z","tags":["AI"],"lastmod":"2021-10-13T00:00:00.000Z","draft":false,"summary":"这篇文章主要介绍了机器翻译中的语言理解和条件生成两个关键问题。文章讨论了深度网络、多头注意力机制和自注意力网络等方面的改进,以提高语言理解能力。在条件生成方面,文章提出了鲁棒transformer、全面转换和信息流等方法来优化生成过程。","layout":"PostSimple","type":"Blog","readingTime":{"text":"4 min read","minutes":3.865,"time":231900,"words":773},"slug":"AI/2021-10-13-2018-2019_Tecent_AI_Lab_machine_translation_相关内容","path":"blog/AI/2021-10-13-2018-2019_Tecent_AI_Lab_machine_translation_相关内容","filePath":"blog/AI/2021-10-13-2018-2019_Tecent_AI_Lab_machine_translation_相关内容.mdx","toc":[{"value":"Language Understanding","url":"#language-understanding-1","depth":2},{"value":"Deep Networks","url":"#deep-networks-1","depth":3},{"value":"Multi-Head Attention","url":"#multi-head-attention-1","depth":3},{"value":"Self-Attention","url":"#self-attention-1","depth":3},{"value":"Encoder Representation Interpretation","url":"#encoder-representation-interpretation-1","depth":3},{"value":"Conditional Generation","url":"#conditional-generation-1","depth":2},{"value":"Robust Transformer","url":"#robust-transformer-1","depth":3},{"value":"Full transformation","url":"#full-transformation-1","depth":3},{"value":"Information Flow","url":"#information-flow-1","depth":3},{"value":"2018 CIPS Summer","url":"#2018-cips-summer-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"2018-2019_Tecent_AI_Lab_machine_translation_相关内容","datePublished":"2021-10-13T00:00:00.000Z","dateModified":"2021-10-13T00:00:00.000Z","description":"这篇文章主要介绍了机器翻译中的语言理解和条件生成两个关键问题。文章讨论了深度网络、多头注意力机制和自注意力网络等方面的改进,以提高语言理解能力。在条件生成方面,文章提出了鲁棒transformer、全面转换和信息流等方法来优化生成过程。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-10-13-2018-2019_Tecent_AI_Lab_machine_translation_相关内容","author":[{"@type":"Person","name":"Tz"}]}},{"title":"为什么使用self-attention,机器翻译下的评估","date":"2021-10-09T00:00:00.000Z","tags":["AI"],"lastmod":"2021-10-09T00:00:00.000Z","draft":false,"summary":"这篇文章主要比较了RNNs、CNNs和self-attention网络在机器翻译中的表现。实验发现,在长距离主谓一致任务中,RNNs的表现优于CNNs和self-attention网络;而在词义消歧任务中,self-attention网络(Transformer)的语义特征提取能力最强。文章指出评估神经机器翻译模型架构需要考虑内在因素的权衡,而不仅仅关注BLEU分数。","layout":"PostSimple","type":"Blog","readingTime":{"text":"6 min read","minutes":5.5,"time":330000,"words":1100},"slug":"AI/2021-10-09-为什么使用self-attention,机器翻译下的评估","path":"blog/AI/2021-10-09-为什么使用self-attention,机器翻译下的评估","filePath":"blog/AI/2021-10-09-为什么使用self-attention,机器翻译下的评估.mdx","toc":[{"value":"Introduction","url":"#introduction-18","depth":2},{"value":"相关工作","url":"#相关工作-2","depth":2},{"value":"背景","url":"#背景-2","depth":2},{"value":"主谓一致","url":"#主谓一致-1","depth":2},{"value":"针对CNNs研究","url":"#针对cnns研究-1","depth":3},{"value":"RNNs vs. Transformer","url":"#rnns-vs-transformer-1","depth":3},{"value":"WSD词义消歧","url":"#wsd词义消歧-1","depth":2},{"value":"Post-publication Experiments","url":"#post-publication-experiments-1","depth":2},{"value":"结论","url":"#结论-2","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"为什么使用self-attention,机器翻译下的评估","datePublished":"2021-10-09T00:00:00.000Z","dateModified":"2021-10-09T00:00:00.000Z","description":"这篇文章主要比较了RNNs、CNNs和self-attention网络在机器翻译中的表现。实验发现,在长距离主谓一致任务中,RNNs的表现优于CNNs和self-attention网络;而在词义消歧任务中,self-attention网络(Transformer)的语义特征提取能力最强。文章指出评估神经机器翻译模型架构需要考虑内在因素的权衡,而不仅仅关注BLEU分数。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-10-09-为什么使用self-attention,机器翻译下的评估"}},{"title":"2021_Facebook_AI_BPE对Transformer模型记忆力的影响","date":"2021-10-08T00:00:00.000Z","tags":["AI"],"lastmod":"2021-10-08T00:00:00.000Z","draft":false,"summary":"这篇文章主要研究了BPE词表大小对Transformer模型记忆能力的影响。实验表明,增加BPE词表大小可以提高模型的记忆能力,原因可能是BPE减少了训练序列的长度。作者通过三个任务验证了这一结论,并排除了其他可能的解释,最终确定序列长度的减少是观察到记忆效果增强的主要因素。","layout":"PostSimple","type":"Blog","readingTime":{"text":"6 min read","minutes":5.495,"time":329700,"words":1099},"slug":"AI/2021-10-08-2021_Facebook_AI_BPE对Transformer模型记忆力的影响","path":"blog/AI/2021-10-08-2021_Facebook_AI_BPE对Transformer模型记忆力的影响","filePath":"blog/AI/2021-10-08-2021_Facebook_AI_BPE对Transformer模型记忆力的影响.mdx","toc":[{"value":"学习记忆任务","url":"#学习记忆任务-1","depth":2},{"value":"学习随机标签映射","url":"#学习随机标签映射-1","depth":3},{"value":"成员推理","url":"#成员推理-1","depth":3},{"value":"训练数据恢复","url":"#训练数据恢复-1","depth":3},{"value":"模型和超参数","url":"#模型和超参数-1","depth":2},{"value":"BPE 设置","url":"#bpe-设置-1","depth":3},{"value":"控制学习参数的数量","url":"#控制学习参数的数量-1","depth":3},{"value":"改变模型为分类器","url":"#改变模型为分类器-1","depth":3},{"value":"模型和训练细节","url":"#模型和训练细节-1","depth":3},{"value":"BPE影响记忆实验","url":"#bpe影响记忆实验-1","depth":2},{"value":"解释","url":"#解释-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"2021_Facebook_AI_BPE对Transformer模型记忆力的影响","datePublished":"2021-10-08T00:00:00.000Z","dateModified":"2021-10-08T00:00:00.000Z","description":"这篇文章主要研究了BPE词表大小对Transformer模型记忆能力的影响。实验表明,增加BPE词表大小可以提高模型的记忆能力,原因可能是BPE减少了训练序列的长度。作者通过三个任务验证了这一结论,并排除了其他可能的解释,最终确定序列长度的减少是观察到记忆效果增强的主要因素。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-10-08-2021_Facebook_AI_BPE对Transformer模型记忆力的影响"}},{"title":"2018_ACL_迭代回译","date":"2021-09-29T00:00:00.000Z","tags":["AI"],"lastmod":"2021-09-29T00:00:00.000Z","draft":false,"summary":"这篇文章主要介绍了迭代回译技术在神经机器翻译中的应用。研究表明,使用高质量的模型进行回译可以显著提升翻译质量,在高资源和低资源场景下都能取得良好效果。文章还探讨了单语数据利用、模型质量影响等相关问题,为迭代回译技术的应用提供了实践指导。","layout":"PostSimple","type":"Blog","readingTime":{"text":"4 min read","minutes":3.78,"time":226800,"words":756},"slug":"AI/2021-09-29-2018_ACL_迭代回译","path":"blog/AI/2021-09-29-2018_ACL_迭代回译","filePath":"blog/AI/2021-09-29-2018_ACL_迭代回译.mdx","toc":[{"value":"单语数据利用相关工作","url":"#单语数据利用相关工作-1","depth":2},{"value":"模型回译质量的影响","url":"#模型回译质量的影响-1","depth":2},{"value":"高资源数据实验","url":"#高资源数据实验-1","depth":2},{"value":"低资源数据实验","url":"#低资源数据实验-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"2018_ACL_迭代回译","datePublished":"2021-09-29T00:00:00.000Z","dateModified":"2021-09-29T00:00:00.000Z","description":"这篇文章主要介绍了迭代回译技术在神经机器翻译中的应用。研究表明,使用高质量的模型进行回译可以显著提升翻译质量,在高资源和低资源场景下都能取得良好效果。文章还探讨了单语数据利用、模型质量影响等相关问题,为迭代回译技术的应用提供了实践指导。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-09-29-2018_ACL_迭代回译","author":[{"@type":"Person","name":"Tz"}]}},{"title":"2018_ACL_The_Best_of_Both_Worlds:_Combining_Recent_Advances_in_Neural_Machine_翻译笔记","date":"2021-09-28T00:00:00.000Z","tags":["AI"],"lastmod":"2021-09-28T00:00:00.000Z","draft":false,"summary":"这篇论文提出了改进的RNMT+模型,单模型效果优于Transformer和原始RNN。作者对多头注意力、层归一化等技术进行了消融分析,并通过混合Transformer和RNMT+的编码器和解码器,实验出了更好的模型架构。","layout":"PostSimple","type":"Blog","readingTime":{"text":"7 min read","minutes":6.87,"time":412200,"words":1374},"slug":"AI/2021-09-28-2018_ACL_The_Best_of_Both_Worlds:_Combining_Recent_Advances_in_Neural_Machine_翻译笔记","path":"blog/AI/2021-09-28-2018_ACL_The_Best_of_Both_Worlds:_Combining_Recent_Advances_in_Neural_Machine_翻译笔记","filePath":"blog/AI/2021-09-28-2018_ACL_The_Best_of_Both_Worlds:_Combining_Recent_Advances_in_Neural_Machine_翻译笔记.mdx","toc":[{"value":"Introduction","url":"#introduction-17","depth":2},{"value":"Background","url":"#background-3","depth":2},{"value":"RNN-based NMT Models -RNMT","url":"#rnn-based-nmt-models--rnmt-1","depth":3},{"value":"Convolutional NMT Models - ConvS2S","url":"#convolutional-nmt-models---convs2s-1","depth":2},{"value":"Conditional Transformation-based NMT Models - Transformer","url":"#conditional-transformation-based-nmt-models---transformer-1","depth":3},{"value":"A Theory-Based Characterization of NMT Architectures","url":"#a-theory-based-characterization-of-nmt-architectures-1","depth":3},{"value":"Experiment Setup","url":"#experiment-setup-1","depth":2},{"value":"RNMT+","url":"#rnmt-1","depth":2},{"value":"模型分析比较","url":"#模型分析比较-1","depth":3},{"value":"消融研究","url":"#消融研究-1","depth":2},{"value":"混合NMT模型","url":"#混合nmt模型-1","depth":2},{"value":"Conclusion","url":"#conclusion-1","depth":2},{"value":"模型参数说明","url":"#模型参数说明-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"2018_ACL_The_Best_of_Both_Worlds:_Combining_Recent_Advances_in_Neural_Machine_翻译笔记","datePublished":"2021-09-28T00:00:00.000Z","dateModified":"2021-09-28T00:00:00.000Z","description":"这篇论文提出了改进的RNMT+模型,单模型效果优于Transformer和原始RNN。作者对多头注意力、层归一化等技术进行了消融分析,并通过混合Transformer和RNMT+的编码器和解码器,实验出了更好的模型架构。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-09-28-2018_ACL_The_Best_of_Both_Worlds:_Combining_Recent_Advances_in_Neural_Machine_翻译笔记"}},{"title":"机器翻译的理解","date":"2021-09-27T00:00:00.000Z","tags":["AI"],"lastmod":"2021-09-27T00:00:00.000Z","draft":false,"summary":"这篇文章主要讨论了机器翻译和大规模预训练语言模型在语义理解和系统性知识利用方面的不足。文章指出,当前机器翻译主要依赖于数据和统计规则,缺乏深层语义理解和世界知识的应用,导致翻译质量受限。同时,大规模预训练语言模型虽然规模庞大,但在语义和知识利用方面仍存在不足,需要更合理的内部结构和机制支撑。","layout":"PostSimple","type":"Blog","readingTime":{"text":"3 min read","minutes":2.345,"time":140700,"words":469},"slug":"AI/2021-09-27-机器翻译的理解","path":"blog/AI/2021-09-27-机器翻译的理解","filePath":"blog/AI/2021-09-27-机器翻译的理解.mdx","toc":[{"value":"深度学习机器翻译","url":"#深度学习机器翻译-1","depth":2},{"value":"大规模预训练语言模型","url":"#大规模预训练语言模型-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"机器翻译的理解","datePublished":"2021-09-27T00:00:00.000Z","dateModified":"2021-09-27T00:00:00.000Z","description":"这篇文章主要讨论了机器翻译和大规模预训练语言模型在语义理解和系统性知识利用方面的不足。文章指出,当前机器翻译主要依赖于数据和统计规则,缺乏深层语义理解和世界知识的应用,导致翻译质量受限。同时,大规模预训练语言模型虽然规模庞大,但在语义和知识利用方面仍存在不足,需要更合理的内部结构和机制支撑。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-09-27-机器翻译的理解"}},{"title":"GPT2_领域数据微调","date":"2021-09-16T00:00:00.000Z","tags":["AI"],"lastmod":"2021-09-16T00:00:00.000Z","draft":false,"summary":"这篇文章介绍了如何使用transformers 4.11.0对GPT2-small 12层模型进行微调。文章详细说明了环境准备、数据准备和训练过程，包括使用run_clm.py脚本进行单机多卡训练的具体步骤。最后，文章还解释了如何计算模型的困惑度，即对模型输出的损失进行指数运算。","layout":"PostSimple","type":"Blog","readingTime":{"text":"2 min read","minutes":1.845,"time":110700,"words":369},"slug":"AI/2021-09-16-GPT2_领域数据微调","path":"blog/AI/2021-09-16-GPT2_领域数据微调","filePath":"blog/AI/2021-09-16-GPT2_领域数据微调.mdx","toc":[{"value":"环境准备","url":"#环境准备-1","depth":2},{"value":"数据准备","url":"#数据准备-1","depth":2},{"value":"开始训练","url":"#开始训练-2","depth":2},{"value":"困惑度的计算","url":"#困惑度的计算-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"GPT2_领域数据微调","datePublished":"2021-09-16T00:00:00.000Z","dateModified":"2021-09-16T00:00:00.000Z","description":"这篇文章介绍了如何使用transformers 4.11.0对GPT2-small 12层模型进行微调。文章详细说明了环境准备、数据准备和训练过程，包括使用run_clm.py脚本进行单机多卡训练的具体步骤。最后，文章还解释了如何计算模型的困惑度，即对模型输出的损失进行指数运算。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-09-16-GPT2_领域数据微调"}},{"title":"EM算法","date":"2021-09-11T00:00:00.000Z","tags":["AI"],"lastmod":"2021-09-11T00:00:00.000Z","draft":false,"summary":"这篇文章主要介绍了Jensen不等式和期望最大化(EM)算法。文章首先定义了Jensen不等式,并给出了一个图形化的例子。然后详细推导了EM算法,包括E步和M步,并用抛硬币的例子说明了EM算法的应用过程。","layout":"PostSimple","type":"Blog","readingTime":{"text":"4 min read","minutes":3.29,"time":197400,"words":658},"slug":"AI/2021-09-11-EM算法","path":"blog/AI/2021-09-11-EM算法","filePath":"blog/AI/2021-09-11-EM算法.mdx","toc":[{"value":"Jensen's Inequality","url":"#jensens-inequality-1","depth":2},{"value":"定义","url":"#定义-2","depth":3},{"value":"例子","url":"#例子-5","depth":3},{"value":"Expectation Maximization","url":"#expectation-maximization-1","depth":2},{"value":"硬币的例子","url":"#硬币的例子-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"EM算法","datePublished":"2021-09-11T00:00:00.000Z","dateModified":"2021-09-11T00:00:00.000Z","description":"这篇文章主要介绍了Jensen不等式和期望最大化(EM)算法。文章首先定义了Jensen不等式,并给出了一个图形化的例子。然后详细推导了EM算法,包括E步和M步,并用抛硬币的例子说明了EM算法的应用过程。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-09-11-EM算法"}},{"title":"sentencepiece_user_defined&control_symbols","date":"2021-09-09T00:00:00.000Z","tags":["AI"],"lastmod":"2021-09-09T00:00:00.000Z","draft":false,"summary":"SentencePiece库中用户自定义符号和控制符号的使用方法。用户自定义符号在任何上下文中都被视为一个token，可以在输入句子中出现；而控制符号只保留ID，即使出现在输入文本中也不会被作为一个token处理，用户需要在编码后显式插入ID。","layout":"PostSimple","type":"Blog","readingTime":{"text":"1 min read","minutes":0.845,"time":50700,"words":169},"slug":"AI/2021-09-09-sentencepiece_user_defined&control_symbols","path":"blog/AI/2021-09-09-sentencepiece_user_defined&control_symbols","filePath":"blog/AI/2021-09-09-sentencepiece_user_defined&control_symbols.mdx","toc":[],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"sentencepiece_user_defined&control_symbols","datePublished":"2021-09-09T00:00:00.000Z","dateModified":"2021-09-09T00:00:00.000Z","description":"SentencePiece库中用户自定义符号和控制符号的使用方法。用户自定义符号在任何上下文中都被视为一个token，可以在输入句子中出现；而控制符号只保留ID，即使出现在输入文本中也不会被作为一个token处理，用户需要在编码后显式插入ID。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-09-09-sentencepiece_user_defined&control_symbols"}},{"title":"概率分布-熵","date":"2021-09-09T00:00:00.000Z","tags":["AI"],"lastmod":"2021-09-09T00:00:00.000Z","draft":false,"summary":"这篇文章主要介绍了概率论中的一些基本概念和特性，包括均值、方差、期望、熵等。文章还通过一个具体的天气预报例子，展示了如何计算联合熵、条件熵和互信息，说明了随机变量之间的相互依赖关系。","layout":"PostSimple","type":"Blog","readingTime":{"text":"4 min read","minutes":3.235,"time":194100,"words":647},"slug":"AI/2021-09-09-概率分布-熵","path":"blog/AI/2021-09-09-概率分布-熵","filePath":"blog/AI/2021-09-09-概率分布-熵.mdx","toc":[{"value":"数据样本集合的特性","url":"#数据样本集合的特性-1","depth":2},{"value":"均值","url":"#均值-1","depth":3},{"value":"方差","url":"#方差-2","depth":3},{"value":"概率分布的特性","url":"#概率分布的特性-1","depth":2},{"value":"期望","url":"#期望-1","depth":3},{"value":"方差","url":"#方差-3","depth":3},{"value":"熵","url":"#熵-1","depth":3},{"value":"联合熵","url":"#联合熵-1","depth":3},{"value":"条件熵","url":"#条件熵-1","depth":3},{"value":"互信息","url":"#互信息-1","depth":3},{"value":"例子","url":"#例子-4","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"概率分布-熵","datePublished":"2021-09-09T00:00:00.000Z","dateModified":"2021-09-09T00:00:00.000Z","description":"这篇文章主要介绍了概率论中的一些基本概念和特性，包括均值、方差、期望、熵等。文章还通过一个具体的天气预报例子，展示了如何计算联合熵、条件熵和互信息，说明了随机变量之间的相互依赖关系。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-09-09-概率分布-熵"}},{"title":"使用SMT特征提高NMT-2016_AAAI_百度","date":"2021-09-07T00:00:00.000Z","tags":["AI"],"lastmod":"2021-09-07T00:00:00.000Z","draft":false,"summary":"这篇文章提出了一种在对数线性框架下将统计机器翻译(SMT)特征与神经机器翻译(NMT)模型集成的方法,以改进NMT的性能。作者组合了三个SMT特征:翻译模型、单词奖励特征和n-gram语言模型,解决了NMT中的OOV问题、翻译不充分问题,并利用了大规模单语数据。实验结果表明,该方法在NIST中英翻译测试集上提升了2.33 BLEU分。","layout":"PostSimple","type":"Blog","readingTime":{"text":"6 min read","minutes":5.335,"time":320100,"words":1067},"slug":"AI/2021-09-07-使用SMT特征提高NMT-2016_AAAI_百度","path":"blog/AI/2021-09-07-使用SMT特征提高NMT-2016_AAAI_百度","filePath":"blog/AI/2021-09-07-使用SMT特征提高NMT-2016_AAAI_百度.mdx","toc":[{"value":"Abstract","url":"#abstract-11","depth":2},{"value":"Introduction","url":"#introduction-16","depth":2},{"value":"Background","url":"#background-2","depth":2},{"value":"log-linear NMT","url":"#log-linear-nmt-1","depth":2},{"value":"Feature Definetion","url":"#feature-definetion-1","depth":3},{"value":"Handling the OOV problem","url":"#handling-the-oov-problem-1","depth":3},{"value":"Decoding","url":"#decoding-1","depth":3},{"value":"Experiments","url":"#experiments-2","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"使用SMT特征提高NMT-2016_AAAI_百度","datePublished":"2021-09-07T00:00:00.000Z","dateModified":"2021-09-07T00:00:00.000Z","description":"这篇文章提出了一种在对数线性框架下将统计机器翻译(SMT)特征与神经机器翻译(NMT)模型集成的方法,以改进NMT的性能。作者组合了三个SMT特征:翻译模型、单词奖励特征和n-gram语言模型,解决了NMT中的OOV问题、翻译不充分问题,并利用了大规模单语数据。实验结果表明,该方法在NIST中英翻译测试集上提升了2.33 BLEU分。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-09-07-使用SMT特征提高NMT-2016_AAAI_百度"}},{"title":"翻译-Finding_the_Words_to_Say:_Hiddent_State_Visualizations_for_Language_Models","date":"2021-08-31T00:00:00.000Z","tags":["AI"],"lastmod":"2021-08-31T00:00:00.000Z","draft":false,"summary":"这篇文章介绍了通过可视化GPT2-XL语言模型隐层状态来探索模型思考过程的方法。文章展示了如何将隐层状态映射到词表并使用softmax计算概率,以及如何查看每层输出token的排名变化,从而分析模型在不同层级的决策过程。通过这些可视化技术,可以洞察模型的内部工作机制,包括句子结构识别、关键词预测以及潜在的性别偏见等。","layout":"PostSimple","type":"Blog","readingTime":{"text":"4 min read","minutes":3.76,"time":225600,"words":752},"slug":"AI/2021-08-31-翻译-Finding_the_Words_to_Say:_Hiddent_State_Visualizations_for_Language_Models","path":"blog/AI/2021-08-31-翻译-Finding_the_Words_to_Say:_Hiddent_State_Visualizations_for_Language_Models","filePath":"blog/AI/2021-08-31-翻译-Finding_the_Words_to_Say:_Hiddent_State_Visualizations_for_Language_Models.mdx","toc":[{"value":"将隐层映射到词表并softmax","url":"#将隐层映射到词表并softmax-1","depth":2},{"value":"查看每层的输出token的排名","url":"#查看每层的输出token的排名-1","depth":2},{"value":"比较相同位置的多个token排名","url":"#比较相同位置的多个token排名-1","depth":2},{"value":"探索的偏见","url":"#探索的偏见-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"翻译-Finding_the_Words_to_Say:_Hiddent_State_Visualizations_for_Language_Models","datePublished":"2021-08-31T00:00:00.000Z","dateModified":"2021-08-31T00:00:00.000Z","description":"这篇文章介绍了通过可视化GPT2-XL语言模型隐层状态来探索模型思考过程的方法。文章展示了如何将隐层状态映射到词表并使用softmax计算概率,以及如何查看每层输出token的排名变化,从而分析模型在不同层级的决策过程。通过这些可视化技术,可以洞察模型的内部工作机制,包括句子结构识别、关键词预测以及潜在的性别偏见等。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-08-31-翻译-Finding_the_Words_to_Say:_Hiddent_State_Visualizations_for_Language_Models"}},{"title":"PR-ROC-AUC曲线","date":"2021-08-24T00:00:00.000Z","tags":["AI"],"lastmod":"2021-08-24T00:00:00.000Z","draft":false,"summary":"这篇文章主要介绍了PR曲线和ROC-AUC曲线的概念及特点。PR曲线反映了查准率和查全率之间的关系,通过调节置信度阈值来绘制。ROC-AUC曲线则反映了真正类率和假正类率的关系,其面积不受正负样本比例影响。","layout":"PostSimple","type":"Blog","readingTime":{"text":"3 min read","minutes":2.88,"time":172800,"words":576},"slug":"AI/2021-08-24-PR-ROC-AUC曲线","path":"blog/AI/2021-08-24-PR-ROC-AUC曲线","filePath":"blog/AI/2021-08-24-PR-ROC-AUC曲线.mdx","toc":[{"value":"PR曲线","url":"#pr曲线-1","depth":2},{"value":"ROC-AUC","url":"#roc-auc-1","depth":3}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"PR-ROC-AUC曲线","datePublished":"2021-08-24T00:00:00.000Z","dateModified":"2021-08-24T00:00:00.000Z","description":"这篇文章主要介绍了PR曲线和ROC-AUC曲线的概念及特点。PR曲线反映了查准率和查全率之间的关系,通过调节置信度阈值来绘制。ROC-AUC曲线则反映了真正类率和假正类率的关系,其面积不受正负样本比例影响。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-08-24-PR-ROC-AUC曲线"}},{"title":"基于字典的跨域神经机器翻译数据增强","date":"2021-08-19T00:00:00.000Z","tags":["AI"],"lastmod":"2021-08-19T00:00:00.000Z","draft":false,"summary":"这篇文章介绍了一种基于字典的数据增强方法,用于跨领域神经机器翻译。该方法通过使用平行领域字典和非领域平行语料,创建伪领域平行语料,主要步骤包括短语句子嵌入、匹配、对齐和替换。实验结果表明,该方法可以有效提高领域覆盖率,改善跨领域神经机器翻译的性能。","layout":"PostSimple","type":"Blog","readingTime":{"text":"3 min read","minutes":2.59,"time":155400,"words":518},"slug":"AI/2021-08-19-基于字典的跨域神经机器翻译数据增强","path":"blog/AI/2021-08-19-基于字典的跨域神经机器翻译数据增强","filePath":"blog/AI/2021-08-19-基于字典的跨域神经机器翻译数据增强.mdx","toc":[{"value":"Dictionary-based Data Augmentation for Cross-domain NMT","url":"#dictionary-based-data-augmentation-for-cross-domain-nmt-1","depth":2},{"value":"Phrase and Sentence Embedding","url":"#phrase-and-sentence-embedding-1","depth":3},{"value":"Phrase-sentence Matching","url":"#phrase-sentence-matching-1","depth":3},{"value":"Phrase Matching, Alignment and Substitution","url":"#phrase-matching-alignment-and-substitution-1","depth":3},{"value":"Experiment settings","url":"#experiment-settings-1","depth":2},{"value":"Further Analysis","url":"#further-analysis-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"基于字典的跨域神经机器翻译数据增强","datePublished":"2021-08-19T00:00:00.000Z","dateModified":"2021-08-19T00:00:00.000Z","description":"这篇文章介绍了一种基于字典的数据增强方法,用于跨领域神经机器翻译。该方法通过使用平行领域字典和非领域平行语料,创建伪领域平行语料,主要步骤包括短语句子嵌入、匹配、对齐和替换。实验结果表明,该方法可以有效提高领域覆盖率,改善跨领域神经机器翻译的性能。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-08-19-基于字典的跨域神经机器翻译数据增强","author":[{"@type":"Person","name":"Tz"}]}},{"title":"WMT2021-新闻-wechat","date":"2021-08-18T00:00:00.000Z","tags":["AI"],"lastmod":"2021-08-18T00:00:00.000Z","draft":false,"summary":"这篇文章介绍了WeChat在WMT21机器翻译比赛中使用的神经机器翻译系统。主要通过改进模型架构(如Pre-Norm和Post-Norm Transformer、AAN等)和大规模合成数据生成(如反向翻译、知识蒸馏等)来提高翻译性能。文章还探讨了一些先进的微调技术和基于Self-BLEU的模型集成方法,以进一步提升系统效果。","layout":"PostSimple","type":"Blog","readingTime":{"text":"10 min read","minutes":9.625,"time":577500,"words":1925},"slug":"AI/2021-08-18-WMT2021-新闻-wechat","path":"blog/AI/2021-08-18-WMT2021-新闻-wechat","filePath":"blog/AI/2021-08-18-WMT2021-新闻-wechat.mdx","toc":[{"value":"Abstract","url":"#abstract-10","depth":2},{"value":"Introduction","url":"#introduction-15","depth":2},{"value":"Model Archtectures","url":"#model-archtectures-1","depth":2},{"value":"model configurations","url":"#model-configurations-1","depth":3},{"value":"Transformer with Diffetent Layer-Norm","url":"#transformer-with-diffetent-layer-norm-1","depth":3},{"value":"Average Attention Transformer","url":"#average-attention-transformer-1","depth":3},{"value":"Weighted Attention Transformer","url":"#weighted-attention-transformer-1","depth":3},{"value":"Mix-AAN Transformer","url":"#mix-aan-transformer-1","depth":3},{"value":"Talking-Heads Attention","url":"#talking-heads-attention-1","depth":3},{"value":"System Overview","url":"#system-overview-5","depth":2},{"value":"Data Filter","url":"#data-filter-1","depth":3},{"value":"General Domain Synthetic Data","url":"#general-domain-synthetic-data-1","depth":3},{"value":"结果","url":"#结果-3","depth":2},{"value":"不同技巧的BLEU","url":"#不同技巧的bleu-1","depth":3},{"value":"深度宽度网络和Mix-AAN网络的集成效果对比","url":"#深度宽度网络和mix-aan网络的集成效果对比-1","depth":3},{"value":"搜索算法的时间和性能","url":"#搜索算法的时间和性能-1","depth":3},{"value":"不同微调方法","url":"#不同微调方法-1","depth":3}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"WMT2021-新闻-wechat","datePublished":"2021-08-18T00:00:00.000Z","dateModified":"2021-08-18T00:00:00.000Z","description":"这篇文章介绍了WeChat在WMT21机器翻译比赛中使用的神经机器翻译系统。主要通过改进模型架构(如Pre-Norm和Post-Norm Transformer、AAN等)和大规模合成数据生成(如反向翻译、知识蒸馏等)来提高翻译性能。文章还探讨了一些先进的微调技术和基于Self-BLEU的模型集成方法,以进一步提升系统效果。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-08-18-WMT2021-新闻-wechat"}},{"title":"WMT2019-新闻-Facebook","date":"2021-08-16T00:00:00.000Z","tags":["AI"],"lastmod":"2021-08-16T00:00:00.000Z","draft":false,"summary":"这篇文章介绍了一个机器翻译系统,主要针对英德和英俄翻译方向。系统采用了大规模反向翻译、数据过滤、模型集成和噪声信道模型重排等技术,相比2018年提升了4.5个BLEU分。文章详细描述了数据预处理、模型训练和解码等各个环节的具体做法。","layout":"PostSimple","type":"Blog","readingTime":{"text":"5 min read","minutes":4.355,"time":261300,"words":871},"slug":"AI/2021-08-16-WMT2019-新闻-Facebook","path":"blog/AI/2021-08-16-WMT2019-新闻-Facebook","filePath":"blog/AI/2021-08-16-WMT2019-新闻-Facebook.mdx","toc":[{"value":"Abstract","url":"#abstract-9","depth":2},{"value":"Introduction","url":"#introduction-14","depth":2},{"value":"Data","url":"#data-1","depth":2},{"value":"Data preprocessing","url":"#data-preprocessing-1","depth":3},{"value":"Data filtering","url":"#data-filtering-1","depth":3},{"value":"System Overview","url":"#system-overview-4","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"WMT2019-新闻-Facebook","datePublished":"2021-08-16T00:00:00.000Z","dateModified":"2021-08-16T00:00:00.000Z","description":"这篇文章介绍了一个机器翻译系统,主要针对英德和英俄翻译方向。系统采用了大规模反向翻译、数据过滤、模型集成和噪声信道模型重排等技术,相比2018年提升了4.5个BLEU分。文章详细描述了数据预处理、模型训练和解码等各个环节的具体做法。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-08-16-WMT2019-新闻-Facebook"}},{"title":"CCMT2020-OPPO","date":"2021-08-15T00:00:00.000Z","tags":["AI"],"lastmod":"2021-08-15T00:00:00.000Z","draft":false,"summary":"这篇文章主要介绍了OPPO在CCMT 2020机器翻译比赛中的系统设计和发现。OPPO在7个任务方向中的6个排名第一,主要采用了Transformer模型,并使用了回译、领域微调、知识蒸馏等技术。文章还发现,在低资源语料上简单应用不同的中文分词工具,可以在多个任务中带来明显的性能提升。","layout":"PostSimple","type":"Blog","readingTime":{"text":"8 min read","minutes":7.745,"time":464700,"words":1549},"slug":"AI/2021-08-15-CCMT2020-OPPO","path":"blog/AI/2021-08-15-CCMT2020-OPPO","filePath":"blog/AI/2021-08-15-CCMT2020-OPPO.mdx","toc":[{"value":"Introduction","url":"#introduction-12","depth":2},{"value":"Applying Multiple Word Segmentation Tools","url":"#applying-multiple-word-segmentation-tools-1","depth":2},{"value":"English ↔ Chinese Machine Translation Task","url":"#english--chinese-machine-translation-task-1","depth":2},{"value":"Japanese → English Translation Task (Patent Domain)","url":"#japanese--english-translation-task-patent-domain-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"CCMT2020-OPPO","datePublished":"2021-08-15T00:00:00.000Z","dateModified":"2021-08-15T00:00:00.000Z","description":"这篇文章主要介绍了OPPO在CCMT 2020机器翻译比赛中的系统设计和发现。OPPO在7个任务方向中的6个排名第一,主要采用了Transformer模型,并使用了回译、领域微调、知识蒸馏等技术。文章还发现,在低资源语料上简单应用不同的中文分词工具,可以在多个任务中带来明显的性能提升。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-08-15-CCMT2020-OPPO"}},{"title":"WMT2020-新闻-小牛","date":"2021-08-15T00:00:00.000Z","tags":["AI"],"lastmod":"2021-08-15T00:00:00.000Z","draft":false,"summary":"这篇文章介绍了NiuTrans团队在WMT20机器翻译评测中的系统。该系统在日英和英日翻译方向上排名第一,主要应用了迭代回译、宽深Transformer模型、迭代知识蒸馏和迭代微调等技术。系统的训练步骤包括数据预处理、生成伪数据、多样化翻译模型、知识蒸馏、领域微调和后处理等。","layout":"PostSimple","type":"Blog","readingTime":{"text":"6 min read","minutes":5.065,"time":303900,"words":1013},"slug":"AI/2021-08-15-WMT2020-新闻-小牛","path":"blog/AI/2021-08-15-WMT2020-新闻-小牛","filePath":"blog/AI/2021-08-15-WMT2020-新闻-小牛.mdx","toc":[{"value":"Introduction","url":"#introduction-13","depth":2},{"value":"System overview","url":"#system-overview-3","depth":2},{"value":"Data Preprocessing and Filtering","url":"#data-preprocessing-and-filtering-1","depth":3},{"value":"Iterativa Back Translation","url":"#iterativa-back-translation-1","depth":3},{"value":"Multilingual Model","url":"#multilingual-model-1","depth":3},{"value":"Model Architectures and Ensemble","url":"#model-architectures-and-ensemble-1","depth":3},{"value":"Iterative KD and Fine-tuning","url":"#iterative-kd-and-fine-tuning-1","depth":3},{"value":"Reranking","url":"#reranking-3","depth":3},{"value":"Post Editing","url":"#post-editing-1","depth":3},{"value":"Experiment","url":"#experiment-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"WMT2020-新闻-小牛","datePublished":"2021-08-15T00:00:00.000Z","dateModified":"2021-08-15T00:00:00.000Z","description":"这篇文章介绍了NiuTrans团队在WMT20机器翻译评测中的系统。该系统在日英和英日翻译方向上排名第一,主要应用了迭代回译、宽深Transformer模型、迭代知识蒸馏和迭代微调等技术。系统的训练步骤包括数据预处理、生成伪数据、多样化翻译模型、知识蒸馏、领域微调和后处理等。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-08-15-WMT2020-新闻-小牛"}},{"title":"Atman机器翻译模型笔记","date":"2021-08-13T00:00:00.000Z","tags":["AI"],"lastmod":"2021-08-13T00:00:00.000Z","draft":false,"summary":"这篇文章主要介绍了机器翻译模型的训练过程和数据处理方法。文章详细描述了从大规模语料训练基础模型到领域精调的完整流程,包括使用平行语料和单语回译语料进行训练,以及采用联合训练、EWC约束等技术来优化模型性能。此外,文章还讨论了语料清洗和数据增强的方法,如使用语言模型、句对相似度和统计规则进行数据筛选,以及通过反向翻译和对偶学习等技术来扩充训练数据。","layout":"PostSimple","type":"Blog","readingTime":{"text":"8 min read","minutes":7.215,"time":432900,"words":1443},"slug":"AI/2021-08-13-Atman机器翻译模型笔记","path":"blog/AI/2021-08-13-Atman机器翻译模型笔记","filePath":"blog/AI/2021-08-13-Atman机器翻译模型笔记.mdx","toc":[{"value":"Atman","url":"#atman-1","depth":3}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"Atman机器翻译模型笔记","datePublished":"2021-08-13T00:00:00.000Z","dateModified":"2021-08-13T00:00:00.000Z","description":"这篇文章主要介绍了机器翻译模型的训练过程和数据处理方法。文章详细描述了从大规模语料训练基础模型到领域精调的完整流程,包括使用平行语料和单语回译语料进行训练,以及采用联合训练、EWC约束等技术来优化模型性能。此外,文章还讨论了语料清洗和数据增强的方法,如使用语言模型、句对相似度和统计规则进行数据筛选,以及通过反向翻译和对偶学习等技术来扩充训练数据。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-08-13-Atman机器翻译模型笔记","author":[{"@type":"Person","name":"Tz"}]}},{"title":"WMT2020-生物医学-华为","date":"2021-08-13T00:00:00.000Z","tags":["AI"],"lastmod":"2021-08-13T00:00:00.000Z","draft":false,"summary":"华为在WMT20生物医学翻译任务中的方法。研究者探讨了领域内字典对提高跨领域神经机器翻译性能的影响,并利用预训练机器翻译模型进行迁移学习。通过领域数据增强、重排序等技术,在英-法、英-德、英-意大利语对上取得了最先进的结果。","layout":"PostSimple","type":"Blog","readingTime":{"text":"4 min read","minutes":3.58,"time":214800,"words":716},"slug":"AI/2021-08-13-WMT2020-生物医学-华为","path":"blog/AI/2021-08-13-WMT2020-生物医学-华为","filePath":"blog/AI/2021-08-13-WMT2020-生物医学-华为.mdx","toc":[{"value":"Abstract","url":"#abstract","depth":2},{"value":"Introduction","url":"#introduction","depth":2},{"value":"The Data","url":"#the-data","depth":2},{"value":"The Approaches","url":"#the-approaches","depth":2},{"value":"In-domain dictionary","url":"#in-domain-dictionary","depth":3},{"value":"Reranking","url":"#reranking","depth":3},{"value":"Data Processing","url":"#data-processing","depth":3},{"value":"Experimental Results","url":"#experimental-results","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"WMT2020-生物医学-华为","datePublished":"2021-08-13T00:00:00.000Z","dateModified":"2021-08-13T00:00:00.000Z","description":"华为在WMT20生物医学翻译任务中的方法。研究者探讨了领域内字典对提高跨领域神经机器翻译性能的影响,并利用预训练机器翻译模型进行迁移学习。通过领域数据增强、重排序等技术,在英-法、英-德、英-意大利语对上取得了最先进的结果。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-08-13-WMT2020-生物医学-华为"}},{"title":"理解大规模反向翻译","date":"2021-08-13T00:00:00.000Z","tags":["AI"],"lastmod":"2021-08-13T00:00:00.000Z","draft":false,"summary":"1. 研究发现在反向翻译中,使用采样(sampling)或带噪声的束搜索(noised beam search)生成合成数据比标准束搜索或贪心搜索更有效,可以提供更强的训练信号。2. 通过大规模实验比较了合成数据和真实双语数据的效果,以及不同领域数据的影响,在WMT14英德翻译任务上达到了35 BLEU的最佳结果。","layout":"PostSimple","type":"Blog","readingTime":{"text":"7 min read","minutes":6.845,"time":410700,"words":1369},"slug":"AI/2021-08-13-理解大规模反向翻译","path":"blog/AI/2021-08-13-理解大规模反向翻译","filePath":"blog/AI/2021-08-13-理解大规模反向翻译.mdx","toc":[{"value":"Abstract","url":"#abstract-8","depth":2},{"value":"Introduction","url":"#introduction-11","depth":2},{"value":"Generating synthetic sources","url":"#generating-synthetic-sources-1","depth":2},{"value":"Model and hyperparameters","url":"#model-and-hyperparameters-1","depth":2},{"value":"Results","url":"#results-1","depth":2},{"value":"生成方法对比","url":"#生成方法对比-1","depth":3},{"value":"生成方法分析","url":"#生成方法分析-1","depth":3},{"value":"低资源和高资源设置","url":"#低资源和高资源设置-1","depth":3},{"value":"领域的合成数据","url":"#领域的合成数据-1","depth":3},{"value":"bitext上采样","url":"#bitext上采样-1","depth":3},{"value":"大规模上的结果","url":"#大规模上的结果-1","depth":3},{"value":"\bWMT18","url":"#wmt18-1","depth":2}],"structuredData":{"@context":"https://schema.org","@type":"BlogPosting","headline":"理解大规模反向翻译","datePublished":"2021-08-13T00:00:00.000Z","dateModified":"2021-08-13T00:00:00.000Z","description":"1. 研究发现在反向翻译中,使用采样(sampling)或带噪声的束搜索(noised beam search)生成合成数据比标准束搜索或贪心搜索更有效,可以提供更强的训练信号。2. 通过大规模实验比较了合成数据和真实双语数据的效果,以及不同领域数据的影响,在WMT14英德翻译任务上达到了35 BLEU的最佳结果。","image":"/static/images/twitter-card.png","url":"https://tailwind-nextjs-starter-blog.vercel.app/blog/AI/2021-08-13-理解大规模反向翻译"}}],"title":"Ai"}]],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","tags","children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","tags","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"zh-cn","className":"__variable_587f35 scroll-smooth ","suppressHydrationWarning":true,"children":[["$","link",null,{"rel":"apple-touch-icon","sizes":"76x76","href":"/static/favicons/apple-touch-icon.png"}],["$","link",null,{"rel":"icon","type":"image/png","sizes":"32x32","href":"/static/favicons/favicon-32x32.png"}],["$","link",null,{"rel":"icon","type":"image/png","sizes":"16x16","href":"/static/favicons/favicon-16x16.png"}],["$","link",null,{"rel":"icon","type":"image/png","href":"/static/favicons/favicon.ico"}],["$","link",null,{"rel":"manifest","href":"/static/favicons/site.webmanifest"}],["$","link",null,{"rel":"mask-icon","href":"/static/favicons/safari-pinned-tab.svg","color":"#5bbad5"}],["$","meta",null,{"name":"msapplication-TileColor","content":"#000000"}],["$","meta",null,{"name":"theme-color","media":"(prefers-color-scheme: light)","content":"#fff"}],["$","meta",null,{"name":"theme-color","media":"(prefers-color-scheme: dark)","content":"#000"}],["$","link",null,{"rel":"alternate","type":"application/rss+xml","href":"/feed.xml"}],["$","body",null,{"className":"scrollbar-track-slate-400 bg-background pl-[calc(100vw-100%)] antialiased ","children":[["$","$L6",null,{"gaId":"G-JJL8NM5GV2"}],["$","$L7",null,{"children":[["$undefined","$undefined","$undefined",["$","$L8",null,{"async":true,"defer":true,"data-website-id":"$undefined","src":"https://analytics.umami.is/script.js"}],"$undefined","$undefined"],["$","section",null,{"className":"mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0","children":["$","div",null,{"className":"flex h-screen flex-col justify-between font-sans","children":[["$","$L9",null,{"kbarConfig":{"searchDocumentsPath":"search.json"},"children":[["$","$La",null,{}],["$","main",null,{"className":"mb-auto pt-20","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"flex flex-col items-start justify-start md:mt-24 md:flex-row md:items-center md:justify-center md:space-x-6","children":[["$","div",null,{"className":"space-x-2 pb-8 pt-6 md:space-y-5","children":["$","h1",null,{"className":"text-6xl font-extrabold leading-9 tracking-tight text-gray-900 dark:text-gray-100 md:border-r-2 md:px-6 md:text-8xl md:leading-14","children":"404"}]}],["$","div",null,{"className":"max-w-md","children":[["$","p",null,{"className":"mb-4 text-xl font-bold leading-normal md:text-2xl","children":"Sorry we couldn't find this page."}],["$","p",null,{"className":"mb-8","children":"But dont worry, you can find plenty of other things on our homepage."}],["$","$Lb",null,{"href":"/","className":"focus:shadow-outline-blue inline rounded-lg border border-transparent bg-blue-600 px-4 py-2 text-sm font-medium leading-5 text-white shadow transition-colors duration-150 hover:bg-blue-700 focus:outline-none dark:hover:bg-blue-500","children":"Back to homepage"}]]}]]}],"notFoundStyles":[],"styles":null}]}]]}],["$","footer",null,{"children":["$","div",null,{"className":"mt-16 flex flex-col items-center","children":[["$","div",null,{"className":"mb-3 flex space-x-4","children":[["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"mailto:tztw4723@gmail.com","children":[["$","span",null,{"className":"sr-only","children":"mail"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 20 20","className":"dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6","children":[["$","title",null,{"children":"Mail"}],["$","path",null,{"d":"M2.003 5.884L10 9.882l7.997-3.998A2 2 0 0016 4H4a2 2 0 00-1.997 1.884z"}],["$","path",null,{"d":"M18 8.118l-8 4-8-4V14a2 2 0 002 2h12a2 2 0 002-2V8.118z"}]]}]]}],["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"https://github.com/taoztw","children":[["$","span",null,{"className":"sr-only","children":"github"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 24 24","className":"dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6","children":[["$","title",null,{"children":"Github"}],["$","path",null,{"d":"M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"}]]}]]}],["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"https://facebook.com","children":[["$","span",null,{"className":"sr-only","children":"facebook"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 24 24","className":"dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6","children":[["$","title",null,{"children":"Facebook"}],["$","path",null,{"d":"M24 12.073c0-6.627-5.373-12-12-12s-12 5.373-12 12c0 5.99 4.388 10.954 10.125 11.854v-8.385H7.078v-3.47h3.047V9.43c0-3.007 1.792-4.669 4.533-4.669 1.312 0 2.686.235 2.686.235v2.953H15.83c-1.491 0-1.956.925-1.956 1.874v2.25h3.328l-.532 3.47h-2.796v8.385C19.612 23.027 24 18.062 24 12.073z"}]]}]]}],["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"https://youtube.com","children":[["$","span",null,{"className":"sr-only","children":"youtube"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 24 24","className":"dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6","children":[["$","title",null,{"children":"Youtube"}],["$","path",null,{"d":"M23.499 6.203a3.008 3.008 0 00-2.089-2.089c-1.87-.501-9.4-.501-9.4-.501s-7.509-.01-9.399.501a3.008 3.008 0 00-2.088 2.09A31.258 31.26 0 000 12.01a31.258 31.26 0 00.523 5.785 3.008 3.008 0 002.088 2.089c1.869.502 9.4.502 9.4.502s7.508 0 9.399-.502a3.008 3.008 0 002.089-2.09 31.258 31.26 0 00.5-5.784 31.258 31.26 0 00-.5-5.808zm-13.891 9.4V8.407l6.266 3.604z"}]]}]]}],["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"https://www.linkedin.com","children":[["$","span",null,{"className":"sr-only","children":"linkedin"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 24 24","className":"dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6","children":[["$","title",null,{"children":"Linkedin"}],["$","path",null,{"d":"M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"}]]}]]}],null,["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"https://twitter.com/x","children":[["$","span",null,{"className":"sr-only","children":"x"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 24 24","className":"dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6","children":[["$","title",null,{"children":"X"}],["$","path",null,{"d":"M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 6.932ZM17.61 20.644h2.039L6.486 3.24H4.298Z"}]]}]]}],["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"https://www.instagram.com","children":[["$","span",null,{"className":"sr-only","children":"instagram"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 24 24","className":"dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6","children":[["$","title",null,{"children":"Instagram"}],["$","path",null,{"d":"$c"}]]}]]}],["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"https://www.threads.net","children":[["$","span",null,{"className":"sr-only","children":"threads"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 24 24","className":"dark:hover:text-primary-400 fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 h-6 w-6","children":[["$","title",null,{"children":"Threads"}],["$","path",null,{"d":"$d"}]]}]]}]]}],["$","div",null,{"className":"mb-2 flex space-x-2 text-sm text-gray-500 dark:text-gray-400","children":[["$","div",null,{"children":"Tz"}],["$","div",null,{"children":" • "}],["$","div",null,{"children":"© 2024"}],["$","div",null,{"children":" • "}],["$","a",null,{"target":"_blank","rel":"noopener noreferrer","href":"https://beian.miit.gov.cn/","children":"京ICP备2023010160号"}]]}],["$","div",null,{"className":"mb-8 text-sm text-gray-500 dark:text-gray-400","children":["$","a",null,{"target":"_blank","rel":"noopener noreferrer","href":"https://github.com/timlrx/tailwind-nextjs-starter-blog","children":"Tailwind Nextjs Theme"}]}]]}]}]]}]}]]}]]}]]}],null],null],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/b1aaec952e67a960.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/4fdcd319ea1029e9.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","2",{"rel":"stylesheet","href":"/_next/static/css/74fec4f6a9c4330b.css","precedence":"next","crossOrigin":"$undefined"}]],"$Le"]]]]
e:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"ai | Tz Blog"}],["$","meta","3",{"name":"description","content":"Tz Blog ai tagged content"}],["$","meta","4",{"name":"robots","content":"index, follow"}],["$","meta","5",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","6",{"rel":"canonical","href":"https://tailwind-nextjs-starter-blog.vercel.app/tags/ai"}],["$","link","7",{"rel":"alternate","type":"application/rss+xml","href":"https://tailwind-nextjs-starter-blog.vercel.app/tags/ai/feed.xml"}],["$","meta","8",{"property":"og:title","content":"ai | Tz Blog"}],["$","meta","9",{"property":"og:description","content":"Tz Blog ai tagged content"}],["$","meta","10",{"property":"og:url","content":"https://tailwind-nextjs-starter-blog.vercel.app/tags/ai"}],["$","meta","11",{"property":"og:site_name","content":"Tz Blog"}],["$","meta","12",{"property":"og:locale","content":"en_US"}],["$","meta","13",{"property":"og:image","content":"https://tailwind-nextjs-starter-blog.vercel.app/static/images/twitter-card.png"}],["$","meta","14",{"property":"og:type","content":"website"}],["$","meta","15",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","16",{"name":"twitter:title","content":"ai | Tz Blog"}],["$","meta","17",{"name":"twitter:description","content":"Tz Blog ai tagged content"}],["$","meta","18",{"name":"twitter:image","content":"https://tailwind-nextjs-starter-blog.vercel.app/static/images/twitter-card.png"}],["$","meta","19",{"name":"next-size-adjust"}]]
1:null
